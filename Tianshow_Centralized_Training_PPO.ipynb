{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32ms:\\GITHUB\\SiSL Eval\\Multi-UAV-TA-gym-env\\Tianshow_Centralized_Training_PPO.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/GITHUB/SiSL%20Eval/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m partial\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/GITHUB/SiSL%20Eval/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/s%3A/GITHUB/SiSL%20Eval/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/GITHUB/SiSL%20Eval/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtianshou\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/GITHUB/SiSL%20Eval/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtianshou\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv\u001b[39;00m \u001b[39mimport\u001b[39;00m DummyVectorEnv\n",
      "File \u001b[1;32ms:\\Python311\\Lib\\site-packages\\torch\\__init__.py:123\u001b[0m\n\u001b[0;32m    121\u001b[0m is_loaded \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 123\u001b[0m     res \u001b[39m=\u001b[39m kernel32\u001b[39m.\u001b[39;49mLoadLibraryExW(dll, \u001b[39mNone\u001b[39;49;00m, \u001b[39m0x00001100\u001b[39;49m)\n\u001b[0;32m    124\u001b[0m     last_error \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mget_last_error()\n\u001b[0;32m    125\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m last_error \u001b[39m!=\u001b[39m \u001b[39m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy, RainbowPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "\n",
    "from tianshou.utils.net.common import ActorCritic, DataParallelNet, Net\n",
    "from tianshou.utils.net.discrete import Actor, Critic\n",
    "\n",
    "#from torchviz import make_dot\n",
    "\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomCollector\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "#from CustomClass_multi_head import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes_simplified import CustomNetSimple\n",
    "#from Custom_Classes_simplified import CustomCollectorSimple\n",
    "#from Custom_Classes_simplified import CustomParallelToAECWrapperSimple\n",
    "\n",
    "from TaskAllocation.RL_Policies.CustomClass_MultiHead_Transformer import CustomNetMultiHead\n",
    "from TaskAllocation.RL_Policies.CustomClass_MultiHead_Transformer_PPO_Critic import CriticNetMultiHead\n",
    "\n",
    "from TaskAllocation.RL_Policies.DNN_SISL import DNN_SISL\n",
    "from TaskAllocation.RL_Policies.CNN_SISL import CNN_SISL\n",
    "\n",
    "from mUAV_TA.MultiDroneEnvUtils import agentEnvOptions\n",
    "\n",
    "from mUAV_TA.DroneEnv import MultiUAVEnv\n",
    "#from tianshou_DQN import train\n",
    "model = \"CNN_SISL\" # \"CustomNet\" or \"CustomNetSimple\" or \"CustomNetReduced\" or \"CustomNetMultiHead\"\n",
    "test_num = \"_PPO_OCT01\"\n",
    "policyModel = \"PPO\"\n",
    "\n",
    "train_env_num = 10\n",
    "test_env_num = 10\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "load_policy_name = f'PPO_CustomNetMultiHead_PPO_OCT01.pth'\n",
    "save_policy_name = f'PPO_{name}'\n",
    "policy_path = \"ppo_Custom\"\n",
    "\n",
    "same_policy = True\n",
    "\n",
    "load_model = False\n",
    "\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", log_name)\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.99, \n",
    "              \"estimation_step\": 150, \n",
    "              \"target_update_freq\": 150 * 30 * 2,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 1e-4 }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 500,\n",
    "                  \"step_per_epoch\": 150 * 30,\n",
    "                  \"step_per_collect\": 150 * 10,\n",
    "                  \"episode_per_test\": 10,\n",
    "                  \"batch_size\" : 900,\n",
    "                  \"update_per_step\": 1 / 150, #Only run after close a Collect (run many times as necessary to meet the value)\n",
    "                  \"tn_eps_max\": 0.80,\n",
    "                  \"ts_eps_max\": 0.0,\n",
    "                  }\n",
    "\n",
    "config_default = agentEnvOptions(                                        \n",
    "                 render_speed=-1,\n",
    "                 simulation_frame_rate = 0.01, \n",
    "                 action_mode=\"TaskAssign\",\n",
    "                 simulator_module = \"Internal\", \n",
    "                 max_time_steps=150, \n",
    "                 agents= {\"F1\" : 0, \"F2\" : 2, \"R1\" : 0, \"R2\" : 0},                 \n",
    "                 tasks= { \"Att\" : 15 , \"Rec\" : 0, \"Hold\" : 0},\n",
    "                 multiple_tasks_per_agent = False,\n",
    "                 multiple_agents_per_task = True,\n",
    "                 random_init_pos=False,\n",
    "                 num_obstacles=0,\n",
    "                 hidden_obstacles = False,\n",
    "                 fail_rate = 0.0,\n",
    "                 threats_list = [],#[(\"T1\", 4), (\"T2\" , 2)],\n",
    "                 fixed_seed = -1,\n",
    "                 info = \"No Info\")    \n",
    "\n",
    "Run_Data = f'''{name} \n",
    "Policy: {policyModel} \n",
    "Loaded_Model: {load_policy_name if load_model else \"no\"}  \n",
    "log_path: {log_path}  \n",
    "train/test_env_num: {train_env_num} / {test_env_num}  \n",
    "model: {model}  \n",
    "dqn_params: {dqn_params}  \n",
    "trainer_params: {trainer_params} \n",
    "single_policy: {same_policy}\n",
    "\n",
    "--------- Env ------------  \n",
    "\n",
    "Rewards Only Final Quality and SQuality\n",
    "random_init_pos      : {config_default.random_init_pos}\n",
    "max_time_steps       : {config_default.max_time_steps}\n",
    "simulation_frame_rate: {config_default.simulation_frame_rate}\n",
    "Agents               : {config_default.agents}\n",
    "tasks                : {config_default.tasks}\n",
    "random_init_pos      : {config_default.random_init_pos} \n",
    "threats              : {config_default.threats_list}\n",
    "seed                 : {config_default.fixed_seed}\n",
    "'''\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "def generate_dummy_observation(batch_size=1, sequence_length=31, feature_dim=12):\n",
    "    # Generate a random tensor with the given shape\n",
    "    dummy_obs = torch.randn(batch_size, sequence_length, feature_dim)\n",
    "\n",
    "    return dummy_obs\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()\n",
    "    agent_name = env.agents[0]  # Get the name of the first agent\n",
    "    \n",
    "    #print(env.observation_space )\n",
    "    agent_observation_space = env.observation_space # assuming 'agent0' is a valid agent name\n",
    "    state_shape_agent_position = agent_observation_space[\"agent_position\"].shape[0]\n",
    "    state_shape_agent_state = agent_observation_space[\"agent_state\"].shape[0]\n",
    "    state_shape_agent_type = agent_observation_space[\"agent_type\"].shape[0]\n",
    "    state_shape_next_free_time = agent_observation_space[\"next_free_time\"].shape[0]\n",
    "    state_shape_position_after_last_task = agent_observation_space[\"position_after_last_task\"].shape[0]       \n",
    "    #state_shape_agent_relay_area = agent_observation_space[\"agent_relay_area\"].shape[0]\n",
    "        \n",
    "    state_shape_agent = (state_shape_agent_position + state_shape_agent_state +\n",
    "                     state_shape_agent_type+ state_shape_next_free_time + state_shape_position_after_last_task #+                     \n",
    "                     #state_shape_agent_relay_area\n",
    "                     )                 \n",
    "\n",
    "    state_shape_task = 31 * 13 #env.observation_space[\"tasks_info\"].shape[0]\n",
    "                  \n",
    "    action_shape = env.action_space[agent_name].shape[0]\n",
    "    #action_shape = env.action_space[agent_name].n\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"          \n",
    "    \n",
    "    if agent_learn is None:\n",
    "        # model       \n",
    "        \n",
    "        if model == \"CustomNetMultiHead\":\n",
    "            \n",
    "            netActor = CustomNetMultiHead(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "\n",
    "            netCritic = CriticNetMultiHead(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "\n",
    "        if model == \"CNN_SISL\":\n",
    "            \n",
    "            netActor = CNN_SISL(\n",
    "                obs_shape=agent_observation_space.shape,                \n",
    "                action_shape=5,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"                \n",
    "            ).to(device)\n",
    "\n",
    "            netCritic = CNN_SISL(\n",
    "                obs_shape=agent_observation_space.shape,                \n",
    "                action_shape=5,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"                \n",
    "            ).to(device)\n",
    "            \n",
    "                        \n",
    "            if False:#torch.cuda.is_available():\n",
    "                actor = DataParallelNet(Actor(netActor, action_shape, device=None).to(device))\n",
    "                critic = DataParallelNet(Critic(netCritic, device=None).to(device))\n",
    "            else:\n",
    "                actor = Actor(netActor, action_shape, device=device).to(device)\n",
    "                critic = Critic(netCritic, device=device).to(device)\n",
    "            \n",
    "            actor_critic = ActorCritic(actor, critic)\n",
    "        \n",
    "        # orthogonal initialization\n",
    "        # for m in actor_critic.modules():\n",
    "        #     if isinstance(m, torch.nn.Linear):\n",
    "        #         torch.nn.init.orthogonal_(m.weight)\n",
    "        #         torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "        \n",
    "        dist = torch.distributions.Categorical         \n",
    "            \n",
    "        #optim_actor  = torch.optim.Adam(netActor.parameters(),  lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "        #optim_critic = torch.optim.Adam(netCritic.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "        optim = torch.optim.Adam(actor_critic.parameters(), lr=dqn_params[\"lr\"])\n",
    "                \n",
    "        agent_learn = PPOPolicy(\n",
    "            actor=actor,\n",
    "            critic=critic,\n",
    "            optim=optim,\n",
    "            dist_fn=dist,\n",
    "            action_scaling=isinstance(env.action_space, Box),\n",
    "            discount_factor=0.99,\n",
    "            max_grad_norm=0.5,\n",
    "            eps_clip=0.2,\n",
    "            vf_coef=0.5,\n",
    "            ent_coef=0.0,\n",
    "            gae_lambda=0.95,\n",
    "            reward_normalization=0,\n",
    "            dual_clip=None,\n",
    "            value_clip=0,\n",
    "            action_space=env.action_space,\n",
    "            deterministic_eval=True,\n",
    "            advantage_normalization=0,\n",
    "            recompute_advantage=0,\n",
    "        )\n",
    "        \n",
    " \n",
    "        if load_model == True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "                          \n",
    "        \n",
    "        agents = [None for _ in range(len(env.agents))]        \n",
    "        \n",
    "        if not same_policy:\n",
    "\n",
    "            for i,agent in enumerate(env.agents):             \n",
    "                if agent[0] == \"F\":                \n",
    "                    agents[i] = agent_learn2\n",
    "                    #print(\"F\")\n",
    "                else:\n",
    "                    agents[i] = agent_learn\n",
    "                    #print(\"R\")\n",
    "        else:\n",
    "            agents = [agent_learn for _ in range(len(env.agents))]\n",
    "\n",
    "        # print(agents)\n",
    "        # print([o.type for o in agents_obj])\n",
    "\n",
    "\n",
    "        # agent_learn2\n",
    "        \n",
    "    policy = MultiAgentPolicyManager(agents, env)  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    env_paralell = MultiUAVEnv(config=config_default)    \n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "print(Run_Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()    \n",
    "\n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = CustomCollector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(100_000, len(train_envs)),\n",
    "        # PrioritizedVectorReplayBuffer( 100_000, len(train_envs), alpha=0.6, beta=0.4) , \n",
    "        #ListReplayBuffer(100000)       \n",
    "        # exploration_noise=True             \n",
    "    )\n",
    "    test_collector = CustomCollector(policy, test_envs, exploration_noise=True)\n",
    "     \n",
    "    # print(\"Buffer Warming Up \")\n",
    "    \n",
    "    # for i in range(10):#int(trainer_params['batch_size'] / (300 * 10 ) )):\n",
    "        \n",
    "    #     train_collector.collect(n_episode=train_env_num)#,random=True) #trainer_params['batch_size'] * train_env_num))\n",
    "    #     #train_collector.collect(n_step=300 * 10)\n",
    "    #     print(\".\", end=\"\") \n",
    "    \n",
    "    # print(\"\\nBuffer Lenght: \", len(train_collector.buffer)/ 150 ) \n",
    "    #train_collector.collect(n_episode=trainer_params['batch_size'])\n",
    "    #test_collector.collect(n_episode=2 )\n",
    "    #test_collector.collect(n_step=trainer_params['batch size'] * train_env_num)\n",
    "    \n",
    "    # ======== tensorboard logging setup =========\n",
    "    #         \n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"Config\", str(Run_Data))\n",
    "    #if same_policy:\n",
    "    #    writer.add_text(\"Model\", str(policy.policies[agents[0]].model).replace('\\n', '  \\n'))    \n",
    "    #else:\n",
    "    #     writer.add_text(\"ModelR\", str(policy.policies['R_agent0'].model).replace('\\n', '  \\n'))\n",
    "    #     writer.add_text(\"ModelF\", str(policy.policies['F_agent0'].model).replace('\\n', '  \\n'))\n",
    "    \n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    global_step_holder = [0]  \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        if same_policy:             \n",
    "            torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \".pth\")\n",
    "            print(\"Best Saved\")\n",
    "        else:\n",
    "            torch.save(policy.policies['R_agent0'].state_dict(), model_save_path + \"R.pth\")\n",
    "            torch.save(policy.policies['F_agent0'].state_dict(), model_save_path + \"F.pth\")\n",
    "            print(\"Bests Saved\")\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 9939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])  \n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "        #print(rews)  \n",
    "        global_step_holder[0] += 1    \n",
    "\n",
    "        #if rews[:,0].mean() != 0:\n",
    "        #    print( rews)\n",
    "        return rews[:,0]\n",
    "\n",
    "\n",
    "    #Define the hook function\n",
    "    def register_activation_hook(module, input, output, layer_name, writer, global_step_holder):\n",
    "        #print(f\"Hook executed for {layer_name} at step {global_step_holder[0]}\")\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # If the output is a tuple, use the first element\n",
    "        writer.add_histogram(f\"activations/{layer_name}\", output, global_step_holder[0])\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function0 = partial(register_activation_hook, layer_name=\"task_embeddings\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook0 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function0)\n",
    "\n",
    "    #Register the hook\n",
    "    # hook_function1 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    # hook1 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function1)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function2 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook2 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function2)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function3 = partial(register_activation_hook, layer_name=\"tasks_info\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook3 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function3)\n",
    "        \n",
    "    #Add Logger Details\n",
    "    def log_gradients(policy, writer, global_step_holder, **kwargs):\n",
    "        for name, param in policy.model.named_parameters():\n",
    "            writer.add_histogram(f\"{name}.grad\", param.grad, global_step_holder[0])    \n",
    "\n",
    "    #Modify the hook definition to pass the writer and global_step_holder\n",
    "    # policy.policies['agent0'].post_optim_hook = partial(log_gradients, writer=writer, global_step_holder=global_step_holder)\n",
    "        \n",
    "   \n",
    "    def condensed_make_dot(var, params=None):\n",
    "        dot = make_dot(var, params)\n",
    "        \n",
    "        # Here's where you'd condense or modify the graph.\n",
    "        # For example, to remove all nodes related to ReLU operations:\n",
    "        # (This is just a conceptual example. You'd modify this to fit your needs.)\n",
    "        nodes_to_remove = [n for n in dot.body if 'Relu' in n]\n",
    "        nodes_to_remove += [n for n in dot.body if 'Accumu' in n]       \n",
    "        for n in nodes_to_remove:\n",
    "            dot.body.remove(n)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    if False:\n",
    "\n",
    "        dummy_input = generate_dummy_observation()    \n",
    "        output = policy.policies['agent0'].model(dummy_input)     \n",
    "        \n",
    "        dot = make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "        # Save as .png\n",
    "        dot.format = 'svg'\n",
    "        dot.render(filename='model_architecture', directory=log_path, cleanup=True)\n",
    "\n",
    "    # dot = condensed_make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "    # # Save as .png\n",
    "    # dot.format = 'svg'\n",
    "    # dot.render(filename='model_architecture_condensed', directory=log_path, cleanup=True)\n",
    "\n",
    "    \n",
    "    # policy.policies[agents[0]].set_eps(0.8)\n",
    "    \n",
    "    # for i in range(int(15000)):  # total step\n",
    "        \n",
    "    #     collect_result = train_collector.collect(n_step=450)\n",
    "\n",
    "    #     # or every 1000 steps, we test it on test_collector\n",
    "    #     if collect_result['rews'].mean() >= 10 or i % 1500 == 0:\n",
    "    #         policy.policies[agents[0]].set_eps(0.0)\n",
    "            \n",
    "    #         result = test_collector.collect(n_episode=1)\n",
    "            \n",
    "    #         if result['rews'].mean() >= 10:\n",
    "    #             print(f'Finished training! Test mean returns: {result[\"rews\"].mean()}')\n",
    "    #             break\n",
    "    #         else:\n",
    "    #             # back to training eps\n",
    "    #             policy.policies[agents[0]].set_eps(0.8)\n",
    "\n",
    "    #     # train policy with a sampled batch data from buffer\n",
    "    #     losses = policy.policies[agents[0]].update(64, train_collector.buffer)\n",
    "    #     print(losses)\n",
    "\n",
    "\n",
    "    # # ======== Step 5: Run the trainer =========   \n",
    "    result = OnpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,\n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        repeat_per_collect=10, #TODO: understand\n",
    "        episode_per_test=trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger,\n",
    "    ).run()\n",
    "    assert stop_fn(result[\"best_reward\"])     \n",
    "\n",
    "    writer.close()\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "import torch\n",
    "\n",
    "import mUAV_TA.MultiDroneEnvUtils as utils\n",
    "#from Custom_Classes import CustomCollector\n",
    "\n",
    "def _get_env_eval():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    case =  {'case' : 0, 'F1':2, 'F2': 2, \"R1\" : 12, 'R2' : 3, \"Att\" : 4, \"Rec\" : 22}\n",
    "\n",
    "    config = utils.agentEnvOptions( \n",
    "                 render_mode = 'human',                  \n",
    "                 render_speed=1,\n",
    "                 simulation_frame_rate = 0.01, \n",
    "                 action_mode=\"TaskAssign\",\n",
    "                 simulator_module = \"Internal\", \n",
    "                 max_time_steps=200, \n",
    "                 agents= {\"F1\" : 4, \"F2\" : 2, \"R1\" : 6},                 \n",
    "                 tasks= { \"Att\" : 4 , \"Rec\" : 16, \"Hold\" : 4},\n",
    "                 multiple_tasks_per_agent = False,\n",
    "                 multiple_agents_per_task = True,\n",
    "                 random_init_pos=False,\n",
    "                 num_obstacles=0,\n",
    "                 hidden_obstacles = False,\n",
    "                 fail_rate = 0.0,\n",
    "                 info = \"No Info\")\n",
    "   \n",
    "    \n",
    "    env_paralell = MultiUAVEnv()\n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "\n",
    "# Create a new instance of the policy with the same architecture as the saved policy\n",
    "name = 'policy_CustomNetMultiHead_Eval_TBTA_Relative_Representation_01.pth' \n",
    "load_policy_name = f'policy_{name}'\n",
    "\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", name)\n",
    "\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "\n",
    "# Load the saved checkpoint\n",
    "policy_test = policy.policies['agent0']\n",
    "policy_test.load_state_dict(torch.load(model_save_path ))\n",
    "\n",
    "envs = DummyVectorEnv([_get_env_eval for _ in range(1)])\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "#collector = CustomCollector(policy.policies['agent0'], envs, exploration_noise=True)\n",
    "#collector = CustomCollector(policy_test, envs, exploration_noise=False)\n",
    "collector = CustomCollector(policy, envs, exploration_noise=True)\n",
    "\n",
    "#results = collector.collect(n_episode=1)\n",
    "results = collector.collect(n_episode=1)#, gym_reset_kwargs={'seed' :2})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['rews']\n",
    "print(np.mean(results['rews'][results['rews'] > -10]))\n",
    "\n",
    "\n",
    "#create a function  to print a histogram of the results['rews']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(results['rews'][results['rews'] > -10], bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import st\n",
    "import torch\n",
    "from tianshou.data import Batch\n",
    "\n",
    "# load policy as in your original code\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "policy_test = policy.policies['agent0']\n",
    "state_saved = torch.load(model_save_path )\n",
    "#print(policy_test)\n",
    "policy_test.load_state_dict(state_saved)\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "# initialize your environment\n",
    "#env = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "env = MultiDroneEnv(None)\n",
    "env.max_time_steps = 50\n",
    "\n",
    "# simulate the interaction with the environment manually\n",
    "for i in range(10):\n",
    "    for episode in range(1):  # simulate 10 episodes\n",
    "        \n",
    "        #env.render_speed = 1\n",
    "        obs, _  = env.reset(seed=episode)         \n",
    "        info         = env.get_initial_state()\n",
    "        \n",
    "        drones = info[\"drones\"]\n",
    "        tasks = info[\"tasks\"]\n",
    "            \n",
    "        done = {0 : False}\n",
    "        truncations = {0 : False}\n",
    "        \n",
    "        episodo_reward = 0\n",
    "        #obs, reward, done, truncations, info = env.step(action)\n",
    "\n",
    "        while not all(done.values()) and not all(truncations.values()):\n",
    "            \n",
    "            agent_id = \"agent\" + str(env.agent_selector._current_agent)\n",
    "            # Create a Batch of observations\n",
    "            obs_batch = Batch(obs=[obs[agent_id]], info=[{}])  # add empty info for each observation\n",
    "            \n",
    "            #print(obs_batch)\n",
    "            # Forward the batch of observations through the policy to get the actions\n",
    "            action = policy_test(obs_batch).act\n",
    "            action = {agent_id : action[0]}\n",
    "        \n",
    "            obs, reward, done, truncations, info = env.step(action)\n",
    "            \n",
    "            episodo_reward += sum(reward.values())/env.n_agents\n",
    "\n",
    "        \n",
    "\n",
    "    print(episodo_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
