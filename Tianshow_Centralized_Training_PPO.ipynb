{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomNetMultiHead_PPO_OCT01 \n",
      "Policy: PPO \n",
      "Loaded_Model: no  \n",
      "log_path: ./Logs\\dqn\\CustomNetMultiHead_PPO_OCT01231107-103504  \n",
      "train/test_env_num: 10 / 10  \n",
      "model: CustomNetMultiHead  \n",
      "dqn_params: {'discount_factor': 0.99, 'estimation_step': 150, 'target_update_freq': 9000, 'optminizer': 'Adam', 'lr': 0.0001}  \n",
      "trainer_params: {'max_epoch': 500, 'step_per_epoch': 4500, 'step_per_collect': 1500, 'episode_per_test': 10, 'batch_size': 900, 'update_per_step': 0.0033333333333333335, 'tn_eps_max': 0.8, 'ts_eps_max': 0.0} \n",
      "single_policy: True\n",
      "\n",
      "--------- Env ------------  \n",
      "\n",
      "Rewards Only Final Quality and SQuality\n",
      "random_init_pos      : False\n",
      "max_time_steps       : 150\n",
      "simulation_frame_rate: 0.01\n",
      "Agents               : {'F1': 0, 'F2': 0, 'R1': 1, 'R2': 0}\n",
      "tasks                : {'Att': 0, 'Rec': 2, 'Hold': 0}\n",
      "random_init_pos      : False \n",
      "threats              : []\n",
      "seed                 : -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Dict, Discrete, MultiDiscrete, Box\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy, RainbowPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "\n",
    "from tianshou.utils.net.common import ActorCritic, DataParallelNet, Net\n",
    "from tianshou.utils.net.discrete import Actor, Critic\n",
    "\n",
    "#from torchviz import make_dot\n",
    "\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomCollector\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "#from CustomClass_multi_head import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes_simplified import CustomNetSimple\n",
    "#from Custom_Classes_simplified import CustomCollectorSimple\n",
    "#from Custom_Classes_simplified import CustomParallelToAECWrapperSimple\n",
    "\n",
    "from TaskAllocation.RL_Policies.CustomClass_MultiHead_Transformer import CustomNetMultiHead\n",
    "from TaskAllocation.RL_Policies.CustomClass_MultiHead_Transformer_PPO_Critic import CriticNetMultiHead\n",
    "\n",
    "from mUAV_TA.MultiDroneEnvUtils import agentEnvOptions\n",
    "\n",
    "from mUAV_TA.DroneEnv import MultiUAVEnv\n",
    "#from tianshou_DQN import train\n",
    "model = \"CustomNetMultiHead\" # \"CustomNet\" or \"CustomNetSimple\" or \"CustomNetReduced\" or \"CustomNetMultiHead\"\n",
    "test_num = \"_PPO_OCT01\"\n",
    "policyModel = \"PPO\"\n",
    "\n",
    "train_env_num = 10\n",
    "test_env_num = 10\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "load_policy_name = f'PPO_CustomNetMultiHead_PPO_OCT01.pth'\n",
    "save_policy_name = f'PPO_{name}'\n",
    "policy_path = \"ppo_Custom\"\n",
    "\n",
    "same_policy = True\n",
    "\n",
    "load_model = True\n",
    "\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", log_name)\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.99, \n",
    "              \"estimation_step\": 150, \n",
    "              \"target_update_freq\": 150 * 30 * 2,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 1e-4 }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 500,\n",
    "                  \"step_per_epoch\": 150 * 30,\n",
    "                  \"step_per_collect\": 150 * 10,\n",
    "                  \"episode_per_test\": 10,\n",
    "                  \"batch_size\" : 900,\n",
    "                  \"update_per_step\": 1 / 300, #Only run after close a Collect (run many times as necessary to meet the value)\n",
    "                  \"tn_eps_max\": 0.80,\n",
    "                  \"ts_eps_max\": 0.0,\n",
    "                  }\n",
    "\n",
    "config_default = agentEnvOptions(                                        \n",
    "                 render_speed=-1,\n",
    "                 simulation_frame_rate = 0.01, \n",
    "                 action_mode=\"TaskAssign\",\n",
    "                 simulator_module = \"Internal\", \n",
    "                 max_time_steps=150, \n",
    "                 agents= {\"F1\" : 0, \"F2\" : 0, \"R1\" : 4, \"R2\" : 4},                 \n",
    "                 tasks= { \"Att\" : 0 , \"Rec\" : 20, \"Hold\" : 0},\n",
    "                 multiple_tasks_per_agent = False,\n",
    "                 multiple_agents_per_task = True,\n",
    "                 random_init_pos=False,\n",
    "                 num_obstacles=0,\n",
    "                 hidden_obstacles = False,\n",
    "                 fail_rate = 0.0,\n",
    "                 threats_list = [],#[(\"T1\", 4), (\"T2\" , 2)],\n",
    "                 fixed_seed = -1,\n",
    "                 info = \"No Info\")    \n",
    "\n",
    "Run_Data = f'''{name} \n",
    "Policy: {policyModel} \n",
    "Loaded_Model: {load_policy_name if load_model else \"no\"}  \n",
    "log_path: {log_path}  \n",
    "train/test_env_num: {train_env_num} / {test_env_num}  \n",
    "model: {model}  \n",
    "dqn_params: {dqn_params}  \n",
    "trainer_params: {trainer_params} \n",
    "single_policy: {same_policy}\n",
    "\n",
    "--------- Env ------------  \n",
    "\n",
    "Rewards Only Final Quality and SQuality\n",
    "random_init_pos      : {config_default.random_init_pos}\n",
    "max_time_steps       : {config_default.max_time_steps}\n",
    "simulation_frame_rate: {config_default.simulation_frame_rate}\n",
    "Agents               : {config_default.agents}\n",
    "tasks                : {config_default.tasks}\n",
    "random_init_pos      : {config_default.random_init_pos} \n",
    "threats              : {config_default.threats_list}\n",
    "seed                 : {config_default.fixed_seed}\n",
    "'''\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "def generate_dummy_observation(batch_size=1, sequence_length=31, feature_dim=12):\n",
    "    # Generate a random tensor with the given shape\n",
    "    dummy_obs = torch.randn(batch_size, sequence_length, feature_dim)\n",
    "\n",
    "    return dummy_obs\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()\n",
    "    agent_name = env.agents[0]  # Get the name of the first agent\n",
    "    \n",
    "    #print(env.observation_space )\n",
    "    agent_observation_space = env.observation_space # assuming 'agent0' is a valid agent name\n",
    "    state_shape_agent_position = agent_observation_space[\"agent_position\"].shape[0]\n",
    "    state_shape_agent_state = agent_observation_space[\"agent_state\"].shape[0]\n",
    "    state_shape_agent_type = agent_observation_space[\"agent_type\"].shape[0]\n",
    "    state_shape_next_free_time = agent_observation_space[\"next_free_time\"].shape[0]\n",
    "    state_shape_position_after_last_task = agent_observation_space[\"position_after_last_task\"].shape[0]       \n",
    "    #state_shape_agent_relay_area = agent_observation_space[\"agent_relay_area\"].shape[0]\n",
    "        \n",
    "    state_shape_agent = (state_shape_agent_position + state_shape_agent_state +\n",
    "                     state_shape_agent_type+ state_shape_next_free_time + state_shape_position_after_last_task #+                     \n",
    "                     #state_shape_agent_relay_area\n",
    "                     )                 \n",
    "\n",
    "    state_shape_task = 31 * 13 #env.observation_space[\"tasks_info\"].shape[0]\n",
    "                  \n",
    "    action_shape = env.action_space[agent_name].shape[0]\n",
    "    #action_shape = env.action_space[agent_name].n\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"          \n",
    "    \n",
    "    if agent_learn is None:\n",
    "        # model       \n",
    "        \n",
    "        if model == \"CustomNetMultiHead\":\n",
    "            \n",
    "            netActor = CustomNetMultiHead(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "\n",
    "            netCritic = CriticNetMultiHead(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "                        \n",
    "            if False:#torch.cuda.is_available():\n",
    "                actor = DataParallelNet(Actor(netActor, action_shape, device=None).to(device))\n",
    "                critic = DataParallelNet(Critic(netCritic, device=None).to(device))\n",
    "            else:\n",
    "                actor = Actor(netActor, action_shape, device=device).to(device)\n",
    "                critic = Critic(netCritic, device=device).to(device)\n",
    "            \n",
    "            actor_critic = ActorCritic(actor, critic)\n",
    "        \n",
    "        # orthogonal initialization\n",
    "        # for m in actor_critic.modules():\n",
    "        #     if isinstance(m, torch.nn.Linear):\n",
    "        #         torch.nn.init.orthogonal_(m.weight)\n",
    "        #         torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "        \n",
    "        dist = torch.distributions.Categorical         \n",
    "            \n",
    "        #optim_actor  = torch.optim.Adam(netActor.parameters(),  lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "        #optim_critic = torch.optim.Adam(netCritic.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "        optim = torch.optim.Adam(actor_critic.parameters(), lr=dqn_params[\"lr\"])\n",
    "                \n",
    "        agent_learn = PPOPolicy(\n",
    "            actor=actor,\n",
    "            critic=critic,\n",
    "            optim=optim,\n",
    "            dist_fn=dist,\n",
    "            action_scaling=isinstance(env.action_space, Box),\n",
    "            discount_factor=0.99,\n",
    "            max_grad_norm=0.5,\n",
    "            eps_clip=0.2,\n",
    "            vf_coef=0.5,\n",
    "            ent_coef=0.0,\n",
    "            gae_lambda=0.95,\n",
    "            reward_normalization=0,\n",
    "            dual_clip=None,\n",
    "            value_clip=0,\n",
    "            action_space=env.action_space,\n",
    "            deterministic_eval=True,\n",
    "            advantage_normalization=0,\n",
    "            recompute_advantage=0,\n",
    "        )\n",
    "        \n",
    " \n",
    "        if load_model == True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "                          \n",
    "        \n",
    "        agents = [None for _ in range(len(env.agents))]        \n",
    "        \n",
    "        if not same_policy:\n",
    "\n",
    "            for i,agent in enumerate(env.agents):             \n",
    "                if agent[0] == \"F\":                \n",
    "                    agents[i] = agent_learn2\n",
    "                    #print(\"F\")\n",
    "                else:\n",
    "                    agents[i] = agent_learn\n",
    "                    #print(\"R\")\n",
    "        else:\n",
    "            agents = [agent_learn for _ in range(len(env.agents))]\n",
    "\n",
    "        # print(agents)\n",
    "        # print([o.type for o in agents_obj])\n",
    "\n",
    "\n",
    "        # agent_learn2\n",
    "        \n",
    "    policy = MultiAgentPolicyManager(agents, env)  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    env_paralell = MultiUAVEnv(config=config_default)    \n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "print(Run_Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 4501it [02:34, 29.15it/s, R1_agent0/loss=566.787, R1_agent0/loss/clip=-10.505, R1_agent0/loss/ent=3.429, R1_agent0/loss/vf=1154.582, env_step=4500, len=150, n/ep=10, n/st=1500, rew=62.15]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 62.341556 ± 103.943643, best_reward: 108.905556 ± 115.436030 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 4501it [02:35, 28.88it/s, R1_agent0/loss=799.111, R1_agent0/loss/clip=-13.937, R1_agent0/loss/ent=3.429, R1_agent0/loss/vf=1626.095, env_step=9000, len=150, n/ep=10, n/st=1500, rew=140.19]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #2: test_reward: 141.549800 ± 70.129758, best_reward: 141.549800 ± 70.129758 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 4501it [02:42, 27.77it/s, R1_agent0/loss=840.541, R1_agent0/loss/clip=-14.458, R1_agent0/loss/ent=3.429, R1_agent0/loss/vf=1709.999, env_step=13500, len=150, n/ep=10, n/st=1500, rew=156.07]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 78.499578 ± 76.347959, best_reward: 141.549800 ± 70.129758 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 4501it [02:58, 25.23it/s, R1_agent0/loss=879.063, R1_agent0/loss/clip=-14.720, R1_agent0/loss/ent=3.429, R1_agent0/loss/vf=1787.566, env_step=18000, len=150, n/ep=10, n/st=1500, rew=125.52]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 140.459044 ± 98.924779, best_reward: 141.549800 ± 70.129758 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 4501it [03:07, 23.95it/s, R1_agent0/loss=881.493, R1_agent0/loss/clip=-14.152, R1_agent0/loss/ent=3.429, R1_agent0/loss/vf=1791.290, env_step=22500, len=150, n/ep=10, n/st=1500, rew=94.25]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 110.593111 ± 136.497933, best_reward: 141.549800 ± 70.129758 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 4501it [02:55, 25.58it/s, R1_agent0/loss=838.355, R1_agent0/loss/clip=-12.403, R1_agent0/loss/ent=3.429, R1_agent0/loss/vf=1701.517, env_step=27000, len=150, n/ep=10, n/st=1500, rew=139.44]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 108.385111 ± 115.807520, best_reward: 141.549800 ± 70.129758 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 4501it [02:39, 28.30it/s, R1_agent0/loss=854.322, R1_agent0/loss/clip=-9.415, R1_agent0/loss/ent=3.429, R1_agent0/loss/vf=1727.475, env_step=31500, len=150, n/ep=10, n/st=1500, rew=140.24]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 78.852200 ± 103.667405, best_reward: 141.549800 ± 70.129758 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 4501it [02:39, 28.23it/s, R1_agent0/loss=873.130, R1_agent0/loss/clip=-5.973, R1_agent0/loss/ent=3.429, R1_agent0/loss/vf=1758.206, env_step=36000, len=150, n/ep=10, n/st=1500, rew=140.45]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #8: test_reward: 156.516556 ± 129.561728, best_reward: 156.516556 ± 129.561728 in #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9:  67%|######6   | 3000/4500 [01:49<00:54, 27.42it/s, R1_agent0/loss=1056.712, R1_agent0/loss/clip=-5.554, R1_agent0/loss/ent=3.429, R1_agent0/loss/vf=2124.530, env_step=39000, len=150, n/ep=10, n/st=1500, rew=171.74]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\GITHUB\\Multi-UAV-TA-gym-env\\Tianshow_Centralized_Training_PPO.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m     dot\u001b[39m.\u001b[39mrender(filename\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel_architecture\u001b[39m\u001b[39m'\u001b[39m, directory\u001b[39m=\u001b[39mlog_path, cleanup\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m \u001b[39m# dot = condensed_make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39m# # Save as .png\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39m# dot.format = 'svg'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m \u001b[39m# # ======== Step 5: Run the trainer =========   \u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m result \u001b[39m=\u001b[39m OnpolicyTrainer(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m     policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m     train_collector\u001b[39m=\u001b[39;49mtrain_collector,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m     test_collector\u001b[39m=\u001b[39;49mtest_collector,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m     max_epoch\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mmax_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m     step_per_epoch\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mstep_per_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m     repeat_per_collect\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \u001b[39m#TODO: understand\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m     episode_per_test\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mepisode_per_test\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m     step_per_collect\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mstep_per_collect\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m     stop_fn\u001b[39m=\u001b[39;49mstop_fn,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m     save_best_fn\u001b[39m=\u001b[39;49msave_best_fn,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m )\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m \u001b[39massert\u001b[39;00m stop_fn(result[\u001b[39m\"\u001b[39m\u001b[39mbest_reward\u001b[39m\u001b[39m\"\u001b[39m])     \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m writer\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\trainer\\base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[0;32m    443\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[0;32m    444\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[0;32m    445\u001b[0m     )\n\u001b[0;32m    446\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\trainer\\base.py:288\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m result: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[0;32m    287\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     data, result, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step()\n\u001b[0;32m    289\u001b[0m     t\u001b[39m.\u001b[39mupdate(result[\u001b[39m\"\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    290\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\trainer\\base.py:384\u001b[0m, in \u001b[0;36mBaseTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_fn:\n\u001b[0;32m    383\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_fn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv_step)\n\u001b[1;32m--> 384\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_collector\u001b[39m.\u001b[39;49mcollect(\n\u001b[0;32m    385\u001b[0m     n_step\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_per_collect, n_episode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisode_per_collect\n\u001b[0;32m    386\u001b[0m )\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mn/ep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_metric:\n\u001b[0;32m    388\u001b[0m     rew \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_metric(result[\u001b[39m\"\u001b[39m\u001b[39mrews\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\GITHUB\\Multi-UAV-TA-gym-env\\TaskAllocation\\RL_Policies\\Custom_Classes.py:178\u001b[0m, in \u001b[0;36mCustomCollector.collect\u001b[1;34m(self, n_step, n_episode, random, render, gym_reset_kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollect\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[0;32m    173\u001b[0m             n_step: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \n\u001b[0;32m    174\u001b[0m             n_episode: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \n\u001b[0;32m    175\u001b[0m             random: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \n\u001b[0;32m    176\u001b[0m             render: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    177\u001b[0m             gym_reset_kwargs: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:        \n\u001b[1;32m--> 178\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcollect(n_step\u001b[39m=\u001b[39;49mn_step, n_episode\u001b[39m=\u001b[39;49mn_episode, random\u001b[39m=\u001b[39;49mrandom, render\u001b[39m=\u001b[39;49mrender, gym_reset_kwargs\u001b[39m=\u001b[39;49mgym_reset_kwargs)\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\data\\collector.py:278\u001b[0m, in \u001b[0;36mCollector.collect\u001b[1;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m no_grad:\n\u001b[0;32m    276\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# faster than retain_grad version\u001b[39;00m\n\u001b[0;32m    277\u001b[0m         \u001b[39m# self.data.obs will be used by agent to get result\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, last_state)\n\u001b[0;32m    279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    280\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, last_state)\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\policy\\multiagent\\mapolicy.py:146\u001b[0m, in \u001b[0;36mMultiAgentPolicyManager.forward\u001b[1;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tmp_batch\u001b[39m.\u001b[39mobs_next, \u001b[39m'\u001b[39m\u001b[39mobs\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    145\u001b[0m         tmp_batch\u001b[39m.\u001b[39mobs_next \u001b[39m=\u001b[39m tmp_batch\u001b[39m.\u001b[39mobs_next\u001b[39m.\u001b[39mobs\n\u001b[1;32m--> 146\u001b[0m out \u001b[39m=\u001b[39m policy(\n\u001b[0;32m    147\u001b[0m     batch\u001b[39m=\u001b[39mtmp_batch,\n\u001b[0;32m    148\u001b[0m     state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m state[agent_id],\n\u001b[0;32m    149\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    150\u001b[0m )\n\u001b[0;32m    151\u001b[0m act \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mact           \n\u001b[0;32m    152\u001b[0m each_state \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mstate \\\n\u001b[0;32m    153\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mhasattr\u001b[39m(out, \u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m out\u001b[39m.\u001b[39mstate \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \\\n\u001b[0;32m    154\u001b[0m     \u001b[39melse\u001b[39;00m Batch()\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\policy\\modelfree\\pg.py:108\u001b[0m, in \u001b[0;36mPGPolicy.forward\u001b[1;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     90\u001b[0m     batch: Batch,\n\u001b[0;32m     91\u001b[0m     state: Optional[Union[\u001b[39mdict\u001b[39m, Batch, np\u001b[39m.\u001b[39mndarray]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m     93\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Batch:\n\u001b[0;32m     94\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute action over the given batch data.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[0;32m     96\u001b[0m \u001b[39m    :return: A :class:`~tianshou.data.Batch` which has 4 keys:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39m        more detailed explanation.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     logits, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor(batch\u001b[39m.\u001b[39;49mobs, state\u001b[39m=\u001b[39;49mstate, info\u001b[39m=\u001b[39;49mbatch\u001b[39m.\u001b[39;49minfo)\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(logits, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    110\u001b[0m         dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_fn(\u001b[39m*\u001b[39mlogits)\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\utils\\net\\discrete.py:67\u001b[0m, in \u001b[0;36mActor.forward\u001b[1;34m(self, obs, state, info)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     62\u001b[0m     obs: Union[np\u001b[39m.\u001b[39mndarray, torch\u001b[39m.\u001b[39mTensor],\n\u001b[0;32m     63\u001b[0m     state: Any \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     64\u001b[0m     info: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {},\n\u001b[0;32m     65\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Any]:\n\u001b[0;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Mapping: s -> Q(s, \\*).\"\"\"\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     logits, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(obs, state)\n\u001b[0;32m     68\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast(logits)\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax_output:\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\GITHUB\\Multi-UAV-TA-gym-env\\TaskAllocation\\RL_Policies\\CustomClass_MultiHead_Transformer.py:178\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, obs, state, info)\u001b[0m\n\u001b[0;32m    166\u001b[0m     batch_tasks.append([\n\u001b[0;32m    167\u001b[0m         agent_type[i] / 4, #1\n\u001b[0;32m    168\u001b[0m         distance,      #1\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         is_alloc_task,    #1                                                \n\u001b[0;32m    174\u001b[0m         ])\n\u001b[0;32m    176\u001b[0m     batch_tasks[-1].extend(reqs_result)\n\u001b[1;32m--> 178\u001b[0m # Pad the task_values array to match the maximum number of tasks                \n\u001b[0;32m    179\u001b[0m num_padding_needed = self.max_tasks - len(batch_tasks)\n\u001b[0;32m    180\u001b[0m padding = [[-1] * self.task_size for _ in range(num_padding_needed)]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()    \n",
    "\n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = CustomCollector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(100_000, len(train_envs)),\n",
    "        # PrioritizedVectorReplayBuffer( 100_000, len(train_envs), alpha=0.6, beta=0.4) , \n",
    "        #ListReplayBuffer(100000)       \n",
    "        # exploration_noise=True             \n",
    "    )\n",
    "    test_collector = CustomCollector(policy, test_envs, exploration_noise=True)\n",
    "     \n",
    "    # print(\"Buffer Warming Up \")\n",
    "    \n",
    "    # for i in range(10):#int(trainer_params['batch_size'] / (300 * 10 ) )):\n",
    "        \n",
    "    #     train_collector.collect(n_episode=train_env_num)#,random=True) #trainer_params['batch_size'] * train_env_num))\n",
    "    #     #train_collector.collect(n_step=300 * 10)\n",
    "    #     print(\".\", end=\"\") \n",
    "    \n",
    "    # print(\"\\nBuffer Lenght: \", len(train_collector.buffer)/ 150 ) \n",
    "    #train_collector.collect(n_episode=trainer_params['batch_size'])\n",
    "    #test_collector.collect(n_episode=2 )\n",
    "    #test_collector.collect(n_step=trainer_params['batch size'] * train_env_num)\n",
    "    \n",
    "    # ======== tensorboard logging setup =========\n",
    "    #         \n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"Config\", str(Run_Data))\n",
    "    #if same_policy:\n",
    "    #    writer.add_text(\"Model\", str(policy.policies[agents[0]].model).replace('\\n', '  \\n'))    \n",
    "    #else:\n",
    "    #     writer.add_text(\"ModelR\", str(policy.policies['R_agent0'].model).replace('\\n', '  \\n'))\n",
    "    #     writer.add_text(\"ModelF\", str(policy.policies['F_agent0'].model).replace('\\n', '  \\n'))\n",
    "    \n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    global_step_holder = [0]  \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        if same_policy:             \n",
    "            torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \".pth\")\n",
    "            print(\"Best Saved\")\n",
    "        else:\n",
    "            torch.save(policy.policies['R_agent0'].state_dict(), model_save_path + \"R.pth\")\n",
    "            torch.save(policy.policies['F_agent0'].state_dict(), model_save_path + \"F.pth\")\n",
    "            print(\"Bests Saved\")\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 9939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])  \n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "        #print(rews)  \n",
    "        global_step_holder[0] += 1    \n",
    "\n",
    "        #if rews[:,0].mean() != 0:\n",
    "        #    print( rews)\n",
    "        return rews[:,0]\n",
    "\n",
    "\n",
    "    #Define the hook function\n",
    "    def register_activation_hook(module, input, output, layer_name, writer, global_step_holder):\n",
    "        #print(f\"Hook executed for {layer_name} at step {global_step_holder[0]}\")\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # If the output is a tuple, use the first element\n",
    "        writer.add_histogram(f\"activations/{layer_name}\", output, global_step_holder[0])\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function0 = partial(register_activation_hook, layer_name=\"task_embeddings\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook0 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function0)\n",
    "\n",
    "    #Register the hook\n",
    "    # hook_function1 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    # hook1 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function1)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function2 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook2 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function2)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function3 = partial(register_activation_hook, layer_name=\"tasks_info\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook3 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function3)\n",
    "        \n",
    "    #Add Logger Details\n",
    "    def log_gradients(policy, writer, global_step_holder, **kwargs):\n",
    "        for name, param in policy.model.named_parameters():\n",
    "            writer.add_histogram(f\"{name}.grad\", param.grad, global_step_holder[0])    \n",
    "\n",
    "    #Modify the hook definition to pass the writer and global_step_holder\n",
    "    # policy.policies['agent0'].post_optim_hook = partial(log_gradients, writer=writer, global_step_holder=global_step_holder)\n",
    "        \n",
    "   \n",
    "    def condensed_make_dot(var, params=None):\n",
    "        dot = make_dot(var, params)\n",
    "        \n",
    "        # Here's where you'd condense or modify the graph.\n",
    "        # For example, to remove all nodes related to ReLU operations:\n",
    "        # (This is just a conceptual example. You'd modify this to fit your needs.)\n",
    "        nodes_to_remove = [n for n in dot.body if 'Relu' in n]\n",
    "        nodes_to_remove += [n for n in dot.body if 'Accumu' in n]       \n",
    "        for n in nodes_to_remove:\n",
    "            dot.body.remove(n)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    if False:\n",
    "\n",
    "        dummy_input = generate_dummy_observation()    \n",
    "        output = policy.policies['agent0'].model(dummy_input)     \n",
    "        \n",
    "        dot = make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "        # Save as .png\n",
    "        dot.format = 'svg'\n",
    "        dot.render(filename='model_architecture', directory=log_path, cleanup=True)\n",
    "\n",
    "    # dot = condensed_make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "    # # Save as .png\n",
    "    # dot.format = 'svg'\n",
    "    # dot.render(filename='model_architecture_condensed', directory=log_path, cleanup=True)\n",
    "\n",
    "    \n",
    "    # policy.policies[agents[0]].set_eps(0.8)\n",
    "    \n",
    "    # for i in range(int(15000)):  # total step\n",
    "        \n",
    "    #     collect_result = train_collector.collect(n_step=450)\n",
    "\n",
    "    #     # or every 1000 steps, we test it on test_collector\n",
    "    #     if collect_result['rews'].mean() >= 10 or i % 1500 == 0:\n",
    "    #         policy.policies[agents[0]].set_eps(0.0)\n",
    "            \n",
    "    #         result = test_collector.collect(n_episode=1)\n",
    "            \n",
    "    #         if result['rews'].mean() >= 10:\n",
    "    #             print(f'Finished training! Test mean returns: {result[\"rews\"].mean()}')\n",
    "    #             break\n",
    "    #         else:\n",
    "    #             # back to training eps\n",
    "    #             policy.policies[agents[0]].set_eps(0.8)\n",
    "\n",
    "    #     # train policy with a sampled batch data from buffer\n",
    "    #     losses = policy.policies[agents[0]].update(64, train_collector.buffer)\n",
    "    #     print(losses)\n",
    "\n",
    "\n",
    "    # # ======== Step 5: Run the trainer =========   \n",
    "    result = OnpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,\n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        repeat_per_collect=10, #TODO: understand\n",
    "        episode_per_test=trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger,\n",
    "    ).run()\n",
    "    assert stop_fn(result[\"best_reward\"])     \n",
    "\n",
    "    writer.close()\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "import torch\n",
    "\n",
    "import mUAV_TA.MultiDroneEnvUtils as utils\n",
    "#from Custom_Classes import CustomCollector\n",
    "\n",
    "def _get_env_eval():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    case =  {'case' : 0, 'F1':2, 'F2': 2, \"R1\" : 12, 'R2' : 3, \"Att\" : 4, \"Rec\" : 22}\n",
    "\n",
    "    config = utils.agentEnvOptions( \n",
    "                 render_mode = 'human',                  \n",
    "                 render_speed=1,\n",
    "                 simulation_frame_rate = 0.01, \n",
    "                 action_mode=\"TaskAssign\",\n",
    "                 simulator_module = \"Internal\", \n",
    "                 max_time_steps=200, \n",
    "                 agents= {\"F1\" : 4, \"F2\" : 2, \"R1\" : 6},                 \n",
    "                 tasks= { \"Att\" : 4 , \"Rec\" : 16, \"Hold\" : 4},\n",
    "                 multiple_tasks_per_agent = False,\n",
    "                 multiple_agents_per_task = True,\n",
    "                 random_init_pos=False,\n",
    "                 num_obstacles=0,\n",
    "                 hidden_obstacles = False,\n",
    "                 fail_rate = 0.0,\n",
    "                 info = \"No Info\")\n",
    "   \n",
    "    \n",
    "    env_paralell = MultiUAVEnv()\n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "\n",
    "# Create a new instance of the policy with the same architecture as the saved policy\n",
    "name = 'policy_CustomNetMultiHead_Eval_TBTA_Relative_Representation_01.pth' \n",
    "load_policy_name = f'policy_{name}'\n",
    "\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", name)\n",
    "\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "\n",
    "# Load the saved checkpoint\n",
    "policy_test = policy.policies['agent0']\n",
    "policy_test.load_state_dict(torch.load(model_save_path ))\n",
    "\n",
    "envs = DummyVectorEnv([_get_env_eval for _ in range(1)])\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "#collector = CustomCollector(policy.policies['agent0'], envs, exploration_noise=True)\n",
    "#collector = CustomCollector(policy_test, envs, exploration_noise=False)\n",
    "collector = CustomCollector(policy, envs, exploration_noise=True)\n",
    "\n",
    "#results = collector.collect(n_episode=1)\n",
    "results = collector.collect(n_episode=1)#, gym_reset_kwargs={'seed' :2})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['rews']\n",
    "print(np.mean(results['rews'][results['rews'] > -10]))\n",
    "\n",
    "\n",
    "#create a function  to print a histogram of the results['rews']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(results['rews'][results['rews'] > -10], bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import st\n",
    "import torch\n",
    "from tianshou.data import Batch\n",
    "\n",
    "# load policy as in your original code\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "policy_test = policy.policies['agent0']\n",
    "state_saved = torch.load(model_save_path )\n",
    "#print(policy_test)\n",
    "policy_test.load_state_dict(state_saved)\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "# initialize your environment\n",
    "#env = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "env = MultiDroneEnv(None)\n",
    "env.max_time_steps = 50\n",
    "\n",
    "# simulate the interaction with the environment manually\n",
    "for i in range(10):\n",
    "    for episode in range(1):  # simulate 10 episodes\n",
    "        \n",
    "        #env.render_speed = 1\n",
    "        obs, _  = env.reset(seed=episode)         \n",
    "        info         = env.get_initial_state()\n",
    "        \n",
    "        drones = info[\"drones\"]\n",
    "        tasks = info[\"tasks\"]\n",
    "            \n",
    "        done = {0 : False}\n",
    "        truncations = {0 : False}\n",
    "        \n",
    "        episodo_reward = 0\n",
    "        #obs, reward, done, truncations, info = env.step(action)\n",
    "\n",
    "        while not all(done.values()) and not all(truncations.values()):\n",
    "            \n",
    "            agent_id = \"agent\" + str(env.agent_selector._current_agent)\n",
    "            # Create a Batch of observations\n",
    "            obs_batch = Batch(obs=[obs[agent_id]], info=[{}])  # add empty info for each observation\n",
    "            \n",
    "            #print(obs_batch)\n",
    "            # Forward the batch of observations through the policy to get the actions\n",
    "            action = policy_test(obs_batch).act\n",
    "            action = {agent_id : action[0]}\n",
    "        \n",
    "            obs, reward, done, truncations, info = env.step(action)\n",
    "            \n",
    "            episodo_reward += sum(reward.values())/env.n_agents\n",
    "\n",
    "        \n",
    "\n",
    "    print(episodo_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
