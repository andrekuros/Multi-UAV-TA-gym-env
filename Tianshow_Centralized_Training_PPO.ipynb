{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomNetMultiHead_PPO_OCT01 \n",
      "Policy: PPO \n",
      "Loaded_Model: PPO_CustomNetMultiHead_PPO_OCT01.pth  \n",
      "log_path: ./Logs\\dqn\\CustomNetMultiHead_PPO_OCT01231110-200147  \n",
      "train/test_env_num: 10 / 10  \n",
      "model: CustomNetMultiHead  \n",
      "dqn_params: {'discount_factor': 0.99, 'estimation_step': 150, 'target_update_freq': 9000, 'optminizer': 'Adam', 'lr': 0.0001}  \n",
      "trainer_params: {'max_epoch': 500, 'step_per_epoch': 4500, 'step_per_collect': 1500, 'episode_per_test': 10, 'batch_size': 900, 'update_per_step': 0.0033333333333333335, 'tn_eps_max': 0.8, 'ts_eps_max': 0.0} \n",
      "single_policy: True\n",
      "\n",
      "--------- Env ------------  \n",
      "\n",
      "Rewards Only Final Quality and SQuality\n",
      "random_init_pos      : False\n",
      "max_time_steps       : 150\n",
      "simulation_frame_rate: 0.01\n",
      "Agents               : {'F1': 0, 'F2': 2, 'R1': 0, 'R2': 0}\n",
      "tasks                : {'Att': 15, 'Rec': 0, 'Hold': 0}\n",
      "random_init_pos      : False \n",
      "threats              : []\n",
      "seed                 : -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Dict, Discrete, MultiDiscrete, Box\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy, RainbowPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "\n",
    "from tianshou.utils.net.common import ActorCritic, DataParallelNet, Net\n",
    "from tianshou.utils.net.discrete import Actor, Critic\n",
    "\n",
    "#from torchviz import make_dot\n",
    "\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomCollector\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "#from CustomClass_multi_head import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes_simplified import CustomNetSimple\n",
    "#from Custom_Classes_simplified import CustomCollectorSimple\n",
    "#from Custom_Classes_simplified import CustomParallelToAECWrapperSimple\n",
    "\n",
    "from TaskAllocation.RL_Policies.CustomClass_MultiHead_Transformer import CustomNetMultiHead\n",
    "from TaskAllocation.RL_Policies.CustomClass_MultiHead_Transformer_PPO_Critic import CriticNetMultiHead\n",
    "\n",
    "from mUAV_TA.MultiDroneEnvUtils import agentEnvOptions\n",
    "\n",
    "from mUAV_TA.DroneEnv import MultiUAVEnv\n",
    "#from tianshou_DQN import train\n",
    "model = \"CustomNetMultiHead\" # \"CustomNet\" or \"CustomNetSimple\" or \"CustomNetReduced\" or \"CustomNetMultiHead\"\n",
    "test_num = \"_PPO_OCT01\"\n",
    "policyModel = \"PPO\"\n",
    "\n",
    "train_env_num = 10\n",
    "test_env_num = 10\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "load_policy_name = f'PPO_CustomNetMultiHead_PPO_OCT01.pth'\n",
    "save_policy_name = f'PPO_{name}'\n",
    "policy_path = \"ppo_Custom\"\n",
    "\n",
    "same_policy = True\n",
    "\n",
    "load_model = True\n",
    "\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", log_name)\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.99, \n",
    "              \"estimation_step\": 150, \n",
    "              \"target_update_freq\": 150 * 30 * 2,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 1e-4 }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 500,\n",
    "                  \"step_per_epoch\": 150 * 30,\n",
    "                  \"step_per_collect\": 150 * 10,\n",
    "                  \"episode_per_test\": 10,\n",
    "                  \"batch_size\" : 900,\n",
    "                  \"update_per_step\": 1 / 300, #Only run after close a Collect (run many times as necessary to meet the value)\n",
    "                  \"tn_eps_max\": 0.80,\n",
    "                  \"ts_eps_max\": 0.0,\n",
    "                  }\n",
    "\n",
    "config_default = agentEnvOptions(                                        \n",
    "                 render_speed=-1,\n",
    "                 simulation_frame_rate = 0.01, \n",
    "                 action_mode=\"TaskAssign\",\n",
    "                 simulator_module = \"Internal\", \n",
    "                 max_time_steps=150, \n",
    "                 agents= {\"F1\" : 0, \"F2\" : 2, \"R1\" : 0, \"R2\" : 0},                 \n",
    "                 tasks= { \"Att\" : 15 , \"Rec\" : 0, \"Hold\" : 0},\n",
    "                 multiple_tasks_per_agent = False,\n",
    "                 multiple_agents_per_task = True,\n",
    "                 random_init_pos=False,\n",
    "                 num_obstacles=0,\n",
    "                 hidden_obstacles = False,\n",
    "                 fail_rate = 0.0,\n",
    "                 threats_list = [],#[(\"T1\", 4), (\"T2\" , 2)],\n",
    "                 fixed_seed = -1,\n",
    "                 info = \"No Info\")    \n",
    "\n",
    "Run_Data = f'''{name} \n",
    "Policy: {policyModel} \n",
    "Loaded_Model: {load_policy_name if load_model else \"no\"}  \n",
    "log_path: {log_path}  \n",
    "train/test_env_num: {train_env_num} / {test_env_num}  \n",
    "model: {model}  \n",
    "dqn_params: {dqn_params}  \n",
    "trainer_params: {trainer_params} \n",
    "single_policy: {same_policy}\n",
    "\n",
    "--------- Env ------------  \n",
    "\n",
    "Rewards Only Final Quality and SQuality\n",
    "random_init_pos      : {config_default.random_init_pos}\n",
    "max_time_steps       : {config_default.max_time_steps}\n",
    "simulation_frame_rate: {config_default.simulation_frame_rate}\n",
    "Agents               : {config_default.agents}\n",
    "tasks                : {config_default.tasks}\n",
    "random_init_pos      : {config_default.random_init_pos} \n",
    "threats              : {config_default.threats_list}\n",
    "seed                 : {config_default.fixed_seed}\n",
    "'''\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "def generate_dummy_observation(batch_size=1, sequence_length=31, feature_dim=12):\n",
    "    # Generate a random tensor with the given shape\n",
    "    dummy_obs = torch.randn(batch_size, sequence_length, feature_dim)\n",
    "\n",
    "    return dummy_obs\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()\n",
    "    agent_name = env.agents[0]  # Get the name of the first agent\n",
    "    \n",
    "    #print(env.observation_space )\n",
    "    agent_observation_space = env.observation_space # assuming 'agent0' is a valid agent name\n",
    "    state_shape_agent_position = agent_observation_space[\"agent_position\"].shape[0]\n",
    "    state_shape_agent_state = agent_observation_space[\"agent_state\"].shape[0]\n",
    "    state_shape_agent_type = agent_observation_space[\"agent_type\"].shape[0]\n",
    "    state_shape_next_free_time = agent_observation_space[\"next_free_time\"].shape[0]\n",
    "    state_shape_position_after_last_task = agent_observation_space[\"position_after_last_task\"].shape[0]       \n",
    "    #state_shape_agent_relay_area = agent_observation_space[\"agent_relay_area\"].shape[0]\n",
    "        \n",
    "    state_shape_agent = (state_shape_agent_position + state_shape_agent_state +\n",
    "                     state_shape_agent_type+ state_shape_next_free_time + state_shape_position_after_last_task #+                     \n",
    "                     #state_shape_agent_relay_area\n",
    "                     )                 \n",
    "\n",
    "    state_shape_task = 31 * 13 #env.observation_space[\"tasks_info\"].shape[0]\n",
    "                  \n",
    "    action_shape = env.action_space[agent_name].shape[0]\n",
    "    #action_shape = env.action_space[agent_name].n\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"          \n",
    "    \n",
    "    if agent_learn is None:\n",
    "        # model       \n",
    "        \n",
    "        if model == \"CustomNetMultiHead\":\n",
    "            \n",
    "            netActor = CustomNetMultiHead(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "\n",
    "            netCritic = CriticNetMultiHead(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "                        \n",
    "            if False:#torch.cuda.is_available():\n",
    "                actor = DataParallelNet(Actor(netActor, action_shape, device=None).to(device))\n",
    "                critic = DataParallelNet(Critic(netCritic, device=None).to(device))\n",
    "            else:\n",
    "                actor = Actor(netActor, action_shape, device=device).to(device)\n",
    "                critic = Critic(netCritic, device=device).to(device)\n",
    "            \n",
    "            actor_critic = ActorCritic(actor, critic)\n",
    "        \n",
    "        # orthogonal initialization\n",
    "        # for m in actor_critic.modules():\n",
    "        #     if isinstance(m, torch.nn.Linear):\n",
    "        #         torch.nn.init.orthogonal_(m.weight)\n",
    "        #         torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "        \n",
    "        dist = torch.distributions.Categorical         \n",
    "            \n",
    "        #optim_actor  = torch.optim.Adam(netActor.parameters(),  lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "        #optim_critic = torch.optim.Adam(netCritic.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "        optim = torch.optim.Adam(actor_critic.parameters(), lr=dqn_params[\"lr\"])\n",
    "                \n",
    "        agent_learn = PPOPolicy(\n",
    "            actor=actor,\n",
    "            critic=critic,\n",
    "            optim=optim,\n",
    "            dist_fn=dist,\n",
    "            action_scaling=isinstance(env.action_space, Box),\n",
    "            discount_factor=0.99,\n",
    "            max_grad_norm=0.5,\n",
    "            eps_clip=0.2,\n",
    "            vf_coef=0.5,\n",
    "            ent_coef=0.0,\n",
    "            gae_lambda=0.95,\n",
    "            reward_normalization=0,\n",
    "            dual_clip=None,\n",
    "            value_clip=0,\n",
    "            action_space=env.action_space,\n",
    "            deterministic_eval=True,\n",
    "            advantage_normalization=0,\n",
    "            recompute_advantage=0,\n",
    "        )\n",
    "        \n",
    " \n",
    "        if load_model == True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "                          \n",
    "        \n",
    "        agents = [None for _ in range(len(env.agents))]        \n",
    "        \n",
    "        if not same_policy:\n",
    "\n",
    "            for i,agent in enumerate(env.agents):             \n",
    "                if agent[0] == \"F\":                \n",
    "                    agents[i] = agent_learn2\n",
    "                    #print(\"F\")\n",
    "                else:\n",
    "                    agents[i] = agent_learn\n",
    "                    #print(\"R\")\n",
    "        else:\n",
    "            agents = [agent_learn for _ in range(len(env.agents))]\n",
    "\n",
    "        # print(agents)\n",
    "        # print([o.type for o in agents_obj])\n",
    "\n",
    "\n",
    "        # agent_learn2\n",
    "        \n",
    "    policy = MultiAgentPolicyManager(agents, env)  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    env_paralell = MultiUAVEnv(config=config_default)    \n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "print(Run_Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded-> ppo_Custom\\PPO_CustomNetMultiHead_PPO_OCT01.pth\n",
      "Best Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 4501it [02:36, 28.73it/s, F2_agent0/loss=0.583, F2_agent0/loss/clip=0.368, F2_agent0/loss/ent=3.428, F2_agent0/loss/vf=0.431, F2_agent1/loss=1883.638, F2_agent1/loss/clip=-32.959, F2_agent1/loss/ent=3.428, F2_agent1/loss/vf=3833.193, env_step=4500, len=150, n/ep=10, n/st=1500, rew=146.61]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 105.038368 ± 53.121889, best_reward: 156.605666 ± 119.600587 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 4501it [02:37, 28.55it/s, F2_agent0/loss=0.615, F2_agent0/loss/clip=0.408, F2_agent0/loss/ent=3.428, F2_agent0/loss/vf=0.414, F2_agent1/loss=1698.237, F2_agent1/loss/clip=-32.205, F2_agent1/loss/ent=3.428, F2_agent1/loss/vf=3460.883, env_step=9000, len=150, n/ep=10, n/st=1500, rew=142.65]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 132.456506 ± 82.273259, best_reward: 156.605666 ± 119.600587 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 4501it [03:14, 23.10it/s, F2_agent0/loss=0.629, F2_agent0/loss/clip=0.415, F2_agent0/loss/ent=3.427, F2_agent0/loss/vf=0.428, F2_agent1/loss=1752.439, F2_agent1/loss/clip=-32.735, F2_agent1/loss/ent=3.426, F2_agent1/loss/vf=3570.349, env_step=13500, len=150, n/ep=10, n/st=1500, rew=125.63]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 122.358602 ± 71.063978, best_reward: 156.605666 ± 119.600587 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4:  67%|######6   | 3000/4500 [02:27<01:13, 20.37it/s, F2_agent0/loss=0.627, F2_agent0/loss/clip=0.409, F2_agent0/loss/ent=3.425, F2_agent0/loss/vf=0.435, F2_agent1/loss=1787.856, F2_agent1/loss/clip=-33.143, F2_agent1/loss/ent=3.423, F2_agent1/loss/vf=3641.999, env_step=15000, len=150, n/ep=10, n/st=1500, rew=163.22] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (750, 31)) of distribution Categorical(probs: torch.Size([750, 31])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n       grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\GITHUB\\Multi-UAV-TA-gym-env\\Tianshow_Centralized_Training_PPO.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m     dot\u001b[39m.\u001b[39mrender(filename\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel_architecture\u001b[39m\u001b[39m'\u001b[39m, directory\u001b[39m=\u001b[39mlog_path, cleanup\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m \u001b[39m# dot = condensed_make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39m# # Save as .png\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39m# dot.format = 'svg'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=180'>181</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m \u001b[39m# # ======== Step 5: Run the trainer =========   \u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m result \u001b[39m=\u001b[39m OnpolicyTrainer(\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m     policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=184'>185</a>\u001b[0m     train_collector\u001b[39m=\u001b[39;49mtrain_collector,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m     test_collector\u001b[39m=\u001b[39;49mtest_collector,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m     max_epoch\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mmax_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m     step_per_epoch\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mstep_per_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m     repeat_per_collect\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \u001b[39m#TODO: understand\u001b[39;49;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m     episode_per_test\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mepisode_per_test\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=190'>191</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=191'>192</a>\u001b[0m     step_per_collect\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mstep_per_collect\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=192'>193</a>\u001b[0m     stop_fn\u001b[39m=\u001b[39;49mstop_fn,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m     save_best_fn\u001b[39m=\u001b[39;49msave_best_fn,\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m )\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m \u001b[39massert\u001b[39;00m stop_fn(result[\u001b[39m\"\u001b[39m\u001b[39mbest_reward\u001b[39m\u001b[39m\"\u001b[39m])     \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training_PPO.ipynb#W1sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m writer\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\trainer\\base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[0;32m    443\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[0;32m    444\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[0;32m    445\u001b[0m     )\n\u001b[0;32m    446\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\trainer\\base.py:299\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m         result[\u001b[39m\"\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_step)\n\u001b[0;32m    297\u001b[0m         t\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 299\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_update_fn(data, result)\n\u001b[0;32m    300\u001b[0m     t\u001b[39m.\u001b[39mset_postfix(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata)\n\u001b[0;32m    302\u001b[0m \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\trainer\\onpolicy.py:131\u001b[0m, in \u001b[0;36mOnpolicyTrainer.policy_update_fn\u001b[1;34m(self, data, result)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Perform one on-policy update.\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mupdate(\n\u001b[0;32m    132\u001b[0m     \u001b[39m0\u001b[39;49m,\n\u001b[0;32m    133\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_collector\u001b[39m.\u001b[39;49mbuffer,\n\u001b[0;32m    134\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[0;32m    135\u001b[0m     repeat\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepeat_per_collect,\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    137\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector\u001b[39m.\u001b[39mreset_buffer(keep_statistics\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    138\u001b[0m step \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m([\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m [\u001b[39mlen\u001b[39m(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m losses\u001b[39m.\u001b[39mvalues() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, \u001b[39mlist\u001b[39m)])\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\policy\\base.py:277\u001b[0m, in \u001b[0;36mBasePolicy.update\u001b[1;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    276\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_fn(batch, buffer, indices)\n\u001b[1;32m--> 277\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn(batch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_process_fn(batch, buffer, indices)\n\u001b[0;32m    279\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\policy\\multiagent\\mapolicy.py:196\u001b[0m, in \u001b[0;36mMultiAgentPolicyManager.learn\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m data \u001b[39m=\u001b[39m batch[agent_id]\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data\u001b[39m.\u001b[39mis_empty():\n\u001b[1;32m--> 196\u001b[0m     out \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mlearn(batch\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m out\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    198\u001b[0m         results[agent_id \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m k] \u001b[39m=\u001b[39m v\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\policy\\modelfree\\ppo.py:107\u001b[0m, in \u001b[0;36mPPOPolicy.learn\u001b[1;34m(self, batch, batch_size, repeat, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_returns(batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices)\n\u001b[0;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m minibatch \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39msplit(batch_size, merge_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    106\u001b[0m     \u001b[39m# calculate loss for actor\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(minibatch)\u001b[39m.\u001b[39mdist\n\u001b[0;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_norm_adv:\n\u001b[0;32m    109\u001b[0m         mean, std \u001b[39m=\u001b[39m minibatch\u001b[39m.\u001b[39madv\u001b[39m.\u001b[39mmean(), minibatch\u001b[39m.\u001b[39madv\u001b[39m.\u001b[39mstd()\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\tianshou\\policy\\modelfree\\pg.py:112\u001b[0m, in \u001b[0;36mPGPolicy.forward\u001b[1;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_fn(\u001b[39m*\u001b[39mlogits)\n\u001b[0;32m    111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdist_fn(logits)\n\u001b[0;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic_eval \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdiscrete\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m batch_shape \u001b[39m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[1;32md:\\Python310\\lib\\site-packages\\torch\\distributions\\distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[0;32m     67\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[1;32m---> 68\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m             )\n\u001b[0;32m     75\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (750, 31)) of distribution Categorical(probs: torch.Size([750, 31])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n       grad_fn=<DivBackward0>)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()    \n",
    "\n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = CustomCollector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(100_000, len(train_envs)),\n",
    "        # PrioritizedVectorReplayBuffer( 100_000, len(train_envs), alpha=0.6, beta=0.4) , \n",
    "        #ListReplayBuffer(100000)       \n",
    "        # exploration_noise=True             \n",
    "    )\n",
    "    test_collector = CustomCollector(policy, test_envs, exploration_noise=True)\n",
    "     \n",
    "    # print(\"Buffer Warming Up \")\n",
    "    \n",
    "    # for i in range(10):#int(trainer_params['batch_size'] / (300 * 10 ) )):\n",
    "        \n",
    "    #     train_collector.collect(n_episode=train_env_num)#,random=True) #trainer_params['batch_size'] * train_env_num))\n",
    "    #     #train_collector.collect(n_step=300 * 10)\n",
    "    #     print(\".\", end=\"\") \n",
    "    \n",
    "    # print(\"\\nBuffer Lenght: \", len(train_collector.buffer)/ 150 ) \n",
    "    #train_collector.collect(n_episode=trainer_params['batch_size'])\n",
    "    #test_collector.collect(n_episode=2 )\n",
    "    #test_collector.collect(n_step=trainer_params['batch size'] * train_env_num)\n",
    "    \n",
    "    # ======== tensorboard logging setup =========\n",
    "    #         \n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"Config\", str(Run_Data))\n",
    "    #if same_policy:\n",
    "    #    writer.add_text(\"Model\", str(policy.policies[agents[0]].model).replace('\\n', '  \\n'))    \n",
    "    #else:\n",
    "    #     writer.add_text(\"ModelR\", str(policy.policies['R_agent0'].model).replace('\\n', '  \\n'))\n",
    "    #     writer.add_text(\"ModelF\", str(policy.policies['F_agent0'].model).replace('\\n', '  \\n'))\n",
    "    \n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    global_step_holder = [0]  \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        if same_policy:             \n",
    "            torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \".pth\")\n",
    "            print(\"Best Saved\")\n",
    "        else:\n",
    "            torch.save(policy.policies['R_agent0'].state_dict(), model_save_path + \"R.pth\")\n",
    "            torch.save(policy.policies['F_agent0'].state_dict(), model_save_path + \"F.pth\")\n",
    "            print(\"Bests Saved\")\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 9939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])  \n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "        #print(rews)  \n",
    "        global_step_holder[0] += 1    \n",
    "\n",
    "        #if rews[:,0].mean() != 0:\n",
    "        #    print( rews)\n",
    "        return rews[:,0]\n",
    "\n",
    "\n",
    "    #Define the hook function\n",
    "    def register_activation_hook(module, input, output, layer_name, writer, global_step_holder):\n",
    "        #print(f\"Hook executed for {layer_name} at step {global_step_holder[0]}\")\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # If the output is a tuple, use the first element\n",
    "        writer.add_histogram(f\"activations/{layer_name}\", output, global_step_holder[0])\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function0 = partial(register_activation_hook, layer_name=\"task_embeddings\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook0 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function0)\n",
    "\n",
    "    #Register the hook\n",
    "    # hook_function1 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    # hook1 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function1)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function2 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook2 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function2)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function3 = partial(register_activation_hook, layer_name=\"tasks_info\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook3 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function3)\n",
    "        \n",
    "    #Add Logger Details\n",
    "    def log_gradients(policy, writer, global_step_holder, **kwargs):\n",
    "        for name, param in policy.model.named_parameters():\n",
    "            writer.add_histogram(f\"{name}.grad\", param.grad, global_step_holder[0])    \n",
    "\n",
    "    #Modify the hook definition to pass the writer and global_step_holder\n",
    "    # policy.policies['agent0'].post_optim_hook = partial(log_gradients, writer=writer, global_step_holder=global_step_holder)\n",
    "        \n",
    "   \n",
    "    def condensed_make_dot(var, params=None):\n",
    "        dot = make_dot(var, params)\n",
    "        \n",
    "        # Here's where you'd condense or modify the graph.\n",
    "        # For example, to remove all nodes related to ReLU operations:\n",
    "        # (This is just a conceptual example. You'd modify this to fit your needs.)\n",
    "        nodes_to_remove = [n for n in dot.body if 'Relu' in n]\n",
    "        nodes_to_remove += [n for n in dot.body if 'Accumu' in n]       \n",
    "        for n in nodes_to_remove:\n",
    "            dot.body.remove(n)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    if False:\n",
    "\n",
    "        dummy_input = generate_dummy_observation()    \n",
    "        output = policy.policies['agent0'].model(dummy_input)     \n",
    "        \n",
    "        dot = make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "        # Save as .png\n",
    "        dot.format = 'svg'\n",
    "        dot.render(filename='model_architecture', directory=log_path, cleanup=True)\n",
    "\n",
    "    # dot = condensed_make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "    # # Save as .png\n",
    "    # dot.format = 'svg'\n",
    "    # dot.render(filename='model_architecture_condensed', directory=log_path, cleanup=True)\n",
    "\n",
    "    \n",
    "    # policy.policies[agents[0]].set_eps(0.8)\n",
    "    \n",
    "    # for i in range(int(15000)):  # total step\n",
    "        \n",
    "    #     collect_result = train_collector.collect(n_step=450)\n",
    "\n",
    "    #     # or every 1000 steps, we test it on test_collector\n",
    "    #     if collect_result['rews'].mean() >= 10 or i % 1500 == 0:\n",
    "    #         policy.policies[agents[0]].set_eps(0.0)\n",
    "            \n",
    "    #         result = test_collector.collect(n_episode=1)\n",
    "            \n",
    "    #         if result['rews'].mean() >= 10:\n",
    "    #             print(f'Finished training! Test mean returns: {result[\"rews\"].mean()}')\n",
    "    #             break\n",
    "    #         else:\n",
    "    #             # back to training eps\n",
    "    #             policy.policies[agents[0]].set_eps(0.8)\n",
    "\n",
    "    #     # train policy with a sampled batch data from buffer\n",
    "    #     losses = policy.policies[agents[0]].update(64, train_collector.buffer)\n",
    "    #     print(losses)\n",
    "\n",
    "\n",
    "    # # ======== Step 5: Run the trainer =========   \n",
    "    result = OnpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,\n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        repeat_per_collect=10, #TODO: understand\n",
    "        episode_per_test=trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger,\n",
    "    ).run()\n",
    "    assert stop_fn(result[\"best_reward\"])     \n",
    "\n",
    "    writer.close()\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "import torch\n",
    "\n",
    "import mUAV_TA.MultiDroneEnvUtils as utils\n",
    "#from Custom_Classes import CustomCollector\n",
    "\n",
    "def _get_env_eval():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    case =  {'case' : 0, 'F1':2, 'F2': 2, \"R1\" : 12, 'R2' : 3, \"Att\" : 4, \"Rec\" : 22}\n",
    "\n",
    "    config = utils.agentEnvOptions( \n",
    "                 render_mode = 'human',                  \n",
    "                 render_speed=1,\n",
    "                 simulation_frame_rate = 0.01, \n",
    "                 action_mode=\"TaskAssign\",\n",
    "                 simulator_module = \"Internal\", \n",
    "                 max_time_steps=200, \n",
    "                 agents= {\"F1\" : 4, \"F2\" : 2, \"R1\" : 6},                 \n",
    "                 tasks= { \"Att\" : 4 , \"Rec\" : 16, \"Hold\" : 4},\n",
    "                 multiple_tasks_per_agent = False,\n",
    "                 multiple_agents_per_task = True,\n",
    "                 random_init_pos=False,\n",
    "                 num_obstacles=0,\n",
    "                 hidden_obstacles = False,\n",
    "                 fail_rate = 0.0,\n",
    "                 info = \"No Info\")\n",
    "   \n",
    "    \n",
    "    env_paralell = MultiUAVEnv()\n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "\n",
    "# Create a new instance of the policy with the same architecture as the saved policy\n",
    "name = 'policy_CustomNetMultiHead_Eval_TBTA_Relative_Representation_01.pth' \n",
    "load_policy_name = f'policy_{name}'\n",
    "\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", name)\n",
    "\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "\n",
    "# Load the saved checkpoint\n",
    "policy_test = policy.policies['agent0']\n",
    "policy_test.load_state_dict(torch.load(model_save_path ))\n",
    "\n",
    "envs = DummyVectorEnv([_get_env_eval for _ in range(1)])\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "#collector = CustomCollector(policy.policies['agent0'], envs, exploration_noise=True)\n",
    "#collector = CustomCollector(policy_test, envs, exploration_noise=False)\n",
    "collector = CustomCollector(policy, envs, exploration_noise=True)\n",
    "\n",
    "#results = collector.collect(n_episode=1)\n",
    "results = collector.collect(n_episode=1)#, gym_reset_kwargs={'seed' :2})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['rews']\n",
    "print(np.mean(results['rews'][results['rews'] > -10]))\n",
    "\n",
    "\n",
    "#create a function  to print a histogram of the results['rews']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(results['rews'][results['rews'] > -10], bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import st\n",
    "import torch\n",
    "from tianshou.data import Batch\n",
    "\n",
    "# load policy as in your original code\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "policy_test = policy.policies['agent0']\n",
    "state_saved = torch.load(model_save_path )\n",
    "#print(policy_test)\n",
    "policy_test.load_state_dict(state_saved)\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "# initialize your environment\n",
    "#env = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "env = MultiDroneEnv(None)\n",
    "env.max_time_steps = 50\n",
    "\n",
    "# simulate the interaction with the environment manually\n",
    "for i in range(10):\n",
    "    for episode in range(1):  # simulate 10 episodes\n",
    "        \n",
    "        #env.render_speed = 1\n",
    "        obs, _  = env.reset(seed=episode)         \n",
    "        info         = env.get_initial_state()\n",
    "        \n",
    "        drones = info[\"drones\"]\n",
    "        tasks = info[\"tasks\"]\n",
    "            \n",
    "        done = {0 : False}\n",
    "        truncations = {0 : False}\n",
    "        \n",
    "        episodo_reward = 0\n",
    "        #obs, reward, done, truncations, info = env.step(action)\n",
    "\n",
    "        while not all(done.values()) and not all(truncations.values()):\n",
    "            \n",
    "            agent_id = \"agent\" + str(env.agent_selector._current_agent)\n",
    "            # Create a Batch of observations\n",
    "            obs_batch = Batch(obs=[obs[agent_id]], info=[{}])  # add empty info for each observation\n",
    "            \n",
    "            #print(obs_batch)\n",
    "            # Forward the batch of observations through the policy to get the actions\n",
    "            action = policy_test(obs_batch).act\n",
    "            action = {agent_id : action[0]}\n",
    "        \n",
    "            obs, reward, done, truncations, info = env.step(action)\n",
    "            \n",
    "            episodo_reward += sum(reward.values())/env.n_agents\n",
    "\n",
    "        \n",
    "\n",
    "    print(episodo_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
