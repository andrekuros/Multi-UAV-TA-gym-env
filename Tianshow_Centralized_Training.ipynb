{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gc import collect\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "\n",
    "from Custom_Classes import CustomNet\n",
    "from Custom_Classes import CustomCollector\n",
    "from Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "#from CustomClass_multi_head import CustomNet\n",
    "from Custom_Classes_simplified import CustomNetSimple\n",
    "#from Custom_Classes_simplified import CustomCollectorSimple\n",
    "#from Custom_Classes_simplified import CustomParallelToAECWrapperSimple\n",
    "\n",
    "from CustomClasses_Transformer_Reduced import CustomNetReduced\n",
    "import importlib\n",
    "\n",
    "from DroneEnv import MultiDroneEnv\n",
    "from tianshou_DQN import train\n",
    "\n",
    "\n",
    "model = \"CustomNetReduced\" # \"CustomNet\" or \"CustomNetSimple\" or \"CustomNetReduced\"\n",
    "test_num = \"1805_Transformer_01_Multi\"\n",
    "\n",
    "train_env_num = 10\n",
    "test_env_num = 10\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "load_policy_name = f'policy_CustomNetSimple1605_01_1_Priorized_1605_01_1_Priorized.pth'\n",
    "save_policy_name = f'policy_{name}.pth'\n",
    "policy_path = \"dqn_Custom\"\n",
    "load_model = False\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", name)\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.99, \n",
    "              \"estimation_step\": 1, \n",
    "              \"target_update_freq\": 100,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 1e-4  }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 200,\n",
    "                  \"step_per_epoch\": 100 * train_env_num,\n",
    "                  \"step_per_collect\": 50 * train_env_num,\n",
    "                  \"episode_per_test\": 100 * test_env_num,\n",
    "                  \"batch_size\" : 32,\n",
    "                  \"update_per_step\": 0.1,\n",
    "                  \"tn_eps_max\": 0.7,\n",
    "                  \"ts_eps_max\": 0.001,\n",
    "                  }\n",
    "\n",
    "Run_Data = f'{name}\\n\\\n",
    "        Loaded_Model: {load_policy_name if load_model == True else \"no\"} \\n\\\n",
    "        log_path: {log_path} \\n\\\n",
    "        train/test_env_num: {train_env_num} / {test_env_num} \\n\\\n",
    "        model: {model} \\n\\\n",
    "        dqn_params: {dqn_params} \\n\\\n",
    "        trainer_params: {trainer_params} \\n\\\n",
    "        obs: Task Info -> Dist | Quality for each drone '\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()\n",
    "    agent_name = env.agents[0]  # Get the name of the first agent\n",
    "           \n",
    "    agent_observation_space = env.observation_space('agent0') # assuming 'agent0' is a valid agent name\n",
    "    state_shape_agent_position = agent_observation_space[\"agent_position\"].shape[0]\n",
    "    state_shape_agent_state = agent_observation_space[\"agent_state\"].shape[0]\n",
    "    state_shape_agent_type = agent_observation_space[\"agent_type\"].shape[0]\n",
    "    state_shape_next_free_time = agent_observation_space[\"next_free_time\"].shape[0]\n",
    "    state_shape_position_after_last_task = agent_observation_space[\"position_after_last_task\"].shape[0]       \n",
    "    #state_shape_agent_relay_area = agent_observation_space[\"agent_relay_area\"].shape[0]\n",
    "    \n",
    "    \n",
    "    state_shape_agent = (state_shape_agent_position + state_shape_agent_state +\n",
    "                     state_shape_agent_type+ state_shape_next_free_time + state_shape_position_after_last_task #+                     \n",
    "                     #state_shape_agent_relay_area\n",
    "                     )                 \n",
    "    \n",
    "\n",
    "    state_shape_task = env.observation_space('agent0')[\"tasks_info\"].shape[0]\n",
    "                  \n",
    "    action_shape = env.action_space[agent_name].shape[0]\n",
    "    #action_shape = env.action_space[agent_name].n\n",
    "               \n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        if model == \"CustomNet\":        \n",
    "            net = CustomNet(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if model == \"CustomNetSimple\":\n",
    "            net = CustomNetSimple(            \n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if model == \"CustomNetReduced\":\n",
    "            net = CustomNetReduced(            \n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"])\n",
    "    \n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor= dqn_params[\"discount_factor\"],\n",
    "            estimation_step=dqn_params[\"estimation_step\"],\n",
    "            target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "        )  \n",
    "        \n",
    "        if load_model == True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "            \n",
    "        \n",
    "        agents = [agent_learn for _ in range(len(env.agents))]\n",
    "        \n",
    "    policy = MultiAgentPolicyManager(agents, env)    \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    env_paralell = MultiDroneEnv()\n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "print(Run_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m rews\u001b[39m.\u001b[39mmean()\u001b[39m#[:,0]\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m# ======== Step 5: Run the trainer =========\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m result \u001b[39m=\u001b[39m offpolicy_trainer(\n\u001b[0;32m     60\u001b[0m     policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m     61\u001b[0m     train_collector\u001b[39m=\u001b[39;49mtrain_collector,\n\u001b[0;32m     62\u001b[0m     test_collector\u001b[39m=\u001b[39;49mtest_collector,        \n\u001b[0;32m     63\u001b[0m     max_epoch\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mmax_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     64\u001b[0m     step_per_epoch\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mstep_per_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     65\u001b[0m     step_per_collect\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mstep_per_collect\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     66\u001b[0m     episode_per_test\u001b[39m=\u001b[39;49m trainer_params[\u001b[39m'\u001b[39;49m\u001b[39mepisode_per_test\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     67\u001b[0m     batch_size\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     68\u001b[0m     train_fn\u001b[39m=\u001b[39;49mtrain_fn,\n\u001b[0;32m     69\u001b[0m     test_fn\u001b[39m=\u001b[39;49mtest_fn,\n\u001b[0;32m     70\u001b[0m     stop_fn\u001b[39m=\u001b[39;49mstop_fn,\n\u001b[0;32m     71\u001b[0m     save_best_fn\u001b[39m=\u001b[39;49msave_best_fn,\n\u001b[0;32m     72\u001b[0m     update_per_step\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mupdate_per_step\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     73\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m     74\u001b[0m     test_in_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m     reward_metric\u001b[39m=\u001b[39;49mreward_metric,\n\u001b[0;32m     76\u001b[0m     show_progress \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     77\u001b[0m     )\n\u001b[0;32m     79\u001b[0m \u001b[39m# return result, policy.policies[agents[1]]\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m==========Result==========\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py:134\u001b[0m, in \u001b[0;36moffpolicy_trainer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moffpolicy_trainer\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m]]:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Wrapper for OffPolicyTrainer run method.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[39m    It is identical to ``OffpolicyTrainer(...).run()``.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[39m    :return: See :func:`~tianshou.trainer.gather_info`.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m OffpolicyTrainer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\trainer\\base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[0;32m    443\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[0;32m    444\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[0;32m    445\u001b[0m     )\n\u001b[0;32m    446\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\trainer\\base.py:315\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39m# test\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     test_stat, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_step()\n\u001b[0;32m    316\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run:\n\u001b[0;32m    317\u001b[0m         epoch_stat\u001b[39m.\u001b[39mupdate(test_stat)\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\trainer\\base.py:344\u001b[0m, in \u001b[0;36mBaseTrainer.test_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    343\u001b[0m stop_fn_flag \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 344\u001b[0m test_result \u001b[39m=\u001b[39m test_episode(\n\u001b[0;32m    345\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_collector, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch,\n\u001b[0;32m    346\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisode_per_test, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_metric\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m rew, rew_std \u001b[39m=\u001b[39m test_result[\u001b[39m\"\u001b[39m\u001b[39mrew\u001b[39m\u001b[39m\"\u001b[39m], test_result[\u001b[39m\"\u001b[39m\u001b[39mrew_std\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    349\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_epoch \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward \u001b[39m<\u001b[39m rew:\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\trainer\\utils.py:27\u001b[0m, in \u001b[0;36mtest_episode\u001b[1;34m(policy, collector, test_fn, epoch, n_episode, logger, global_step, reward_metric)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m test_fn:\n\u001b[0;32m     26\u001b[0m     test_fn(epoch, global_step)\n\u001b[1;32m---> 27\u001b[0m result \u001b[39m=\u001b[39m collector\u001b[39m.\u001b[39;49mcollect(n_episode\u001b[39m=\u001b[39;49mn_episode)\n\u001b[0;32m     28\u001b[0m \u001b[39mif\u001b[39;00m reward_metric:\n\u001b[0;32m     29\u001b[0m     rew \u001b[39m=\u001b[39m reward_metric(result[\u001b[39m\"\u001b[39m\u001b[39mrews\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\Projetos\\Multi-UAV-TA\\Multi-UAV-TA-gym-env\\Custom_Classes.py:174\u001b[0m, in \u001b[0;36mCustomCollector.collect\u001b[1;34m(self, n_step, n_episode, random, render)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollect\u001b[39m(\u001b[39mself\u001b[39m, n_step: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, n_episode: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, random: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, render: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m    173\u001b[0m     \u001b[39m#print(\"CustomCollector collect method called\")\u001b[39;00m\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcollect(n_step\u001b[39m=\u001b[39;49mn_step, n_episode\u001b[39m=\u001b[39;49mn_episode, random\u001b[39m=\u001b[39;49mrandom, render\u001b[39m=\u001b[39;49mrender)\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\data\\collector.py:304\u001b[0m, in \u001b[0;36mCollector.collect\u001b[1;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m action_remap \u001b[39m=\u001b[39m [{agent_id[\u001b[39m0\u001b[39m]: v} \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m action_remap]\n\u001b[0;32m    302\u001b[0m \u001b[39m#agent_index = np.nonzero(result.out == agent_id)[0]\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m obs_next, rew, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m    305\u001b[0m     action_remap,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    306\u001b[0m     ready_env_ids\n\u001b[0;32m    307\u001b[0m )\n\u001b[0;32m    308\u001b[0m done \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlogical_or(terminated, truncated)\n\u001b[0;32m    310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m    311\u001b[0m     obs_next\u001b[39m=\u001b[39mobs_next,\n\u001b[0;32m    312\u001b[0m     rew\u001b[39m=\u001b[39mrew,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m     info\u001b[39m=\u001b[39minfo\n\u001b[0;32m    317\u001b[0m )\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\env\\venvs.py:341\u001b[0m, in \u001b[0;36mBaseVectorEnv.step\u001b[1;34m(self, action, id)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(action) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mid\u001b[39m)\n\u001b[0;32m    340\u001b[0m \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mid\u001b[39m):\n\u001b[1;32m--> 341\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mworkers[j]\u001b[39m.\u001b[39;49msend(action[i])\n\u001b[0;32m    342\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m    343\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mid\u001b[39m:\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\env\\worker\\dummy.py:38\u001b[0m, in \u001b[0;36mDummyEnvWorker.send\u001b[1;34m(self, action, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresult \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\env\\pettingzoo_env.py:96\u001b[0m, in \u001b[0;36mPettingZooEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Dict, List[\u001b[39mint\u001b[39m], \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, Dict]:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     98\u001b[0m     observation, rew, term, trunc, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mlast()\n\u001b[0;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39maction_mask\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m observation:\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\order_enforcing.py:75\u001b[0m, in \u001b[0;36mOrderEnforcingWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:108\u001b[0m, in \u001b[0;36mBaseWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m--> 108\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    110\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39magent_selection\n\u001b[0;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrewards\n",
      "File \u001b[1;32md:\\Projetos\\Multi-UAV-TA\\Multi-UAV-TA-gym-env\\DroneEnv.py:450\u001b[0m, in \u001b[0;36mMultiDroneEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39m# Calcular condições para task\u001b[39;00m\n\u001b[0;32m    449\u001b[0m dir_task \u001b[39m=\u001b[39m current_task\u001b[39m.\u001b[39mposition \u001b[39m-\u001b[39m drone\u001b[39m.\u001b[39mposition\n\u001b[1;32m--> 450\u001b[0m distance_task  \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(dir_task)                                          \n\u001b[0;32m    451\u001b[0m dir_task_norm \u001b[39m=\u001b[39m dir_task \u001b[39m/\u001b[39m distance_task                                          \n\u001b[0;32m    452\u001b[0m \u001b[39m#hdg_task = np.degrees(np.arctan2(dir_task_norm[1], dir_task_norm[0]))\u001b[39;00m\n\u001b[0;32m    453\u001b[0m                   \n\u001b[0;32m    454\u001b[0m \u001b[39m#----------------NAVIGATING--------------------\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\numpy\\linalg\\linalg.py:2526\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2524\u001b[0m     sqnorm \u001b[39m=\u001b[39m x_real\u001b[39m.\u001b[39mdot(x_real) \u001b[39m+\u001b[39m x_imag\u001b[39m.\u001b[39mdot(x_imag)\n\u001b[0;32m   2525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2526\u001b[0m     sqnorm \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mdot(x)\n\u001b[0;32m   2527\u001b[0m ret \u001b[39m=\u001b[39m sqrt(sqnorm)\n\u001b[0;32m   2528\u001b[0m \u001b[39mif\u001b[39;00m keepdims:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()\n",
    "    \n",
    "\n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = CustomCollector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        #VectorReplayBuffer(100_000, len(train_envs)),\n",
    "        PrioritizedVectorReplayBuffer( 100_000, len(train_envs), alpha=0.6, beta=0.4) ,       \n",
    "        exploration_noise=True        \n",
    "    )\n",
    "    test_collector = CustomCollector(policy, test_envs, exploration_noise=True)\n",
    "     \n",
    "    train_collector.collect(n_step=trainer_params['batch_size'] * train_env_num)\n",
    "    #test_collector.collect(n_step=trainer_params['batch size'] * train_env_num)\n",
    "    \n",
    "    # ======== tensorboard logging setup =========\n",
    "    #         \n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(name, str(Run_Data))\n",
    "    logger = TensorboardLogger(writer)\n",
    "        \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 9939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])  \n",
    "        policy.policies[agents[0]].set_eps(epsilon)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        policy.policies[agents[0]].set_eps(epsilon)\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "        #print(rews)\n",
    "        return rews.mean()#[:,0]\n",
    "                           \n",
    "    # ======== Step 5: Run the trainer =========\n",
    "    result = offpolicy_trainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,        \n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],\n",
    "        episode_per_test= trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=trainer_params['update_per_step'],\n",
    "        logger=logger,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric,\n",
    "        show_progress = True\n",
    "        )\n",
    "\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "import torch\n",
    "#from Custom_Classes import CustomCollector\n",
    "\n",
    "# Create a new instance of the policy with the same architecture as the saved policy\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "\n",
    "# Load the saved checkpoint\n",
    "policy_test = policy.policies['agent0']\n",
    "policy_test.load_state_dict(torch.load(model_save_path ))\n",
    "\n",
    "envs = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "\n",
    "#policy_test.policies['agent0'].eval()\n",
    "#policy.policies['agent0'].set_eps(0.9)\n",
    "\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "#collector = CustomCollector(policy.policies['agent0'], envs, exploration_noise=True)\n",
    "collector = CustomCollector(policy_test, envs, exploration_noise=False)\n",
    "\n",
    "#results = collector.collect(n_episode=10)\n",
    "collector.collect(n_episode=1, render=1 / 5000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(results['rews'])\n",
    "print(np.mean(results['rews'][results['rews'] > -10]))\n",
    "\n",
    "\n",
    "#create a function  to print a histogram of the results['rews']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(results['rews'][results['rews'] > -10], bins=100)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
