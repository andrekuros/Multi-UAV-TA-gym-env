{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomNetSimpleEval_TBTA_01_max30agents_timeRew\n",
      "        Loaded_Model: no \n",
      "        log_path: ./Logs\\dqn\\CustomNetSimpleEval_TBTA_01_max30agents_timeRew \n",
      "        train/test_env_num: 10 / 10 \n",
      "        model: CustomNetSimple \n",
      "        dqn_params: {'discount_factor': 0.99, 'estimation_step': 1, 'target_update_freq': 100, 'optminizer': 'Adam', 'lr': 0.0001} \n",
      "        trainer_params: {'max_epoch': 200, 'step_per_epoch': 6000, 'step_per_collect': 500, 'episode_per_test': 100, 'batch_size': 32, 'update_per_step': 0.1, 'tn_eps_max': 0.8, 'ts_eps_max': 0.001} \n",
      "        obs: Task Info -> Dist | Quality for each drone             agents= F1:2, R1:4 | tasks= Rec:15, Att:5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "\n",
    "from Custom_Classes import CustomNet\n",
    "from Custom_Classes import CustomCollector\n",
    "from Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "#from CustomClass_multi_head import CustomNet\n",
    "from Custom_Classes_simplified import CustomNetSimple\n",
    "#from Custom_Classes_simplified import CustomCollectorSimple\n",
    "#from Custom_Classes_simplified import CustomParallelToAECWrapperSimple\n",
    "\n",
    "from CustomClasses_Transformer_Reduced import CustomNetReduced\n",
    "import importlib\n",
    "\n",
    "from DroneEnv import MultiDroneEnv\n",
    "from tianshou_DQN import train\n",
    "\n",
    "\n",
    "model = \"CustomNetSimple\" # \"CustomNet\" or \"CustomNetSimple\" or \"CustomNetReduced\"\n",
    "test_num = \"Eval_TBTA_01_max30agents_timeRew\"\n",
    "\n",
    "train_env_num = 10\n",
    "test_env_num = 10\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "load_policy_name = f'policy_CustomNetSimple1605_01_1_Priorized_1605_01_1_Priorized.pth'\n",
    "save_policy_name = f'policy_{name}.pth'\n",
    "policy_path = \"dqn_Custom\"\n",
    "load_model = False\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", name)\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.99, \n",
    "              \"estimation_step\": 1, \n",
    "              \"target_update_freq\": 100,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 1e-4  }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 200,\n",
    "                  \"step_per_epoch\": 600 * train_env_num,\n",
    "                  \"step_per_collect\": 50 * train_env_num,\n",
    "                  \"episode_per_test\": 10 * test_env_num,\n",
    "                  \"batch_size\" : 32,\n",
    "                  \"update_per_step\": 0.1,\n",
    "                  \"tn_eps_max\": 0.8,\n",
    "                  \"ts_eps_max\": 0.001,\n",
    "                  }\n",
    "\n",
    "Run_Data = f'{name}\\n\\\n",
    "        Loaded_Model: {load_policy_name if load_model == True else \"no\"} \\n\\\n",
    "        log_path: {log_path} \\n\\\n",
    "        train/test_env_num: {train_env_num} / {test_env_num} \\n\\\n",
    "        model: {model} \\n\\\n",
    "        dqn_params: {dqn_params} \\n\\\n",
    "        trainer_params: {trainer_params} \\n\\\n",
    "        obs: Task Info -> Dist | Quality for each drone \\\n",
    "            agents= F1:2, R1:4 | tasks= Rec:15, Att:5'\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()\n",
    "    agent_name = env.agents[0]  # Get the name of the first agent\n",
    "           \n",
    "    agent_observation_space = env.observation_space('agent0') # assuming 'agent0' is a valid agent name\n",
    "    state_shape_agent_position = agent_observation_space[\"agent_position\"].shape[0]\n",
    "    state_shape_agent_state = agent_observation_space[\"agent_state\"].shape[0]\n",
    "    state_shape_agent_type = agent_observation_space[\"agent_type\"].shape[0]\n",
    "    state_shape_next_free_time = agent_observation_space[\"next_free_time\"].shape[0]\n",
    "    state_shape_position_after_last_task = agent_observation_space[\"position_after_last_task\"].shape[0]       \n",
    "    #state_shape_agent_relay_area = agent_observation_space[\"agent_relay_area\"].shape[0]\n",
    "        \n",
    "    state_shape_agent = (state_shape_agent_position + state_shape_agent_state +\n",
    "                     state_shape_agent_type+ state_shape_next_free_time + state_shape_position_after_last_task #+                     \n",
    "                     #state_shape_agent_relay_area\n",
    "                     )                 \n",
    "    \n",
    "\n",
    "    state_shape_task = env.observation_space('agent0')[\"tasks_info\"].shape[0]\n",
    "                  \n",
    "    action_shape = env.action_space[agent_name].shape[0]\n",
    "    #action_shape = env.action_space[agent_name].n\n",
    "               \n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        if model == \"CustomNet\":        \n",
    "            net = CustomNet(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if model == \"CustomNetSimple\":\n",
    "            net = CustomNetSimple(            \n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if model == \"CustomNetReduced\":\n",
    "            net = CustomNetReduced(            \n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"])\n",
    "    \n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor= dqn_params[\"discount_factor\"],\n",
    "            estimation_step=dqn_params[\"estimation_step\"],\n",
    "            target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "        )  \n",
    "        \n",
    "        if load_model == True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "            \n",
    "        \n",
    "        agents = [agent_learn for _ in range(len(env.agents))]\n",
    "        \n",
    "    policy = MultiAgentPolicyManager(agents, env)    \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    env_paralell = MultiDroneEnv()\n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "print(Run_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 6001it [00:57, 104.27it/s, agent0/loss=44.005, agent1/loss=48.470, agent2/loss=54.151, agent3/loss=52.552, agent4/loss=51.525, agent5/loss=51.641, agent6/loss=53.283, agent7/loss=53.875, env_step=6000, len=20, n/ep=20, n/st=500, rew=-61.19]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -122.731227 ± 0.000000, best_reward: -122.731227 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 6001it [00:56, 106.81it/s, agent0/loss=45.230, agent1/loss=46.569, agent2/loss=52.473, agent3/loss=53.451, agent4/loss=52.020, agent5/loss=50.733, agent6/loss=46.244, agent7/loss=48.068, env_step=12000, len=20, n/ep=20, n/st=500, rew=-65.33]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -122.568748 ± 0.000000, best_reward: -122.568748 ± 0.000000 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 6001it [00:52, 113.61it/s, agent0/loss=49.411, agent1/loss=55.326, agent2/loss=56.932, agent3/loss=57.767, agent4/loss=55.754, agent5/loss=55.356, agent6/loss=52.357, agent7/loss=58.205, env_step=18000, len=20, n/ep=20, n/st=500, rew=-65.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -111.153582 ± 0.000000, best_reward: -111.153582 ± 0.000000 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 6001it [00:53, 112.15it/s, agent0/loss=49.535, agent1/loss=44.727, agent2/loss=52.089, agent3/loss=53.266, agent4/loss=50.469, agent5/loss=54.346, agent6/loss=50.432, agent7/loss=51.138, env_step=24000, len=20, n/ep=20, n/st=500, rew=-58.80]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -113.329863 ± 0.000000, best_reward: -111.153582 ± 0.000000 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 6001it [00:53, 112.01it/s, agent0/loss=47.641, agent1/loss=50.662, agent2/loss=54.256, agent3/loss=58.042, agent4/loss=51.461, agent5/loss=52.365, agent6/loss=50.186, agent7/loss=51.987, env_step=30000, len=20, n/ep=20, n/st=500, rew=-44.82]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -99.981940 ± 0.000000, best_reward: -99.981940 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 6001it [00:53, 112.80it/s, agent0/loss=44.687, agent1/loss=48.225, agent2/loss=53.519, agent3/loss=58.533, agent4/loss=48.159, agent5/loss=50.163, agent6/loss=50.327, agent7/loss=51.512, env_step=36000, len=20, n/ep=20, n/st=500, rew=-63.36]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -102.386741 ± 0.000000, best_reward: -99.981940 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 6001it [00:56, 106.83it/s, agent0/loss=44.952, agent1/loss=47.633, agent2/loss=52.411, agent3/loss=55.221, agent4/loss=52.466, agent5/loss=48.846, agent6/loss=51.925, agent7/loss=49.230, env_step=42000, len=20, n/ep=20, n/st=500, rew=-57.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: -122.514011 ± 0.000000, best_reward: -99.981940 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 6001it [01:04, 92.40it/s, agent0/loss=41.841, agent1/loss=48.439, agent2/loss=56.073, agent3/loss=55.842, agent4/loss=46.269, agent5/loss=51.161, agent6/loss=54.383, agent7/loss=49.591, env_step=48000, len=20, n/ep=20, n/st=500, rew=-65.95]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: -122.355592 ± 0.000000, best_reward: -99.981940 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 6001it [00:58, 102.72it/s, agent0/loss=40.942, agent1/loss=50.257, agent2/loss=56.668, agent3/loss=52.830, agent4/loss=51.225, agent5/loss=51.018, agent6/loss=53.929, agent7/loss=52.039, env_step=54000, len=20, n/ep=20, n/st=500, rew=-59.65]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -111.803740 ± 0.000000, best_reward: -99.981940 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 6001it [00:54, 111.07it/s, agent0/loss=46.319, agent1/loss=44.039, agent2/loss=55.065, agent3/loss=57.394, agent4/loss=53.123, agent5/loss=49.781, agent6/loss=57.363, agent7/loss=50.822, env_step=60000, len=20, n/ep=20, n/st=500, rew=-52.14]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -100.524584 ± 0.000000, best_reward: -99.981940 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 6001it [00:54, 109.48it/s, agent0/loss=47.614, agent1/loss=46.677, agent2/loss=54.228, agent3/loss=57.915, agent4/loss=55.204, agent5/loss=52.375, agent6/loss=53.106, agent7/loss=52.966, env_step=66000, len=20, n/ep=20, n/st=500, rew=-50.62]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: -102.659393 ± 0.000000, best_reward: -99.981940 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 6001it [00:56, 106.55it/s, agent0/loss=46.481, agent1/loss=43.388, agent2/loss=52.687, agent3/loss=54.851, agent4/loss=52.549, agent5/loss=57.449, agent6/loss=53.116, agent7/loss=49.814, env_step=72000, len=20, n/ep=20, n/st=500, rew=-49.22]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: -112.109918 ± 0.000000, best_reward: -99.981940 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 6001it [00:56, 106.05it/s, agent0/loss=45.269, agent1/loss=46.166, agent2/loss=50.160, agent3/loss=56.463, agent4/loss=51.226, agent5/loss=46.671, agent6/loss=48.983, agent7/loss=50.193, env_step=78000, len=20, n/ep=20, n/st=500, rew=-49.97]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: -91.159605 ± 0.000000, best_reward: -91.159605 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 6001it [00:56, 106.69it/s, agent0/loss=45.597, agent1/loss=46.099, agent2/loss=52.516, agent3/loss=53.531, agent4/loss=52.430, agent5/loss=51.809, agent6/loss=52.179, agent7/loss=50.895, env_step=84000, len=20, n/ep=20, n/st=500, rew=-52.97]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: -94.365820 ± 0.000000, best_reward: -91.159605 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 6001it [00:57, 104.55it/s, agent0/loss=44.585, agent1/loss=47.039, agent2/loss=58.021, agent3/loss=60.403, agent4/loss=52.775, agent5/loss=47.663, agent6/loss=52.518, agent7/loss=52.514, env_step=90000, len=20, n/ep=20, n/st=500, rew=-51.63]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: -106.168187 ± 0.000000, best_reward: -91.159605 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 6001it [00:58, 102.43it/s, agent0/loss=47.688, agent1/loss=45.293, agent2/loss=52.356, agent3/loss=51.931, agent4/loss=53.204, agent5/loss=52.388, agent6/loss=48.739, agent7/loss=50.901, env_step=96000, len=20, n/ep=20, n/st=500, rew=-36.67]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: -102.219267 ± 0.000000, best_reward: -91.159605 ± 0.000000 in #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 6001it [01:01, 97.64it/s, agent0/loss=44.910, agent1/loss=46.573, agent2/loss=56.722, agent3/loss=52.270, agent4/loss=52.519, agent5/loss=51.494, agent6/loss=52.629, agent7/loss=46.386, env_step=102000, len=20, n/ep=20, n/st=500, rew=-51.06]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: -73.350536 ± 0.000000, best_reward: -73.350536 ± 0.000000 in #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 6001it [01:04, 92.83it/s, agent0/loss=44.373, agent1/loss=43.526, agent2/loss=51.181, agent3/loss=54.851, agent4/loss=49.629, agent5/loss=48.653, agent6/loss=48.919, agent7/loss=52.269, env_step=108000, len=20, n/ep=20, n/st=500, rew=-47.32]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: -67.762214 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 6001it [01:06, 90.78it/s, agent0/loss=44.646, agent1/loss=48.938, agent2/loss=50.446, agent3/loss=53.762, agent4/loss=51.257, agent5/loss=48.735, agent6/loss=49.590, agent7/loss=51.519, env_step=114000, len=20, n/ep=20, n/st=500, rew=-34.25]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: -67.803602 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 6001it [01:06, 90.06it/s, agent0/loss=41.181, agent1/loss=48.876, agent2/loss=54.024, agent3/loss=56.195, agent4/loss=49.159, agent5/loss=48.959, agent6/loss=51.627, agent7/loss=47.547, env_step=120000, len=20, n/ep=20, n/st=500, rew=-47.30]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: -92.401393 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 6001it [01:06, 90.27it/s, agent0/loss=47.030, agent1/loss=46.907, agent2/loss=52.330, agent3/loss=54.376, agent4/loss=49.732, agent5/loss=52.831, agent6/loss=46.594, agent7/loss=52.192, env_step=126000, len=20, n/ep=20, n/st=500, rew=-49.46]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: -102.178212 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 6001it [01:08, 87.21it/s, agent0/loss=49.780, agent1/loss=47.162, agent2/loss=51.683, agent3/loss=55.048, agent4/loss=53.501, agent5/loss=47.064, agent6/loss=48.398, agent7/loss=54.077, env_step=132000, len=20, n/ep=20, n/st=500, rew=-65.34]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #22: test_reward: -101.567857 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 6001it [01:06, 89.94it/s, agent0/loss=44.192, agent1/loss=42.804, agent2/loss=54.847, agent3/loss=51.754, agent4/loss=55.215, agent5/loss=47.701, agent6/loss=51.348, agent7/loss=49.540, env_step=138000, len=20, n/ep=20, n/st=500, rew=-49.59]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: -85.517960 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 6001it [01:16, 78.62it/s, agent0/loss=43.860, agent1/loss=44.385, agent2/loss=50.963, agent3/loss=52.210, agent4/loss=46.348, agent5/loss=49.277, agent6/loss=51.343, agent7/loss=55.603, env_step=144000, len=20, n/ep=20, n/st=500, rew=-45.93]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #24: test_reward: -90.826644 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 6001it [01:20, 74.92it/s, agent0/loss=40.691, agent1/loss=39.578, agent2/loss=53.150, agent3/loss=54.879, agent4/loss=50.689, agent5/loss=54.551, agent6/loss=50.701, agent7/loss=52.510, env_step=150000, len=20, n/ep=20, n/st=500, rew=-53.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #25: test_reward: -90.959073 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 6001it [01:17, 77.46it/s, agent0/loss=43.185, agent1/loss=45.009, agent2/loss=51.404, agent3/loss=57.435, agent4/loss=50.608, agent5/loss=50.412, agent6/loss=50.067, agent7/loss=56.817, env_step=156000, len=20, n/ep=20, n/st=500, rew=-52.85]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #26: test_reward: -101.722793 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 6001it [01:20, 74.83it/s, agent0/loss=46.880, agent1/loss=43.701, agent2/loss=56.612, agent3/loss=52.855, agent4/loss=48.515, agent5/loss=49.923, agent6/loss=44.946, agent7/loss=51.620, env_step=162000, len=20, n/ep=20, n/st=500, rew=-54.63]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #27: test_reward: -113.900749 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 6001it [01:22, 72.71it/s, agent0/loss=43.314, agent1/loss=44.781, agent2/loss=56.449, agent3/loss=51.167, agent4/loss=48.654, agent5/loss=54.273, agent6/loss=49.653, agent7/loss=53.822, env_step=168000, len=20, n/ep=20, n/st=500, rew=-33.44]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #28: test_reward: -102.445426 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 6001it [01:25, 70.53it/s, agent0/loss=45.599, agent1/loss=45.482, agent2/loss=52.768, agent3/loss=59.788, agent4/loss=46.908, agent5/loss=52.710, agent6/loss=46.240, agent7/loss=54.593, env_step=174000, len=20, n/ep=20, n/st=500, rew=-55.19]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: -84.445437 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 6001it [01:19, 75.88it/s, agent0/loss=41.450, agent1/loss=45.666, agent2/loss=51.687, agent3/loss=53.780, agent4/loss=44.513, agent5/loss=49.693, agent6/loss=50.493, agent7/loss=47.766, env_step=180000, len=20, n/ep=20, n/st=500, rew=-42.44]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #30: test_reward: -104.371129 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 6001it [01:14, 80.08it/s, agent0/loss=42.344, agent1/loss=41.150, agent2/loss=54.315, agent3/loss=54.785, agent4/loss=51.016, agent5/loss=51.330, agent6/loss=51.774, agent7/loss=49.521, env_step=186000, len=20, n/ep=20, n/st=500, rew=-44.30]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: -88.334219 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 6001it [01:13, 81.13it/s, agent0/loss=43.748, agent1/loss=43.929, agent2/loss=51.783, agent3/loss=54.761, agent4/loss=48.248, agent5/loss=47.242, agent6/loss=54.637, agent7/loss=50.496, env_step=192000, len=20, n/ep=20, n/st=500, rew=-52.94]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #32: test_reward: -89.628420 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 6001it [01:15, 79.00it/s, agent0/loss=39.421, agent1/loss=43.091, agent2/loss=50.155, agent3/loss=53.375, agent4/loss=45.925, agent5/loss=45.855, agent6/loss=49.221, agent7/loss=52.472, env_step=198000, len=20, n/ep=20, n/st=500, rew=-47.73]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: -102.342758 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 6001it [01:19, 75.67it/s, agent0/loss=44.750, agent1/loss=43.406, agent2/loss=52.743, agent3/loss=52.081, agent4/loss=47.212, agent5/loss=47.764, agent6/loss=51.709, agent7/loss=51.642, env_step=204000, len=20, n/ep=20, n/st=500, rew=-53.61]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #34: test_reward: -102.182579 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 6001it [01:17, 77.85it/s, agent0/loss=41.941, agent1/loss=42.030, agent2/loss=53.012, agent3/loss=49.484, agent4/loss=52.407, agent5/loss=49.459, agent6/loss=48.021, agent7/loss=49.713, env_step=210000, len=20, n/ep=20, n/st=500, rew=-55.53]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35: test_reward: -101.153009 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 6001it [01:15, 79.24it/s, agent0/loss=41.093, agent1/loss=45.472, agent2/loss=50.128, agent3/loss=51.419, agent4/loss=49.030, agent5/loss=47.873, agent6/loss=52.649, agent7/loss=50.067, env_step=216000, len=20, n/ep=20, n/st=500, rew=-40.65]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #36: test_reward: -90.367024 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 6001it [01:16, 78.15it/s, agent0/loss=41.835, agent1/loss=42.500, agent2/loss=52.793, agent3/loss=53.582, agent4/loss=50.711, agent5/loss=50.717, agent6/loss=47.862, agent7/loss=49.405, env_step=222000, len=20, n/ep=20, n/st=500, rew=-70.68]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: -113.624404 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 6001it [01:20, 74.14it/s, agent0/loss=43.465, agent1/loss=43.865, agent2/loss=52.488, agent3/loss=51.728, agent4/loss=46.796, agent5/loss=47.505, agent6/loss=47.673, agent7/loss=50.884, env_step=228000, len=20, n/ep=20, n/st=500, rew=-66.70]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #38: test_reward: -109.116386 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 6001it [01:22, 72.94it/s, agent0/loss=42.955, agent1/loss=36.723, agent2/loss=52.032, agent3/loss=51.849, agent4/loss=49.408, agent5/loss=48.027, agent6/loss=46.362, agent7/loss=50.744, env_step=234000, len=20, n/ep=20, n/st=500, rew=-56.14]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: -113.949044 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 6001it [01:27, 68.64it/s, agent0/loss=44.546, agent1/loss=43.099, agent2/loss=50.735, agent3/loss=55.517, agent4/loss=45.006, agent5/loss=49.418, agent6/loss=51.833, agent7/loss=45.281, env_step=240000, len=20, n/ep=20, n/st=500, rew=-56.59]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40: test_reward: -113.568913 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 6001it [01:41, 59.22it/s, agent0/loss=40.739, agent1/loss=42.295, agent2/loss=52.036, agent3/loss=52.975, agent4/loss=47.394, agent5/loss=51.062, agent6/loss=43.559, agent7/loss=50.752, env_step=246000, len=20, n/ep=20, n/st=500, rew=-69.33]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: -122.683673 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 6001it [01:37, 61.65it/s, agent0/loss=42.405, agent1/loss=42.783, agent2/loss=50.208, agent3/loss=54.205, agent4/loss=45.514, agent5/loss=47.840, agent6/loss=47.635, agent7/loss=49.187, env_step=252000, len=20, n/ep=20, n/st=500, rew=-81.38]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #42: test_reward: -122.008687 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 6001it [01:39, 60.25it/s, agent0/loss=45.007, agent1/loss=47.963, agent2/loss=49.532, agent3/loss=54.324, agent4/loss=46.912, agent5/loss=44.553, agent6/loss=49.624, agent7/loss=49.378, env_step=258000, len=20, n/ep=20, n/st=500, rew=-69.43]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: -122.519477 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 6001it [01:39, 60.16it/s, agent0/loss=43.293, agent1/loss=47.155, agent2/loss=52.332, agent3/loss=54.366, agent4/loss=47.796, agent5/loss=46.811, agent6/loss=48.498, agent7/loss=55.498, env_step=264000, len=20, n/ep=20, n/st=500, rew=-71.81]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44: test_reward: -122.635236 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 6001it [01:37, 61.27it/s, agent0/loss=43.188, agent1/loss=44.461, agent2/loss=53.638, agent3/loss=54.508, agent4/loss=50.798, agent5/loss=44.846, agent6/loss=50.714, agent7/loss=50.646, env_step=270000, len=20, n/ep=20, n/st=500, rew=-71.64]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #45: test_reward: -122.500108 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #46: 6001it [01:35, 62.83it/s, agent0/loss=40.459, agent1/loss=42.872, agent2/loss=50.238, agent3/loss=51.742, agent4/loss=49.737, agent5/loss=44.985, agent6/loss=47.387, agent7/loss=46.162, env_step=276000, len=20, n/ep=20, n/st=500, rew=-63.16]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #46: test_reward: -112.762813 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #47: 6001it [01:31, 65.75it/s, agent0/loss=42.686, agent1/loss=45.137, agent2/loss=48.122, agent3/loss=52.488, agent4/loss=47.351, agent5/loss=44.689, agent6/loss=46.409, agent7/loss=49.371, env_step=282000, len=20, n/ep=20, n/st=500, rew=-68.94]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #47: test_reward: -113.689719 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #48: 6001it [01:29, 67.24it/s, agent0/loss=42.491, agent1/loss=39.274, agent2/loss=48.104, agent3/loss=48.980, agent4/loss=47.086, agent5/loss=48.140, agent6/loss=47.343, agent7/loss=51.732, env_step=288000, len=20, n/ep=20, n/st=500, rew=-70.41]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #48: test_reward: -113.487396 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #49: 6001it [01:26, 69.20it/s, agent0/loss=43.123, agent1/loss=42.283, agent2/loss=48.272, agent3/loss=53.457, agent4/loss=45.953, agent5/loss=51.768, agent6/loss=47.721, agent7/loss=50.184, env_step=294000, len=20, n/ep=20, n/st=500, rew=-67.08]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #49: test_reward: -113.995590 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #50: 6001it [01:30, 66.24it/s, agent0/loss=43.595, agent1/loss=40.073, agent2/loss=52.063, agent3/loss=52.549, agent4/loss=48.132, agent5/loss=49.516, agent6/loss=45.710, agent7/loss=52.230, env_step=300000, len=20, n/ep=20, n/st=500, rew=-71.96]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #50: test_reward: -122.439957 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #51: 6001it [01:24, 70.77it/s, agent0/loss=39.537, agent1/loss=40.700, agent2/loss=52.459, agent3/loss=51.977, agent4/loss=43.682, agent5/loss=43.395, agent6/loss=46.545, agent7/loss=47.839, env_step=306000, len=20, n/ep=20, n/st=500, rew=-71.01]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #51: test_reward: -122.222687 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #52: 6001it [01:23, 72.17it/s, agent0/loss=39.061, agent1/loss=39.907, agent2/loss=48.453, agent3/loss=50.496, agent4/loss=48.416, agent5/loss=46.641, agent6/loss=48.838, agent7/loss=49.444, env_step=312000, len=20, n/ep=20, n/st=500, rew=-67.92]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #52: test_reward: -102.557873 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #53: 6001it [01:24, 71.39it/s, agent0/loss=38.204, agent1/loss=46.217, agent2/loss=51.109, agent3/loss=52.009, agent4/loss=46.278, agent5/loss=43.250, agent6/loss=48.319, agent7/loss=47.300, env_step=318000, len=20, n/ep=20, n/st=500, rew=-68.65]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #53: test_reward: -113.763659 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #54: 6001it [01:22, 72.98it/s, agent0/loss=40.501, agent1/loss=41.576, agent2/loss=49.395, agent3/loss=54.620, agent4/loss=44.257, agent5/loss=44.803, agent6/loss=47.237, agent7/loss=46.455, env_step=324000, len=20, n/ep=20, n/st=500, rew=-60.77]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #54: test_reward: -112.932111 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #55: 6001it [01:24, 71.08it/s, agent0/loss=40.444, agent1/loss=38.834, agent2/loss=48.947, agent3/loss=51.749, agent4/loss=42.773, agent5/loss=45.779, agent6/loss=46.340, agent7/loss=51.357, env_step=330000, len=20, n/ep=20, n/st=500, rew=-62.14]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #55: test_reward: -102.321374 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #56: 6001it [01:22, 72.53it/s, agent0/loss=42.099, agent1/loss=40.957, agent2/loss=49.053, agent3/loss=49.265, agent4/loss=43.214, agent5/loss=44.664, agent6/loss=44.720, agent7/loss=50.344, env_step=336000, len=20, n/ep=20, n/st=500, rew=-60.99]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #56: test_reward: -102.312722 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #57: 6001it [01:22, 72.44it/s, agent0/loss=41.060, agent1/loss=43.335, agent2/loss=48.592, agent3/loss=52.385, agent4/loss=47.117, agent5/loss=48.352, agent6/loss=46.800, agent7/loss=47.413, env_step=342000, len=20, n/ep=20, n/st=500, rew=-61.42]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #57: test_reward: -102.196611 ± 0.000000, best_reward: -67.762214 ± 0.000000 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #58:  25%|##5       | 1500/6000 [00:20<01:00, 74.55it/s, agent0/loss=42.089, agent1/loss=41.102, agent2/loss=47.246, agent3/loss=48.743, agent4/loss=45.764, agent5/loss=50.565, agent6/loss=42.312, agent7/loss=50.357, env_step=343000, len=20, n/ep=20, n/st=500, rew=-62.76]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m rews\u001b[39m.\u001b[39mmean()\u001b[39m#[:,0]\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m# ======== Step 5: Run the trainer =========\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m result \u001b[39m=\u001b[39m offpolicy_trainer(\n\u001b[0;32m     60\u001b[0m     policy\u001b[39m=\u001b[39;49mpolicy,\n\u001b[0;32m     61\u001b[0m     train_collector\u001b[39m=\u001b[39;49mtrain_collector,\n\u001b[0;32m     62\u001b[0m     test_collector\u001b[39m=\u001b[39;49mtest_collector,        \n\u001b[0;32m     63\u001b[0m     max_epoch\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mmax_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     64\u001b[0m     step_per_epoch\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mstep_per_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     65\u001b[0m     step_per_collect\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mstep_per_collect\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     66\u001b[0m     episode_per_test\u001b[39m=\u001b[39;49m trainer_params[\u001b[39m'\u001b[39;49m\u001b[39mepisode_per_test\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     67\u001b[0m     batch_size\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     68\u001b[0m     train_fn\u001b[39m=\u001b[39;49mtrain_fn,\n\u001b[0;32m     69\u001b[0m     test_fn\u001b[39m=\u001b[39;49mtest_fn,\n\u001b[0;32m     70\u001b[0m     stop_fn\u001b[39m=\u001b[39;49mstop_fn,\n\u001b[0;32m     71\u001b[0m     save_best_fn\u001b[39m=\u001b[39;49msave_best_fn,\n\u001b[0;32m     72\u001b[0m     update_per_step\u001b[39m=\u001b[39;49mtrainer_params[\u001b[39m'\u001b[39;49m\u001b[39mupdate_per_step\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     73\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m     74\u001b[0m     test_in_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m     reward_metric\u001b[39m=\u001b[39;49mreward_metric,\n\u001b[0;32m     76\u001b[0m     show_progress \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     77\u001b[0m     )\n\u001b[0;32m     79\u001b[0m \u001b[39m# return result, policy.policies[agents[1]]\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m==========Result==========\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py:134\u001b[0m, in \u001b[0;36moffpolicy_trainer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moffpolicy_trainer\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m]]:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Wrapper for OffPolicyTrainer run method.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[39m    It is identical to ``OffpolicyTrainer(...).run()``.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[0;32m    132\u001b[0m \u001b[39m    :return: See :func:`~tianshou.trainer.gather_info`.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m OffpolicyTrainer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\trainer\\base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 441\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[0;32m    443\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[0;32m    444\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[0;32m    445\u001b[0m     )\n\u001b[0;32m    446\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\trainer\\base.py:299\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    296\u001b[0m         result[\u001b[39m\"\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_step)\n\u001b[0;32m    297\u001b[0m         t\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m--> 299\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_update_fn(data, result)\n\u001b[0;32m    300\u001b[0m     t\u001b[39m.\u001b[39mset_postfix(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata)\n\u001b[0;32m    302\u001b[0m \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py:122\u001b[0m, in \u001b[0;36mOffpolicyTrainer.policy_update_fn\u001b[1;34m(self, data, result)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_per_step \u001b[39m*\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39mn/st\u001b[39m\u001b[39m\"\u001b[39m])):\n\u001b[0;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 122\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_collector\u001b[39m.\u001b[39;49mbuffer)\n\u001b[0;32m    123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_update_data(data, losses)\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\policy\\base.py:277\u001b[0m, in \u001b[0;36mBasePolicy.update\u001b[1;34m(self, sample_size, buffer, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    276\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_fn(batch, buffer, indices)\n\u001b[1;32m--> 277\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearn(batch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    278\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_process_fn(batch, buffer, indices)\n\u001b[0;32m    279\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\policy\\multiagent\\mapolicy.py:197\u001b[0m, in \u001b[0;36mMultiAgentPolicyManager.learn\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m    195\u001b[0m data \u001b[39m=\u001b[39m batch[agent_id]\n\u001b[0;32m    196\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data\u001b[39m.\u001b[39mis_empty():\n\u001b[1;32m--> 197\u001b[0m     out \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39mlearn(batch\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    198\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m out\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    199\u001b[0m         results[agent_id \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m k] \u001b[39m=\u001b[39m v\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\tianshou\\policy\\modelfree\\dqn.py:185\u001b[0m, in \u001b[0;36mDQNPolicy.learn\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m     loss \u001b[39m=\u001b[39m (td_error\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m weight)\u001b[39m.\u001b[39mmean()\n\u001b[0;32m    184\u001b[0m batch\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m td_error  \u001b[39m# prio-buffer\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    187\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mw:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()\n",
    "    \n",
    "\n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = CustomCollector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        #VectorReplayBuffer(100_000, len(train_envs)),\n",
    "        PrioritizedVectorReplayBuffer( 100_000, len(train_envs), alpha=0.6, beta=0.4) ,       \n",
    "        exploration_noise=True        \n",
    "    )\n",
    "    test_collector = CustomCollector(policy, test_envs, exploration_noise=True)\n",
    "     \n",
    "    train_collector.collect(n_step=trainer_params['batch_size'] * train_env_num)\n",
    "    #test_collector.collect(n_step=trainer_params['batch size'] * train_env_num)\n",
    "    \n",
    "    # ======== tensorboard logging setup =========\n",
    "    #         \n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(name, str(Run_Data))\n",
    "    logger = TensorboardLogger(writer)\n",
    "        \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 9939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])  \n",
    "        policy.policies[agents[0]].set_eps(epsilon)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        policy.policies[agents[0]].set_eps(epsilon)\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "        #print(rews)\n",
    "        return rews.mean()#[:,0]\n",
    "                           \n",
    "    # ======== Step 5: Run the trainer =========\n",
    "    result = offpolicy_trainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,        \n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],\n",
    "        episode_per_test= trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=trainer_params['update_per_step'],\n",
    "        logger=logger,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric,\n",
    "        show_progress = True\n",
    "        )\n",
    "\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "w:\\OneDrive\\Doutorado\\SwarmSimASA\\PettingZoo\\lib\\site-packages\\torch\\nn\\init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "import torch\n",
    "#from Custom_Classes import CustomCollector\n",
    "\n",
    "# Create a new instance of the policy with the same architecture as the saved policy\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "\n",
    "# Load the saved checkpoint\n",
    "policy_test = policy.policies['agent0']\n",
    "policy_test.load_state_dict(torch.load(model_save_path ))\n",
    "\n",
    "envs = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "\n",
    "envs.max_time_steps = 200\n",
    "#policy_test.policies['agent0'].eval()\n",
    "#policy.policies['agent0'].set_eps(0.9)\n",
    "\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "\n",
    "#collector = CustomCollector(policy.policies['agent0'], envs, exploration_noise=True)\n",
    "#collector = CustomCollector(policy_test, envs, exploration_noise=False)\n",
    "collector = CustomCollector(policy, envs, exploration_noise=True)\n",
    "\n",
    "results = collector.collect(n_episode=10)\n",
    "#collector.collect(n_episode=1, render=1 / 5000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n/ep': 10,\n",
       " 'n/st': 400,\n",
       " 'rews': array([[146.60998543, 146.60998543, 146.60998543, 146.60998543,\n",
       "         146.60998543, 146.60998543, 146.60998543, 146.60998543],\n",
       "        [149.65674368, 149.65674368, 149.65674368, 149.65674368,\n",
       "         149.65674368, 149.65674368, 149.65674368, 149.65674368],\n",
       "        [150.12206002, 150.12206002, 150.12206002, 150.12206002,\n",
       "         150.12206002, 150.12206002, 150.12206002, 150.12206002],\n",
       "        [150.34100261, 150.34100261, 150.34100261, 150.34100261,\n",
       "         150.34100261, 150.34100261, 150.34100261, 150.34100261],\n",
       "        [136.37620211, 136.37620211, 136.37620211, 136.37620211,\n",
       "         136.37620211, 136.37620211, 136.37620211, 136.37620211],\n",
       "        [145.54718066, 145.54718066, 145.54718066, 145.54718066,\n",
       "         145.54718066, 145.54718066, 145.54718066, 145.54718066],\n",
       "        [144.85799617, 144.85799617, 144.85799617, 144.85799617,\n",
       "         144.85799617, 144.85799617, 144.85799617, 144.85799617],\n",
       "        [146.66782528, 146.66782528, 146.66782528, 146.66782528,\n",
       "         146.66782528, 146.66782528, 146.66782528, 146.66782528],\n",
       "        [150.7993451 , 150.7993451 , 150.7993451 , 150.7993451 ,\n",
       "         150.7993451 , 150.7993451 , 150.7993451 , 150.7993451 ],\n",
       "        [135.14677407, 135.14677407, 135.14677407, 135.14677407,\n",
       "         135.14677407, 135.14677407, 135.14677407, 135.14677407]]),\n",
       " 'lens': array([40, 40, 40, 40, 40, 40, 40, 40, 40, 40]),\n",
       " 'idxs': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'rew': 145.61251151264796,\n",
       " 'len': 40.0,\n",
       " 'rew_std': 5.3242556920708966,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mmax\u001b[39;49m(results[\u001b[39m'\u001b[39;49m\u001b[39mrews\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39mmean(results[\u001b[39m'\u001b[39m\u001b[39mrews\u001b[39m\u001b[39m'\u001b[39m][results[\u001b[39m'\u001b[39m\u001b[39mrews\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m \u001b[39m-\u001b[39m\u001b[39m10\u001b[39m]))\n\u001b[0;32m      5\u001b[0m \u001b[39m#create a function  to print a histogram of the results['rews']\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "max(results['rews'])\n",
    "print(np.mean(results['rews'][results['rews'] > -10]))\n",
    "\n",
    "\n",
    "#create a function  to print a histogram of the results['rews']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(results['rews'][results['rews'] > -10], bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import st\n",
    "import torch\n",
    "from tianshou.data import Batch\n",
    "\n",
    "# load policy as in your original code\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "policy_test = policy.policies['agent0']\n",
    "state_saved = torch.load(model_save_path )\n",
    "print(policy_test)\n",
    "policy_test.load_state_dict(state_saved)\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "# initialize your environment\n",
    "#env = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "env = MultiDroneEnv(None)\n",
    "env.max_time_steps = 100\n",
    "\n",
    "# simulate the interaction with the environment manually\n",
    "for episode in range(1):  # simulate 10 episodes\n",
    "    \n",
    "    obs, _  = env.reset(seed=episode)         \n",
    "    info         = env.get_initial_state()\n",
    "    \n",
    "    drones = info[\"drones\"]\n",
    "    tasks = info[\"tasks\"]\n",
    "        \n",
    "    done = {0 : False}\n",
    "    truncations = {0 : False}\n",
    "    \n",
    "    episodo_reward = 0\n",
    "    #obs, reward, done, truncations, info = env.step(action)\n",
    "\n",
    "    while not all(done.values()) and not all(truncations.values()):\n",
    "        \n",
    "        agent_id = \"agent\" + str(env.agent_selector._current_agent)\n",
    "        # Create a Batch of observations\n",
    "        obs_batch = Batch(obs=obs[agent_id], info=[{}])  # add empty info for each observation\n",
    "        \n",
    "        #print(obs_batch)\n",
    "        # Forward the batch of observations through the policy to get the actions\n",
    "        action = policy_test(obs_batch).act\n",
    "        action = {agent_id : action[0]}\n",
    "       \n",
    "        obs, reward, done, truncations, info = env.step(action)\n",
    "          \n",
    "        episodo_reward += sum(reward.values())/env.n_agents\n",
    "\n",
    "       \n",
    "\n",
    "print(episodo_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
