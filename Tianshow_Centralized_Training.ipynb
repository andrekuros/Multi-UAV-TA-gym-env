{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomNetMultiHead_Eval_TBTA_Relative_Representation_01  \n",
      "Loaded_Model: no  \n",
      "log_path: ./Logs\\dqn\\CustomNetMultiHead_Eval_TBTA_Relative_Representation_01231013-223240  \n",
      "train/test_env_num: 30 / 1  \n",
      "model: CustomNetMultiHead  \n",
      "dqn_params: {'discount_factor': 0.98, 'estimation_step': 100, 'target_update_freq': 900, 'optminizer': 'Adam', 'lr': 5e-05}  \n",
      "trainer_params: {'max_epoch': 200, 'step_per_epoch': 13500, 'step_per_collect': 9000, 'episode_per_test': 1, 'batch_size': 600, 'update_per_step': 0.002, 'tn_eps_max': 0.85, 'ts_eps_max': 0.0} \n",
      " \n",
      "--------- Env ------------  \n",
      "\n",
      "Rewards Only Final Quality and Threats\n",
      "random_init_pos      : False\n",
      "max_time_steps       : 150\n",
      "simulation_frame_rate: 0.01\n",
      "Agents               : {'F1': 4, 'F2': 2, 'R1': 6}\n",
      "tasks                : {'Att': 4, 'Rec': 16, 'Hold': 4}\n",
      "random_init_pos      : False \n",
      "threats              : 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "#from torchviz import make_dot\n",
    "\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomCollector\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "#from CustomClass_multi_head import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes_simplified import CustomNetSimple\n",
    "#from Custom_Classes_simplified import CustomCollectorSimple\n",
    "#from Custom_Classes_simplified import CustomParallelToAECWrapperSimple\n",
    "\n",
    "from TaskAllocation.RL_Policies.CustomClasses_Transformer_Reduced import CustomNetReduced\n",
    "from TaskAllocation.RL_Policies.CustomClass_MultiHead_Transformer import CustomNetMultiHead\n",
    "\n",
    "from mUAV_TA.MultiDroneEnvUtils import agentEnvOptions\n",
    "\n",
    "from mUAV_TA.DroneEnv import MultiUAVEnv\n",
    "#from tianshou_DQN import train\n",
    "model = \"CustomNetMultiHead\" # \"CustomNet\" or \"CustomNetSimple\" or \"CustomNetReduced\" or \"CustomNetMultiHead\"\n",
    "test_num = \"_Eval_TBTA_Relative_Representation_01\"\n",
    "\n",
    "train_env_num = 30\n",
    "test_env_num = 1\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "load_policy_name = f'policy_CustomNetMultiHead_Eval_TBTA_02_simplified_UCF1_new_rew_updR.pth'\n",
    "save_policy_name = f'policy_{name}.pth'\n",
    "policy_path = \"dqn_Custom\"\n",
    "load_model = False\n",
    "\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", log_name)\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.98, \n",
    "              \"estimation_step\": 100, \n",
    "              \"target_update_freq\": 900,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 5e-5  }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 200,\n",
    "                  \"step_per_epoch\": 450 * train_env_num,\n",
    "                  \"step_per_collect\": 300 * train_env_num,\n",
    "                  \"episode_per_test\": 1,\n",
    "                  \"batch_size\" : 600,\n",
    "                  \"update_per_step\": 3 / (50 * train_env_num),\n",
    "                  \"tn_eps_max\": 0.85,\n",
    "                  \"ts_eps_max\": 0.0,\n",
    "                  }\n",
    "\n",
    "config_default = agentEnvOptions()\n",
    "Run_Data = f'''{name}  \n",
    "Loaded_Model: {load_policy_name if load_model else \"no\"}  \n",
    "log_path: {log_path}  \n",
    "train/test_env_num: {train_env_num} / {test_env_num}  \n",
    "model: {model}  \n",
    "dqn_params: {dqn_params}  \n",
    "trainer_params: {trainer_params} \n",
    " \n",
    "--------- Env ------------  \n",
    "\n",
    "Rewards Only Final Quality and Threats\n",
    "random_init_pos      : {config_default.random_init_pos}\n",
    "max_time_steps       : {config_default.max_time_steps}\n",
    "simulation_frame_rate: {config_default.simulation_frame_rate}\n",
    "Agents               : {config_default.agents}\n",
    "tasks                : {config_default.tasks}\n",
    "random_init_pos      : {config_default.random_init_pos} \n",
    "threats              : 4\n",
    "'''\n",
    "\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "def generate_dummy_observation(batch_size=1, sequence_length=31, feature_dim=12):\n",
    "    # Generate a random tensor with the given shape\n",
    "    dummy_obs = torch.randn(batch_size, sequence_length, feature_dim)\n",
    "\n",
    "    return dummy_obs\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()\n",
    "    agent_name = env.agents[0]  # Get the name of the first agent\n",
    "\n",
    "    #print(env.observation_space )\n",
    "    agent_observation_space = env.observation_space # assuming 'agent0' is a valid agent name\n",
    "    state_shape_agent_position = agent_observation_space[\"agent_position\"].shape[0]\n",
    "    state_shape_agent_state = agent_observation_space[\"agent_state\"].shape[0]\n",
    "    state_shape_agent_type = agent_observation_space[\"agent_type\"].shape[0]\n",
    "    state_shape_next_free_time = agent_observation_space[\"next_free_time\"].shape[0]\n",
    "    state_shape_position_after_last_task = agent_observation_space[\"position_after_last_task\"].shape[0]       \n",
    "    #state_shape_agent_relay_area = agent_observation_space[\"agent_relay_area\"].shape[0]\n",
    "        \n",
    "    state_shape_agent = (state_shape_agent_position + state_shape_agent_state +\n",
    "                     state_shape_agent_type+ state_shape_next_free_time + state_shape_position_after_last_task #+                     \n",
    "                     #state_shape_agent_relay_area\n",
    "                     )                 \n",
    "    \n",
    "\n",
    "    state_shape_task = 31 * 12 #env.observation_space[\"tasks_info\"].shape[0]\n",
    "                  \n",
    "    action_shape = env.action_space[agent_name].shape[0]\n",
    "    #action_shape = env.action_space[agent_name].n\n",
    "               \n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        if model == \"CustomNet\":        \n",
    "            net = CustomNet(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if model == \"CustomNetSimple\":\n",
    "            net = CustomNetSimple(            \n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if model == \"CustomNetReduced\":\n",
    "            net = CustomNetReduced(            \n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        if model == \"CustomNetMultiHead\":\n",
    "            net = CustomNetMultiHead(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    \n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"])\n",
    "    \n",
    "        agent_learn = DQNPolicy(\n",
    "            model=net,\n",
    "            optim=optim,\n",
    "            discount_factor= dqn_params[\"discount_factor\"],\n",
    "            estimation_step=dqn_params[\"estimation_step\"],\n",
    "            target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "        )  \n",
    "        \n",
    "        if load_model == True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "            \n",
    "        \n",
    "        agents = [agent_learn for _ in range(len(env.agents))]\n",
    "        \n",
    "    policy = MultiAgentPolicyManager(agents, env)  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    env_paralell = MultiUAVEnv()\n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "print(Run_Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer Warming Up \n",
      "..........\n",
      "Buffer Lenght:  200.0\n",
      "Best Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 18000it [02:36, 114.87it/s, agent0/loss=0.015, agent10/loss=0.016, agent2/loss=0.017, agent4/loss=0.016, agent6/loss=0.013, agent8/loss=0.015, env_step=18000, len=150, n/ep=60, n/st=9000, rew=0.05]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #1: test_reward: 0.200000 ± 0.000000, best_reward: 0.200000 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 18000it [02:36, 115.37it/s, agent0/loss=0.018, agent10/loss=0.017, agent2/loss=0.019, agent4/loss=0.018, agent6/loss=0.016, agent8/loss=0.017, env_step=36000, len=150, n/ep=60, n/st=9000, rew=0.31]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -0.300000 ± 0.000000, best_reward: 0.200000 ± 0.000000 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 18000it [02:40, 112.30it/s, agent0/loss=0.021, agent10/loss=0.020, agent2/loss=0.022, agent4/loss=0.021, agent6/loss=0.019, agent8/loss=0.020, env_step=54000, len=150, n/ep=60, n/st=9000, rew=0.43]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #3: test_reward: 0.400000 ± 0.000000, best_reward: 0.400000 ± 0.000000 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 18000it [02:41, 111.73it/s, agent0/loss=0.025, agent10/loss=0.024, agent2/loss=0.027, agent4/loss=0.026, agent6/loss=0.024, agent8/loss=0.024, env_step=72000, len=150, n/ep=60, n/st=9000, rew=0.56]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #4: test_reward: 0.700000 ± 0.000000, best_reward: 0.700000 ± 0.000000 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 18000it [02:34, 116.57it/s, agent0/loss=0.030, agent10/loss=0.028, agent2/loss=0.030, agent4/loss=0.030, agent6/loss=0.029, agent8/loss=0.028, env_step=90000, len=150, n/ep=60, n/st=9000, rew=0.31]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #5: test_reward: 1.000000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 18000it [02:39, 112.89it/s, agent0/loss=0.035, agent10/loss=0.033, agent2/loss=0.036, agent4/loss=0.033, agent6/loss=0.034, agent8/loss=0.033, env_step=108000, len=150, n/ep=60, n/st=9000, rew=0.57]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 0.700000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 18000it [02:33, 117.08it/s, agent0/loss=0.039, agent10/loss=0.038, agent2/loss=0.039, agent4/loss=0.035, agent6/loss=0.039, agent8/loss=0.038, env_step=126000, len=150, n/ep=60, n/st=9000, rew=0.52]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 1.000000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 18000it [02:39, 112.71it/s, agent0/loss=0.043, agent10/loss=0.041, agent2/loss=0.042, agent4/loss=0.040, agent6/loss=0.043, agent8/loss=0.041, env_step=144000, len=150, n/ep=60, n/st=9000, rew=0.75]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 0.100000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 18000it [02:35, 115.77it/s, agent0/loss=0.048, agent10/loss=0.046, agent2/loss=0.047, agent4/loss=0.045, agent6/loss=0.045, agent8/loss=0.044, env_step=162000, len=150, n/ep=60, n/st=9000, rew=0.65]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 0.700000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 18000it [02:35, 115.60it/s, agent0/loss=0.051, agent10/loss=0.048, agent2/loss=0.051, agent4/loss=0.049, agent6/loss=0.048, agent8/loss=0.048, env_step=180000, len=150, n/ep=60, n/st=9000, rew=0.61]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 18000it [02:35, 115.93it/s, agent0/loss=0.052, agent10/loss=0.050, agent2/loss=0.055, agent4/loss=0.051, agent6/loss=0.051, agent8/loss=0.052, env_step=198000, len=150, n/ep=60, n/st=9000, rew=0.75]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #11: test_reward: -0.100000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 18000it [02:38, 113.54it/s, agent0/loss=0.055, agent10/loss=0.051, agent2/loss=0.056, agent4/loss=0.053, agent6/loss=0.052, agent8/loss=0.055, env_step=216000, len=150, n/ep=60, n/st=9000, rew=0.77]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: 0.120000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 18000it [02:35, 115.87it/s, agent0/loss=0.057, agent10/loss=0.055, agent2/loss=0.057, agent4/loss=0.056, agent6/loss=0.053, agent8/loss=0.056, env_step=234000, len=150, n/ep=60, n/st=9000, rew=0.77]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 18000it [02:36, 115.12it/s, agent0/loss=0.058, agent10/loss=0.057, agent2/loss=0.057, agent4/loss=0.057, agent6/loss=0.054, agent8/loss=0.057, env_step=252000, len=150, n/ep=60, n/st=9000, rew=0.74]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: 0.500000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 18000it [02:35, 115.80it/s, agent0/loss=0.059, agent10/loss=0.057, agent2/loss=0.060, agent4/loss=0.057, agent6/loss=0.054, agent8/loss=0.058, env_step=270000, len=150, n/ep=60, n/st=9000, rew=0.68]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #15: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 18000it [02:31, 118.78it/s, agent0/loss=0.059, agent10/loss=0.057, agent2/loss=0.060, agent4/loss=0.057, agent6/loss=0.055, agent8/loss=0.059, env_step=288000, len=150, n/ep=60, n/st=9000, rew=0.75]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 18000it [02:21, 127.01it/s, agent0/loss=0.061, agent10/loss=0.057, agent2/loss=0.060, agent4/loss=0.059, agent6/loss=0.056, agent8/loss=0.060, env_step=306000, len=150, n/ep=60, n/st=9000, rew=0.81]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: 0.600000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 18000it [02:31, 118.75it/s, agent0/loss=0.062, agent10/loss=0.058, agent2/loss=0.059, agent4/loss=0.059, agent6/loss=0.058, agent8/loss=0.060, env_step=324000, len=150, n/ep=60, n/st=9000, rew=0.81]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 18000it [02:40, 112.17it/s, agent0/loss=0.062, agent10/loss=0.059, agent2/loss=0.061, agent4/loss=0.062, agent6/loss=0.059, agent8/loss=0.058, env_step=342000, len=150, n/ep=60, n/st=9000, rew=0.77]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: 0.700000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 18000it [02:33, 116.90it/s, agent0/loss=0.061, agent10/loss=0.059, agent2/loss=0.061, agent4/loss=0.062, agent6/loss=0.063, agent8/loss=0.059, env_step=360000, len=150, n/ep=60, n/st=9000, rew=0.71]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #20: test_reward: 0.600000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 18000it [02:28, 121.15it/s, agent0/loss=0.060, agent10/loss=0.058, agent2/loss=0.060, agent4/loss=0.061, agent6/loss=0.062, agent8/loss=0.058, env_step=378000, len=150, n/ep=60, n/st=9000, rew=0.80]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: 0.100000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 18000it [02:30, 119.84it/s, agent0/loss=0.062, agent10/loss=0.058, agent2/loss=0.061, agent4/loss=0.061, agent6/loss=0.063, agent8/loss=0.060, env_step=396000, len=150, n/ep=60, n/st=9000, rew=0.84]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #22: test_reward: 0.900000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 18000it [02:27, 121.92it/s, agent0/loss=0.063, agent10/loss=0.058, agent2/loss=0.061, agent4/loss=0.061, agent6/loss=0.061, agent8/loss=0.060, env_step=414000, len=150, n/ep=60, n/st=9000, rew=0.78]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 18000it [02:28, 121.26it/s, agent0/loss=0.063, agent10/loss=0.059, agent2/loss=0.062, agent4/loss=0.062, agent6/loss=0.060, agent8/loss=0.061, env_step=432000, len=150, n/ep=60, n/st=9000, rew=0.86]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #24: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 18000it [02:28, 120.90it/s, agent0/loss=0.062, agent10/loss=0.058, agent2/loss=0.061, agent4/loss=0.063, agent6/loss=0.059, agent8/loss=0.060, env_step=450000, len=150, n/ep=60, n/st=9000, rew=0.74]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #25: test_reward: 0.000000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 18000it [02:28, 121.38it/s, agent0/loss=0.062, agent10/loss=0.059, agent2/loss=0.062, agent4/loss=0.061, agent6/loss=0.060, agent8/loss=0.060, env_step=468000, len=150, n/ep=60, n/st=9000, rew=0.83]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #26: test_reward: 0.800000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 18000it [02:30, 119.50it/s, agent0/loss=0.060, agent10/loss=0.059, agent2/loss=0.062, agent4/loss=0.060, agent6/loss=0.058, agent8/loss=0.060, env_step=486000, len=150, n/ep=60, n/st=9000, rew=0.82]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #27: test_reward: 0.800000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 18000it [02:26, 122.60it/s, agent0/loss=0.060, agent10/loss=0.057, agent2/loss=0.062, agent4/loss=0.060, agent6/loss=0.060, agent8/loss=0.059, env_step=504000, len=150, n/ep=60, n/st=9000, rew=0.85]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #28: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 18000it [02:28, 121.20it/s, agent0/loss=0.059, agent10/loss=0.057, agent2/loss=0.060, agent4/loss=0.062, agent6/loss=0.060, agent8/loss=0.059, env_step=522000, len=150, n/ep=60, n/st=9000, rew=0.70]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 18000it [02:29, 120.38it/s, agent0/loss=0.060, agent10/loss=0.056, agent2/loss=0.059, agent4/loss=0.062, agent6/loss=0.060, agent8/loss=0.060, env_step=540000, len=150, n/ep=60, n/st=9000, rew=0.83]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #30: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 18000it [02:29, 120.47it/s, agent0/loss=0.058, agent10/loss=0.058, agent2/loss=0.058, agent4/loss=0.059, agent6/loss=0.060, agent8/loss=0.060, env_step=558000, len=150, n/ep=60, n/st=9000, rew=0.80]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 18000it [02:28, 120.89it/s, agent0/loss=0.061, agent10/loss=0.059, agent2/loss=0.059, agent4/loss=0.059, agent6/loss=0.060, agent8/loss=0.060, env_step=576000, len=150, n/ep=60, n/st=9000, rew=0.75]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #32: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 18000it [02:30, 119.90it/s, agent0/loss=0.062, agent10/loss=0.060, agent2/loss=0.061, agent4/loss=0.060, agent6/loss=0.058, agent8/loss=0.062, env_step=594000, len=150, n/ep=60, n/st=9000, rew=0.74]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 18000it [02:30, 119.31it/s, agent0/loss=0.059, agent10/loss=0.059, agent2/loss=0.060, agent4/loss=0.061, agent6/loss=0.058, agent8/loss=0.062, env_step=612000, len=150, n/ep=60, n/st=9000, rew=0.82]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #34: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 18000it [02:30, 119.72it/s, agent0/loss=0.061, agent10/loss=0.058, agent2/loss=0.062, agent4/loss=0.062, agent6/loss=0.059, agent8/loss=0.062, env_step=630000, len=150, n/ep=60, n/st=9000, rew=0.75]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 18000it [02:35, 115.53it/s, agent0/loss=0.060, agent10/loss=0.059, agent2/loss=0.062, agent4/loss=0.062, agent6/loss=0.059, agent8/loss=0.061, env_step=648000, len=150, n/ep=60, n/st=9000, rew=0.75]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #36: test_reward: 0.600000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 18000it [02:41, 111.51it/s, agent0/loss=0.063, agent10/loss=0.058, agent2/loss=0.061, agent4/loss=0.062, agent6/loss=0.059, agent8/loss=0.060, env_step=666000, len=150, n/ep=60, n/st=9000, rew=0.81]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: 0.700000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 18000it [02:30, 119.96it/s, agent0/loss=0.059, agent10/loss=0.056, agent2/loss=0.058, agent4/loss=0.061, agent6/loss=0.060, agent8/loss=0.059, env_step=684000, len=150, n/ep=60, n/st=9000, rew=0.83]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #38: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 18000it [02:18, 129.75it/s, agent0/loss=0.060, agent10/loss=0.056, agent2/loss=0.057, agent4/loss=0.059, agent6/loss=0.061, agent8/loss=0.058, env_step=702000, len=150, n/ep=60, n/st=9000, rew=0.77]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: -0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 18000it [02:30, 119.39it/s, agent0/loss=0.059, agent10/loss=0.056, agent2/loss=0.057, agent4/loss=0.058, agent6/loss=0.058, agent8/loss=0.058, env_step=720000, len=150, n/ep=60, n/st=9000, rew=0.82]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #40: test_reward: -0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 18000it [02:21, 126.98it/s, agent0/loss=0.059, agent10/loss=0.057, agent2/loss=0.058, agent4/loss=0.058, agent6/loss=0.056, agent8/loss=0.058, env_step=738000, len=150, n/ep=60, n/st=9000, rew=0.87]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 18000it [02:14, 134.33it/s, agent0/loss=0.059, agent10/loss=0.058, agent2/loss=0.057, agent4/loss=0.059, agent6/loss=0.056, agent8/loss=0.058, env_step=756000, len=150, n/ep=60, n/st=9000, rew=0.79]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #42: test_reward: 0.500000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 18000it [02:19, 128.92it/s, agent0/loss=0.059, agent10/loss=0.058, agent2/loss=0.056, agent4/loss=0.059, agent6/loss=0.058, agent8/loss=0.058, env_step=774000, len=150, n/ep=60, n/st=9000, rew=0.83]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: -0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 18000it [02:30, 119.25it/s, agent0/loss=0.059, agent10/loss=0.055, agent2/loss=0.057, agent4/loss=0.059, agent6/loss=0.058, agent8/loss=0.057, env_step=792000, len=150, n/ep=60, n/st=9000, rew=0.85]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 18000it [02:31, 119.20it/s, agent0/loss=0.058, agent10/loss=0.056, agent2/loss=0.058, agent4/loss=0.059, agent6/loss=0.059, agent8/loss=0.058, env_step=810000, len=150, n/ep=60, n/st=9000, rew=0.85]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #45: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #46: 18000it [02:12, 135.75it/s, agent0/loss=0.061, agent10/loss=0.057, agent2/loss=0.059, agent4/loss=0.060, agent6/loss=0.057, agent8/loss=0.057, env_step=828000, len=150, n/ep=60, n/st=9000, rew=0.73]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #46: test_reward: 1.000000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #47: 18000it [02:13, 135.19it/s, agent0/loss=0.063, agent10/loss=0.058, agent2/loss=0.059, agent4/loss=0.062, agent6/loss=0.057, agent8/loss=0.058, env_step=846000, len=150, n/ep=60, n/st=9000, rew=0.94]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #47: test_reward: 1.000000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #48: 18000it [02:11, 136.61it/s, agent0/loss=0.065, agent10/loss=0.057, agent2/loss=0.058, agent4/loss=0.061, agent6/loss=0.057, agent8/loss=0.059, env_step=864000, len=150, n/ep=60, n/st=9000, rew=0.74]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #48: test_reward: 0.500000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #49: 18000it [02:12, 135.70it/s, agent0/loss=0.063, agent10/loss=0.058, agent2/loss=0.060, agent4/loss=0.059, agent6/loss=0.057, agent8/loss=0.061, env_step=882000, len=150, n/ep=60, n/st=9000, rew=0.75]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #49: test_reward: 0.800000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #50: 18000it [02:11, 136.41it/s, agent0/loss=0.062, agent10/loss=0.057, agent2/loss=0.060, agent4/loss=0.058, agent6/loss=0.056, agent8/loss=0.059, env_step=900000, len=150, n/ep=60, n/st=9000, rew=0.84]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #50: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #51: 18000it [02:14, 134.20it/s, agent0/loss=0.061, agent10/loss=0.058, agent2/loss=0.060, agent4/loss=0.057, agent6/loss=0.055, agent8/loss=0.059, env_step=918000, len=150, n/ep=60, n/st=9000, rew=0.85]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #51: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #52: 18000it [02:13, 135.00it/s, agent0/loss=0.061, agent10/loss=0.057, agent2/loss=0.057, agent4/loss=0.059, agent6/loss=0.056, agent8/loss=0.059, env_step=936000, len=150, n/ep=60, n/st=9000, rew=0.86]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #52: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #53: 18000it [02:12, 135.70it/s, agent0/loss=0.060, agent10/loss=0.058, agent2/loss=0.056, agent4/loss=0.058, agent6/loss=0.057, agent8/loss=0.059, env_step=954000, len=150, n/ep=60, n/st=9000, rew=0.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #53: test_reward: 0.700000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #54: 18000it [02:12, 136.31it/s, agent0/loss=0.059, agent10/loss=0.057, agent2/loss=0.054, agent4/loss=0.057, agent6/loss=0.058, agent8/loss=0.058, env_step=972000, len=150, n/ep=60, n/st=9000, rew=0.83]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #54: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #55: 18000it [02:12, 135.95it/s, agent0/loss=0.057, agent10/loss=0.057, agent2/loss=0.055, agent4/loss=0.056, agent6/loss=0.057, agent8/loss=0.058, env_step=990000, len=150, n/ep=60, n/st=9000, rew=0.89]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #55: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #56: 18000it [02:12, 135.87it/s, agent0/loss=0.057, agent10/loss=0.057, agent2/loss=0.057, agent4/loss=0.056, agent6/loss=0.058, agent8/loss=0.057, env_step=1008000, len=150, n/ep=60, n/st=9000, rew=0.85]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #56: test_reward: -0.100000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #57: 18000it [02:11, 137.25it/s, agent0/loss=0.060, agent10/loss=0.056, agent2/loss=0.059, agent4/loss=0.057, agent6/loss=0.058, agent8/loss=0.054, env_step=1026000, len=150, n/ep=60, n/st=9000, rew=0.86]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #57: test_reward: 0.900000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #58: 18000it [02:11, 136.68it/s, agent0/loss=0.060, agent10/loss=0.056, agent2/loss=0.058, agent4/loss=0.057, agent6/loss=0.059, agent8/loss=0.053, env_step=1044000, len=150, n/ep=60, n/st=9000, rew=0.83]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #58: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #59: 18000it [02:11, 137.12it/s, agent0/loss=0.062, agent10/loss=0.055, agent2/loss=0.059, agent4/loss=0.057, agent6/loss=0.058, agent8/loss=0.054, env_step=1062000, len=150, n/ep=60, n/st=9000, rew=0.95]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #59: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #60: 18000it [02:14, 134.24it/s, agent0/loss=0.061, agent10/loss=0.055, agent2/loss=0.059, agent4/loss=0.056, agent6/loss=0.059, agent8/loss=0.055, env_step=1080000, len=150, n/ep=60, n/st=9000, rew=0.96]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #60: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #61: 18000it [02:14, 134.17it/s, agent0/loss=0.060, agent10/loss=0.055, agent2/loss=0.060, agent4/loss=0.056, agent6/loss=0.058, agent8/loss=0.056, env_step=1098000, len=150, n/ep=60, n/st=9000, rew=0.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #61: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #62: 18000it [02:15, 132.86it/s, agent0/loss=0.059, agent10/loss=0.056, agent2/loss=0.059, agent4/loss=0.058, agent6/loss=0.058, agent8/loss=0.056, env_step=1116000, len=150, n/ep=60, n/st=9000, rew=0.82]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #62: test_reward: 0.100000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #63: 18000it [02:15, 132.74it/s, agent0/loss=0.058, agent10/loss=0.054, agent2/loss=0.057, agent4/loss=0.059, agent6/loss=0.056, agent8/loss=0.057, env_step=1134000, len=150, n/ep=60, n/st=9000, rew=0.77]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #63: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #64: 18000it [02:14, 134.21it/s, agent0/loss=0.059, agent10/loss=0.054, agent2/loss=0.057, agent4/loss=0.060, agent6/loss=0.058, agent8/loss=0.056, env_step=1152000, len=150, n/ep=60, n/st=9000, rew=0.97]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #64: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #65: 18000it [02:14, 134.29it/s, agent0/loss=0.059, agent10/loss=0.055, agent2/loss=0.058, agent4/loss=0.059, agent6/loss=0.058, agent8/loss=0.058, env_step=1170000, len=150, n/ep=60, n/st=9000, rew=0.94]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #65: test_reward: 0.500000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #66: 18000it [02:14, 134.02it/s, agent0/loss=0.059, agent10/loss=0.053, agent2/loss=0.058, agent4/loss=0.058, agent6/loss=0.058, agent8/loss=0.059, env_step=1188000, len=150, n/ep=60, n/st=9000, rew=0.84]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #66: test_reward: 0.500000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #67: 18000it [02:11, 136.73it/s, agent0/loss=0.059, agent10/loss=0.053, agent2/loss=0.056, agent4/loss=0.057, agent6/loss=0.058, agent8/loss=0.058, env_step=1206000, len=150, n/ep=60, n/st=9000, rew=0.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #67: test_reward: 0.500000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #68: 18000it [02:13, 135.13it/s, agent0/loss=0.057, agent10/loss=0.052, agent2/loss=0.057, agent4/loss=0.056, agent6/loss=0.056, agent8/loss=0.057, env_step=1224000, len=150, n/ep=60, n/st=9000, rew=0.86]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #68: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #69: 18000it [02:13, 134.59it/s, agent0/loss=0.058, agent10/loss=0.051, agent2/loss=0.057, agent4/loss=0.056, agent6/loss=0.055, agent8/loss=0.056, env_step=1242000, len=150, n/ep=60, n/st=9000, rew=0.89]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #69: test_reward: 0.600000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #70: 18000it [02:11, 136.62it/s, agent0/loss=0.057, agent10/loss=0.050, agent2/loss=0.057, agent4/loss=0.056, agent6/loss=0.053, agent8/loss=0.053, env_step=1260000, len=150, n/ep=60, n/st=9000, rew=0.83]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #70: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #71: 18000it [02:13, 135.05it/s, agent0/loss=0.056, agent10/loss=0.050, agent2/loss=0.055, agent4/loss=0.056, agent6/loss=0.053, agent8/loss=0.051, env_step=1278000, len=150, n/ep=60, n/st=9000, rew=0.96]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #71: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #72: 18000it [02:12, 135.37it/s, agent0/loss=0.057, agent10/loss=0.051, agent2/loss=0.055, agent4/loss=0.055, agent6/loss=0.053, agent8/loss=0.051, env_step=1296000, len=150, n/ep=60, n/st=9000, rew=0.97]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #72: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #73: 18000it [02:14, 133.73it/s, agent0/loss=0.055, agent10/loss=0.051, agent2/loss=0.054, agent4/loss=0.054, agent6/loss=0.055, agent8/loss=0.054, env_step=1314000, len=150, n/ep=60, n/st=9000, rew=0.91]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #73: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #74: 18000it [02:12, 136.09it/s, agent0/loss=0.056, agent10/loss=0.052, agent2/loss=0.056, agent4/loss=0.053, agent6/loss=0.054, agent8/loss=0.054, env_step=1332000, len=150, n/ep=60, n/st=9000, rew=0.98]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #74: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #75: 18000it [02:13, 135.27it/s, agent0/loss=0.056, agent10/loss=0.052, agent2/loss=0.056, agent4/loss=0.056, agent6/loss=0.055, agent8/loss=0.053, env_step=1350000, len=150, n/ep=60, n/st=9000, rew=0.96]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #75: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #76: 18000it [02:13, 134.98it/s, agent0/loss=0.057, agent10/loss=0.053, agent2/loss=0.058, agent4/loss=0.057, agent6/loss=0.057, agent8/loss=0.055, env_step=1368000, len=150, n/ep=60, n/st=9000, rew=0.93]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #76: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #77: 18000it [02:11, 136.92it/s, agent0/loss=0.057, agent10/loss=0.053, agent2/loss=0.058, agent4/loss=0.057, agent6/loss=0.056, agent8/loss=0.055, env_step=1386000, len=150, n/ep=60, n/st=9000, rew=0.94]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #77: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #78: 18000it [02:14, 133.49it/s, agent0/loss=0.057, agent10/loss=0.052, agent2/loss=0.057, agent4/loss=0.055, agent6/loss=0.054, agent8/loss=0.053, env_step=1404000, len=150, n/ep=60, n/st=9000, rew=0.97]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #78: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #79: 18000it [02:14, 133.51it/s, agent0/loss=0.055, agent10/loss=0.053, agent2/loss=0.056, agent4/loss=0.057, agent6/loss=0.053, agent8/loss=0.053, env_step=1422000, len=150, n/ep=60, n/st=9000, rew=0.95]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #79: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #80: 18000it [02:13, 134.62it/s, agent0/loss=0.057, agent10/loss=0.053, agent2/loss=0.055, agent4/loss=0.057, agent6/loss=0.054, agent8/loss=0.054, env_step=1440000, len=150, n/ep=60, n/st=9000, rew=0.94]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #80: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #81: 18000it [02:14, 133.82it/s, agent0/loss=0.056, agent10/loss=0.055, agent2/loss=0.054, agent4/loss=0.055, agent6/loss=0.055, agent8/loss=0.054, env_step=1458000, len=150, n/ep=60, n/st=9000, rew=0.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #81: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #82: 18000it [02:12, 135.96it/s, agent0/loss=0.057, agent10/loss=0.054, agent2/loss=0.054, agent4/loss=0.053, agent6/loss=0.055, agent8/loss=0.054, env_step=1476000, len=150, n/ep=60, n/st=9000, rew=0.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #82: test_reward: 0.100000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #83: 18000it [02:12, 135.43it/s, agent0/loss=0.056, agent10/loss=0.054, agent2/loss=0.053, agent4/loss=0.053, agent6/loss=0.054, agent8/loss=0.053, env_step=1494000, len=150, n/ep=60, n/st=9000, rew=0.95]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #83: test_reward: 0.100000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #84: 18000it [02:14, 133.88it/s, agent0/loss=0.055, agent10/loss=0.053, agent2/loss=0.053, agent4/loss=0.055, agent6/loss=0.054, agent8/loss=0.051, env_step=1512000, len=150, n/ep=60, n/st=9000, rew=0.85]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #84: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #85: 18000it [02:13, 134.52it/s, agent0/loss=0.051, agent10/loss=0.052, agent2/loss=0.053, agent4/loss=0.055, agent6/loss=0.054, agent8/loss=0.050, env_step=1530000, len=150, n/ep=60, n/st=9000, rew=1.05]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #85: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #86: 18000it [02:14, 133.41it/s, agent0/loss=0.050, agent10/loss=0.051, agent2/loss=0.051, agent4/loss=0.055, agent6/loss=0.052, agent8/loss=0.050, env_step=1548000, len=150, n/ep=60, n/st=9000, rew=0.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #86: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #87: 18000it [02:11, 137.13it/s, agent0/loss=0.052, agent10/loss=0.050, agent2/loss=0.050, agent4/loss=0.054, agent6/loss=0.053, agent8/loss=0.050, env_step=1566000, len=150, n/ep=60, n/st=9000, rew=0.97]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #87: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #88: 18000it [02:14, 133.89it/s, agent0/loss=0.053, agent10/loss=0.049, agent2/loss=0.051, agent4/loss=0.051, agent6/loss=0.051, agent8/loss=0.050, env_step=1584000, len=150, n/ep=60, n/st=9000, rew=0.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #88: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #89: 18000it [02:13, 135.13it/s, agent0/loss=0.053, agent10/loss=0.049, agent2/loss=0.051, agent4/loss=0.051, agent6/loss=0.051, agent8/loss=0.048, env_step=1602000, len=150, n/ep=60, n/st=9000, rew=0.87]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #89: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #90: 18000it [02:10, 137.90it/s, agent0/loss=0.053, agent10/loss=0.049, agent2/loss=0.050, agent4/loss=0.053, agent6/loss=0.052, agent8/loss=0.050, env_step=1620000, len=150, n/ep=60, n/st=9000, rew=0.90]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #90: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #91: 18000it [02:14, 133.59it/s, agent0/loss=0.052, agent10/loss=0.049, agent2/loss=0.050, agent4/loss=0.054, agent6/loss=0.051, agent8/loss=0.051, env_step=1638000, len=150, n/ep=60, n/st=9000, rew=0.94]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #91: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #92: 18000it [02:14, 133.52it/s, agent0/loss=0.053, agent10/loss=0.051, agent2/loss=0.051, agent4/loss=0.053, agent6/loss=0.051, agent8/loss=0.051, env_step=1656000, len=150, n/ep=60, n/st=9000, rew=0.97]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #92: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #93: 18000it [02:12, 135.70it/s, agent0/loss=0.052, agent10/loss=0.052, agent2/loss=0.053, agent4/loss=0.050, agent6/loss=0.050, agent8/loss=0.050, env_step=1674000, len=150, n/ep=60, n/st=9000, rew=0.96]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #93: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #94: 18000it [02:13, 135.15it/s, agent0/loss=0.052, agent10/loss=0.051, agent2/loss=0.052, agent4/loss=0.051, agent6/loss=0.049, agent8/loss=0.050, env_step=1692000, len=150, n/ep=60, n/st=9000, rew=0.96]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #94: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #95: 18000it [02:14, 133.90it/s, agent0/loss=0.051, agent10/loss=0.050, agent2/loss=0.050, agent4/loss=0.052, agent6/loss=0.049, agent8/loss=0.049, env_step=1710000, len=150, n/ep=60, n/st=9000, rew=0.94]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #95: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #96: 18000it [02:13, 134.59it/s, agent0/loss=0.052, agent10/loss=0.048, agent2/loss=0.048, agent4/loss=0.053, agent6/loss=0.050, agent8/loss=0.048, env_step=1728000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #96: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #97: 18000it [02:12, 135.86it/s, agent0/loss=0.053, agent10/loss=0.049, agent2/loss=0.050, agent4/loss=0.052, agent6/loss=0.049, agent8/loss=0.049, env_step=1746000, len=150, n/ep=60, n/st=9000, rew=0.94]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #97: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #98: 18000it [02:12, 135.69it/s, agent0/loss=0.053, agent10/loss=0.049, agent2/loss=0.052, agent4/loss=0.049, agent6/loss=0.050, agent8/loss=0.050, env_step=1764000, len=150, n/ep=60, n/st=9000, rew=0.99]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #98: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #99: 18000it [02:13, 134.88it/s, agent0/loss=0.050, agent10/loss=0.047, agent2/loss=0.052, agent4/loss=0.049, agent6/loss=0.050, agent8/loss=0.051, env_step=1782000, len=150, n/ep=60, n/st=9000, rew=0.89]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #99: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #100: 18000it [02:11, 136.75it/s, agent0/loss=0.050, agent10/loss=0.046, agent2/loss=0.051, agent4/loss=0.048, agent6/loss=0.050, agent8/loss=0.049, env_step=1800000, len=150, n/ep=60, n/st=9000, rew=0.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #100: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #101: 18000it [02:12, 136.35it/s, agent0/loss=0.049, agent10/loss=0.045, agent2/loss=0.050, agent4/loss=0.049, agent6/loss=0.050, agent8/loss=0.048, env_step=1818000, len=150, n/ep=60, n/st=9000, rew=1.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #101: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #102: 18000it [02:16, 131.62it/s, agent0/loss=0.048, agent10/loss=0.045, agent2/loss=0.050, agent4/loss=0.048, agent6/loss=0.048, agent8/loss=0.046, env_step=1836000, len=150, n/ep=60, n/st=9000, rew=0.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #102: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #103: 18000it [02:14, 134.11it/s, agent0/loss=0.049, agent10/loss=0.045, agent2/loss=0.047, agent4/loss=0.049, agent6/loss=0.047, agent8/loss=0.046, env_step=1854000, len=150, n/ep=60, n/st=9000, rew=0.95]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #103: test_reward: 0.100000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #104: 18000it [02:16, 132.31it/s, agent0/loss=0.048, agent10/loss=0.046, agent2/loss=0.048, agent4/loss=0.049, agent6/loss=0.047, agent8/loss=0.046, env_step=1872000, len=150, n/ep=60, n/st=9000, rew=1.04]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #104: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #105: 18000it [02:13, 134.76it/s, agent0/loss=0.048, agent10/loss=0.047, agent2/loss=0.046, agent4/loss=0.047, agent6/loss=0.047, agent8/loss=0.046, env_step=1890000, len=150, n/ep=60, n/st=9000, rew=1.04]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #105: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #106: 18000it [02:13, 135.22it/s, agent0/loss=0.047, agent10/loss=0.047, agent2/loss=0.048, agent4/loss=0.048, agent6/loss=0.047, agent8/loss=0.047, env_step=1908000, len=150, n/ep=60, n/st=9000, rew=0.98]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #106: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #107: 18000it [02:13, 134.85it/s, agent0/loss=0.049, agent10/loss=0.048, agent2/loss=0.047, agent4/loss=0.048, agent6/loss=0.048, agent8/loss=0.048, env_step=1926000, len=150, n/ep=60, n/st=9000, rew=1.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #107: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #108: 18000it [02:15, 133.02it/s, agent0/loss=0.051, agent10/loss=0.047, agent2/loss=0.048, agent4/loss=0.049, agent6/loss=0.048, agent8/loss=0.049, env_step=1944000, len=150, n/ep=60, n/st=9000, rew=0.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #108: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #109: 18000it [02:15, 133.33it/s, agent0/loss=0.051, agent10/loss=0.048, agent2/loss=0.048, agent4/loss=0.048, agent6/loss=0.047, agent8/loss=0.049, env_step=1962000, len=150, n/ep=60, n/st=9000, rew=0.98]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #109: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #110: 18000it [02:13, 134.39it/s, agent0/loss=0.049, agent10/loss=0.045, agent2/loss=0.047, agent4/loss=0.048, agent6/loss=0.044, agent8/loss=0.048, env_step=1980000, len=150, n/ep=60, n/st=9000, rew=1.05]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #110: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #111: 18000it [02:15, 132.80it/s, agent0/loss=0.045, agent10/loss=0.044, agent2/loss=0.047, agent4/loss=0.047, agent6/loss=0.044, agent8/loss=0.046, env_step=1998000, len=150, n/ep=60, n/st=9000, rew=0.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #111: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #112: 18000it [02:13, 134.33it/s, agent0/loss=0.044, agent10/loss=0.042, agent2/loss=0.046, agent4/loss=0.045, agent6/loss=0.045, agent8/loss=0.043, env_step=2016000, len=150, n/ep=60, n/st=9000, rew=0.97]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #112: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #113: 18000it [02:16, 131.39it/s, agent0/loss=0.044, agent10/loss=0.041, agent2/loss=0.046, agent4/loss=0.045, agent6/loss=0.045, agent8/loss=0.043, env_step=2034000, len=150, n/ep=60, n/st=9000, rew=0.98]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #113: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #114: 18000it [02:14, 134.30it/s, agent0/loss=0.044, agent10/loss=0.041, agent2/loss=0.045, agent4/loss=0.043, agent6/loss=0.044, agent8/loss=0.042, env_step=2052000, len=150, n/ep=60, n/st=9000, rew=0.99]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #114: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #115: 18000it [02:15, 132.73it/s, agent0/loss=0.044, agent10/loss=0.042, agent2/loss=0.046, agent4/loss=0.043, agent6/loss=0.043, agent8/loss=0.044, env_step=2070000, len=150, n/ep=60, n/st=9000, rew=1.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #115: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #116: 18000it [02:14, 133.63it/s, agent0/loss=0.044, agent10/loss=0.043, agent2/loss=0.045, agent4/loss=0.044, agent6/loss=0.043, agent8/loss=0.043, env_step=2088000, len=150, n/ep=60, n/st=9000, rew=0.99]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #116: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #117: 18000it [02:13, 134.70it/s, agent0/loss=0.043, agent10/loss=0.041, agent2/loss=0.044, agent4/loss=0.044, agent6/loss=0.042, agent8/loss=0.042, env_step=2106000, len=150, n/ep=60, n/st=9000, rew=0.92]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #117: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #118: 18000it [02:17, 131.18it/s, agent0/loss=0.042, agent10/loss=0.040, agent2/loss=0.044, agent4/loss=0.044, agent6/loss=0.044, agent8/loss=0.043, env_step=2124000, len=150, n/ep=60, n/st=9000, rew=0.97]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #118: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #119: 18000it [02:15, 132.79it/s, agent0/loss=0.042, agent10/loss=0.041, agent2/loss=0.043, agent4/loss=0.044, agent6/loss=0.043, agent8/loss=0.043, env_step=2142000, len=150, n/ep=60, n/st=9000, rew=1.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #119: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #120: 18000it [02:14, 134.12it/s, agent0/loss=0.041, agent10/loss=0.042, agent2/loss=0.044, agent4/loss=0.043, agent6/loss=0.043, agent8/loss=0.043, env_step=2160000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #120: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #121: 18000it [02:14, 134.02it/s, agent0/loss=0.042, agent10/loss=0.043, agent2/loss=0.046, agent4/loss=0.043, agent6/loss=0.041, agent8/loss=0.044, env_step=2178000, len=150, n/ep=60, n/st=9000, rew=0.96]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #121: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #122: 18000it [02:12, 136.33it/s, agent0/loss=0.042, agent10/loss=0.042, agent2/loss=0.044, agent4/loss=0.044, agent6/loss=0.041, agent8/loss=0.044, env_step=2196000, len=150, n/ep=60, n/st=9000, rew=1.03]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #122: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #123: 18000it [02:14, 133.57it/s, agent0/loss=0.043, agent10/loss=0.041, agent2/loss=0.044, agent4/loss=0.044, agent6/loss=0.040, agent8/loss=0.045, env_step=2214000, len=150, n/ep=60, n/st=9000, rew=1.04]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #123: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #124: 18000it [02:13, 134.88it/s, agent0/loss=0.042, agent10/loss=0.040, agent2/loss=0.042, agent4/loss=0.043, agent6/loss=0.040, agent8/loss=0.045, env_step=2232000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #124: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #125: 18000it [02:14, 134.31it/s, agent0/loss=0.040, agent10/loss=0.040, agent2/loss=0.042, agent4/loss=0.043, agent6/loss=0.040, agent8/loss=0.044, env_step=2250000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #125: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #126: 18000it [02:14, 133.46it/s, agent0/loss=0.039, agent10/loss=0.042, agent2/loss=0.043, agent4/loss=0.043, agent6/loss=0.040, agent8/loss=0.042, env_step=2268000, len=150, n/ep=60, n/st=9000, rew=1.08]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #126: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #127: 18000it [02:13, 134.60it/s, agent0/loss=0.039, agent10/loss=0.043, agent2/loss=0.044, agent4/loss=0.043, agent6/loss=0.041, agent8/loss=0.042, env_step=2286000, len=150, n/ep=60, n/st=9000, rew=1.07]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #127: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #128: 18000it [02:14, 133.64it/s, agent0/loss=0.040, agent10/loss=0.042, agent2/loss=0.043, agent4/loss=0.043, agent6/loss=0.042, agent8/loss=0.044, env_step=2304000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #128: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #129: 18000it [02:13, 134.90it/s, agent0/loss=0.040, agent10/loss=0.042, agent2/loss=0.042, agent4/loss=0.042, agent6/loss=0.042, agent8/loss=0.044, env_step=2322000, len=150, n/ep=60, n/st=9000, rew=1.06]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #129: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #130: 18000it [02:13, 134.80it/s, agent0/loss=0.039, agent10/loss=0.043, agent2/loss=0.043, agent4/loss=0.042, agent6/loss=0.040, agent8/loss=0.042, env_step=2340000, len=150, n/ep=60, n/st=9000, rew=1.03]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #130: test_reward: 0.400000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #131: 18000it [02:16, 131.62it/s, agent0/loss=0.039, agent10/loss=0.043, agent2/loss=0.043, agent4/loss=0.041, agent6/loss=0.040, agent8/loss=0.040, env_step=2358000, len=150, n/ep=60, n/st=9000, rew=1.07]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #131: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #132: 18000it [02:14, 134.05it/s, agent0/loss=0.039, agent10/loss=0.041, agent2/loss=0.042, agent4/loss=0.041, agent6/loss=0.041, agent8/loss=0.039, env_step=2376000, len=150, n/ep=60, n/st=9000, rew=1.04]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #132: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #133: 18000it [02:16, 132.35it/s, agent0/loss=0.038, agent10/loss=0.039, agent2/loss=0.041, agent4/loss=0.041, agent6/loss=0.041, agent8/loss=0.040, env_step=2394000, len=150, n/ep=60, n/st=9000, rew=1.03]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #133: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #134: 18000it [02:13, 134.99it/s, agent0/loss=0.038, agent10/loss=0.039, agent2/loss=0.041, agent4/loss=0.040, agent6/loss=0.041, agent8/loss=0.041, env_step=2412000, len=150, n/ep=60, n/st=9000, rew=1.05]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #134: test_reward: 0.200000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #135: 18000it [02:16, 131.65it/s, agent0/loss=0.037, agent10/loss=0.040, agent2/loss=0.041, agent4/loss=0.039, agent6/loss=0.040, agent8/loss=0.042, env_step=2430000, len=150, n/ep=60, n/st=9000, rew=1.04]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #135: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #136: 18000it [02:14, 134.24it/s, agent0/loss=0.038, agent10/loss=0.038, agent2/loss=0.040, agent4/loss=0.038, agent6/loss=0.039, agent8/loss=0.041, env_step=2448000, len=150, n/ep=60, n/st=9000, rew=1.09]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #136: test_reward: 0.300000 ± 0.000000, best_reward: 1.000000 ± 0.000000 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #137: 18000it [02:14, 133.58it/s, agent0/loss=0.037, agent10/loss=0.038, agent2/loss=0.039, agent4/loss=0.038, agent6/loss=0.039, agent8/loss=0.039, env_step=2466000, len=150, n/ep=60, n/st=9000, rew=1.05]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #137: test_reward: 1.100000 ± 0.000000, best_reward: 1.100000 ± 0.000000 in #137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #138: 18000it [02:16, 131.74it/s, agent0/loss=0.037, agent10/loss=0.038, agent2/loss=0.039, agent4/loss=0.039, agent6/loss=0.038, agent8/loss=0.037, env_step=2484000, len=150, n/ep=60, n/st=9000, rew=1.03]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #138: test_reward: 1.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #139: 18000it [02:12, 135.81it/s, agent0/loss=0.036, agent10/loss=0.038, agent2/loss=0.037, agent4/loss=0.039, agent6/loss=0.036, agent8/loss=0.037, env_step=2502000, len=150, n/ep=60, n/st=9000, rew=1.09]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #139: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #140: 18000it [02:14, 133.64it/s, agent0/loss=0.035, agent10/loss=0.038, agent2/loss=0.037, agent4/loss=0.037, agent6/loss=0.034, agent8/loss=0.035, env_step=2520000, len=150, n/ep=60, n/st=9000, rew=1.05]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #140: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #141: 18000it [02:14, 133.93it/s, agent0/loss=0.036, agent10/loss=0.036, agent2/loss=0.036, agent4/loss=0.036, agent6/loss=0.034, agent8/loss=0.035, env_step=2538000, len=150, n/ep=60, n/st=9000, rew=1.10]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #141: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #142: 18000it [02:15, 133.32it/s, agent0/loss=0.037, agent10/loss=0.037, agent2/loss=0.037, agent4/loss=0.037, agent6/loss=0.034, agent8/loss=0.033, env_step=2556000, len=150, n/ep=60, n/st=9000, rew=1.12]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #142: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #143: 18000it [02:18, 130.11it/s, agent0/loss=0.038, agent10/loss=0.036, agent2/loss=0.036, agent4/loss=0.037, agent6/loss=0.034, agent8/loss=0.032, env_step=2574000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #143: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #144: 18000it [02:14, 133.37it/s, agent0/loss=0.038, agent10/loss=0.036, agent2/loss=0.034, agent4/loss=0.035, agent6/loss=0.034, agent8/loss=0.032, env_step=2592000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #144: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #145: 18000it [02:14, 134.19it/s, agent0/loss=0.036, agent10/loss=0.035, agent2/loss=0.033, agent4/loss=0.033, agent6/loss=0.035, agent8/loss=0.034, env_step=2610000, len=150, n/ep=60, n/st=9000, rew=1.07]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #145: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #146: 18000it [02:14, 133.83it/s, agent0/loss=0.035, agent10/loss=0.034, agent2/loss=0.034, agent4/loss=0.034, agent6/loss=0.035, agent8/loss=0.034, env_step=2628000, len=150, n/ep=60, n/st=9000, rew=1.11]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #146: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #147: 18000it [02:16, 132.15it/s, agent0/loss=0.035, agent10/loss=0.033, agent2/loss=0.035, agent4/loss=0.033, agent6/loss=0.034, agent8/loss=0.035, env_step=2646000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #147: test_reward: 1.000000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #148: 18000it [02:15, 133.08it/s, agent0/loss=0.034, agent10/loss=0.033, agent2/loss=0.034, agent4/loss=0.034, agent6/loss=0.033, agent8/loss=0.034, env_step=2664000, len=150, n/ep=60, n/st=9000, rew=1.13]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #148: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #149: 18000it [02:16, 132.32it/s, agent0/loss=0.034, agent10/loss=0.032, agent2/loss=0.032, agent4/loss=0.033, agent6/loss=0.033, agent8/loss=0.032, env_step=2682000, len=150, n/ep=60, n/st=9000, rew=1.08]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #149: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #150: 18000it [02:15, 133.09it/s, agent0/loss=0.033, agent10/loss=0.031, agent2/loss=0.033, agent4/loss=0.033, agent6/loss=0.032, agent8/loss=0.032, env_step=2700000, len=150, n/ep=60, n/st=9000, rew=1.09]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #150: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #151: 18000it [02:16, 131.88it/s, agent0/loss=0.032, agent10/loss=0.030, agent2/loss=0.033, agent4/loss=0.033, agent6/loss=0.032, agent8/loss=0.033, env_step=2718000, len=150, n/ep=60, n/st=9000, rew=1.08]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #151: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #152: 18000it [02:18, 129.99it/s, agent0/loss=0.032, agent10/loss=0.029, agent2/loss=0.033, agent4/loss=0.033, agent6/loss=0.032, agent8/loss=0.032, env_step=2736000, len=150, n/ep=60, n/st=9000, rew=1.07]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #152: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #153: 18000it [02:15, 132.55it/s, agent0/loss=0.030, agent10/loss=0.030, agent2/loss=0.032, agent4/loss=0.032, agent6/loss=0.031, agent8/loss=0.030, env_step=2754000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #153: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #154: 18000it [02:17, 130.93it/s, agent0/loss=0.031, agent10/loss=0.031, agent2/loss=0.032, agent4/loss=0.032, agent6/loss=0.030, agent8/loss=0.030, env_step=2772000, len=150, n/ep=60, n/st=9000, rew=1.07]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #154: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #155: 18000it [02:16, 131.73it/s, agent0/loss=0.031, agent10/loss=0.032, agent2/loss=0.031, agent4/loss=0.032, agent6/loss=0.030, agent8/loss=0.029, env_step=2790000, len=150, n/ep=60, n/st=9000, rew=1.05]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #155: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #156: 18000it [02:12, 135.62it/s, agent0/loss=0.031, agent10/loss=0.032, agent2/loss=0.030, agent4/loss=0.031, agent6/loss=0.031, agent8/loss=0.030, env_step=2808000, len=150, n/ep=60, n/st=9000, rew=1.06]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #156: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #157: 18000it [02:16, 132.03it/s, agent0/loss=0.031, agent10/loss=0.030, agent2/loss=0.031, agent4/loss=0.030, agent6/loss=0.030, agent8/loss=0.029, env_step=2826000, len=150, n/ep=60, n/st=9000, rew=1.18]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #157: test_reward: 1.100000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #158: 18000it [02:18, 129.99it/s, agent0/loss=0.031, agent10/loss=0.028, agent2/loss=0.032, agent4/loss=0.031, agent6/loss=0.028, agent8/loss=0.030, env_step=2844000, len=150, n/ep=60, n/st=9000, rew=1.10]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #158: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #159: 18000it [02:12, 135.52it/s, agent0/loss=0.031, agent10/loss=0.028, agent2/loss=0.031, agent4/loss=0.030, agent6/loss=0.029, agent8/loss=0.030, env_step=2862000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #159: test_reward: 1.100000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #160: 18000it [02:17, 130.54it/s, agent0/loss=0.029, agent10/loss=0.030, agent2/loss=0.030, agent4/loss=0.028, agent6/loss=0.029, agent8/loss=0.030, env_step=2880000, len=150, n/ep=60, n/st=9000, rew=1.09]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #160: test_reward: 1.100000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #161: 18000it [02:17, 131.28it/s, agent0/loss=0.027, agent10/loss=0.029, agent2/loss=0.028, agent4/loss=0.027, agent6/loss=0.029, agent8/loss=0.029, env_step=2898000, len=150, n/ep=60, n/st=9000, rew=1.02]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #161: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #162: 18000it [02:15, 132.66it/s, agent0/loss=0.026, agent10/loss=0.027, agent2/loss=0.028, agent4/loss=0.028, agent6/loss=0.028, agent8/loss=0.029, env_step=2916000, len=150, n/ep=60, n/st=9000, rew=1.15]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #162: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #163: 18000it [02:18, 130.00it/s, agent0/loss=0.027, agent10/loss=0.027, agent2/loss=0.027, agent4/loss=0.028, agent6/loss=0.028, agent8/loss=0.027, env_step=2934000, len=150, n/ep=60, n/st=9000, rew=1.16]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #163: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #164: 18000it [02:16, 131.49it/s, agent0/loss=0.026, agent10/loss=0.027, agent2/loss=0.027, agent4/loss=0.026, agent6/loss=0.028, agent8/loss=0.026, env_step=2952000, len=150, n/ep=60, n/st=9000, rew=1.05]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #164: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #165: 18000it [02:15, 132.76it/s, agent0/loss=0.026, agent10/loss=0.026, agent2/loss=0.027, agent4/loss=0.026, agent6/loss=0.028, agent8/loss=0.026, env_step=2970000, len=150, n/ep=60, n/st=9000, rew=1.10]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #165: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #166: 18000it [02:16, 132.03it/s, agent0/loss=0.024, agent10/loss=0.025, agent2/loss=0.026, agent4/loss=0.025, agent6/loss=0.027, agent8/loss=0.026, env_step=2988000, len=150, n/ep=60, n/st=9000, rew=1.13]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #166: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #167: 18000it [02:19, 129.43it/s, agent0/loss=0.025, agent10/loss=0.025, agent2/loss=0.027, agent4/loss=0.024, agent6/loss=0.027, agent8/loss=0.026, env_step=3006000, len=150, n/ep=60, n/st=9000, rew=1.15]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #167: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #168: 18000it [02:17, 131.33it/s, agent0/loss=0.025, agent10/loss=0.025, agent2/loss=0.027, agent4/loss=0.024, agent6/loss=0.026, agent8/loss=0.025, env_step=3024000, len=150, n/ep=60, n/st=9000, rew=1.12]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #168: test_reward: 1.000000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #169: 18000it [02:15, 133.27it/s, agent0/loss=0.024, agent10/loss=0.024, agent2/loss=0.026, agent4/loss=0.024, agent6/loss=0.025, agent8/loss=0.025, env_step=3042000, len=150, n/ep=60, n/st=9000, rew=1.13]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #169: test_reward: 1.000000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #170: 18000it [02:17, 131.00it/s, agent0/loss=0.024, agent10/loss=0.024, agent2/loss=0.025, agent4/loss=0.025, agent6/loss=0.024, agent8/loss=0.026, env_step=3060000, len=150, n/ep=60, n/st=9000, rew=1.10]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #170: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #171: 18000it [02:17, 130.75it/s, agent0/loss=0.022, agent10/loss=0.023, agent2/loss=0.023, agent4/loss=0.025, agent6/loss=0.023, agent8/loss=0.025, env_step=3078000, len=150, n/ep=60, n/st=9000, rew=1.11]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #171: test_reward: 0.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #172: 18000it [02:18, 129.85it/s, agent0/loss=0.023, agent10/loss=0.023, agent2/loss=0.023, agent4/loss=0.025, agent6/loss=0.024, agent8/loss=0.023, env_step=3096000, len=150, n/ep=60, n/st=9000, rew=1.11]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #172: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #173: 18000it [02:16, 131.69it/s, agent0/loss=0.022, agent10/loss=0.023, agent2/loss=0.023, agent4/loss=0.024, agent6/loss=0.024, agent8/loss=0.022, env_step=3114000, len=150, n/ep=60, n/st=9000, rew=1.15]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #173: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #174: 18000it [02:14, 133.81it/s, agent0/loss=0.022, agent10/loss=0.023, agent2/loss=0.023, agent4/loss=0.023, agent6/loss=0.024, agent8/loss=0.021, env_step=3132000, len=150, n/ep=60, n/st=9000, rew=1.08]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #174: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #175: 18000it [02:17, 131.09it/s, agent0/loss=0.021, agent10/loss=0.021, agent2/loss=0.023, agent4/loss=0.023, agent6/loss=0.024, agent8/loss=0.020, env_step=3150000, len=150, n/ep=60, n/st=9000, rew=1.12]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #175: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #176: 18000it [02:20, 128.16it/s, agent0/loss=0.021, agent10/loss=0.020, agent2/loss=0.021, agent4/loss=0.023, agent6/loss=0.023, agent8/loss=0.021, env_step=3168000, len=150, n/ep=60, n/st=9000, rew=1.06]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #176: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #177: 18000it [02:16, 131.67it/s, agent0/loss=0.021, agent10/loss=0.019, agent2/loss=0.020, agent4/loss=0.022, agent6/loss=0.022, agent8/loss=0.020, env_step=3186000, len=150, n/ep=60, n/st=9000, rew=1.14]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #177: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #178: 18000it [02:17, 130.81it/s, agent0/loss=0.020, agent10/loss=0.019, agent2/loss=0.020, agent4/loss=0.022, agent6/loss=0.021, agent8/loss=0.020, env_step=3204000, len=150, n/ep=60, n/st=9000, rew=1.14]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #178: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #179: 18000it [02:16, 131.89it/s, agent0/loss=0.020, agent10/loss=0.019, agent2/loss=0.020, agent4/loss=0.021, agent6/loss=0.020, agent8/loss=0.019, env_step=3222000, len=150, n/ep=60, n/st=9000, rew=1.14]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #179: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #180: 18000it [02:16, 131.66it/s, agent0/loss=0.019, agent10/loss=0.019, agent2/loss=0.019, agent4/loss=0.020, agent6/loss=0.020, agent8/loss=0.018, env_step=3240000, len=150, n/ep=60, n/st=9000, rew=1.09]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #180: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #181: 18000it [02:16, 132.32it/s, agent0/loss=0.020, agent10/loss=0.019, agent2/loss=0.018, agent4/loss=0.019, agent6/loss=0.019, agent8/loss=0.018, env_step=3258000, len=150, n/ep=60, n/st=9000, rew=1.18]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #181: test_reward: 1.000000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #182: 18000it [02:17, 131.23it/s, agent0/loss=0.020, agent10/loss=0.019, agent2/loss=0.017, agent4/loss=0.018, agent6/loss=0.018, agent8/loss=0.018, env_step=3276000, len=150, n/ep=60, n/st=9000, rew=1.18]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #182: test_reward: 1.000000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #183: 18000it [02:17, 130.79it/s, agent0/loss=0.018, agent10/loss=0.018, agent2/loss=0.017, agent4/loss=0.018, agent6/loss=0.017, agent8/loss=0.017, env_step=3294000, len=150, n/ep=60, n/st=9000, rew=1.16]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #183: test_reward: 1.000000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #184: 18000it [02:13, 134.39it/s, agent0/loss=0.016, agent10/loss=0.017, agent2/loss=0.016, agent4/loss=0.017, agent6/loss=0.016, agent8/loss=0.016, env_step=3312000, len=150, n/ep=60, n/st=9000, rew=1.21]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #184: test_reward: 1.000000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #185: 18000it [02:15, 133.31it/s, agent0/loss=0.016, agent10/loss=0.017, agent2/loss=0.016, agent4/loss=0.017, agent6/loss=0.017, agent8/loss=0.016, env_step=3330000, len=150, n/ep=60, n/st=9000, rew=1.17]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #185: test_reward: 1.000000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #186: 18000it [02:16, 131.69it/s, agent0/loss=0.016, agent10/loss=0.017, agent2/loss=0.016, agent4/loss=0.017, agent6/loss=0.016, agent8/loss=0.017, env_step=3348000, len=150, n/ep=60, n/st=9000, rew=1.17]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #186: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #187: 18000it [02:15, 133.01it/s, agent0/loss=0.015, agent10/loss=0.015, agent2/loss=0.015, agent4/loss=0.017, agent6/loss=0.017, agent8/loss=0.017, env_step=3366000, len=150, n/ep=60, n/st=9000, rew=1.20]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #187: test_reward: 1.000000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #188: 18000it [02:15, 132.75it/s, agent0/loss=0.013, agent10/loss=0.015, agent2/loss=0.014, agent4/loss=0.016, agent6/loss=0.016, agent8/loss=0.016, env_step=3384000, len=150, n/ep=60, n/st=9000, rew=1.19]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #188: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #189: 18000it [02:20, 128.52it/s, agent0/loss=0.012, agent10/loss=0.014, agent2/loss=0.014, agent4/loss=0.015, agent6/loss=0.017, agent8/loss=0.015, env_step=3402000, len=150, n/ep=60, n/st=9000, rew=1.19]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #189: test_reward: 1.200000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #190: 18000it [02:17, 131.37it/s, agent0/loss=0.012, agent10/loss=0.014, agent2/loss=0.013, agent4/loss=0.014, agent6/loss=0.016, agent8/loss=0.014, env_step=3420000, len=150, n/ep=60, n/st=9000, rew=1.22]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #190: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #191: 18000it [02:17, 131.34it/s, agent0/loss=0.012, agent10/loss=0.014, agent2/loss=0.013, agent4/loss=0.013, agent6/loss=0.014, agent8/loss=0.014, env_step=3438000, len=150, n/ep=60, n/st=9000, rew=1.17]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #191: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #192: 18000it [02:18, 129.97it/s, agent0/loss=0.012, agent10/loss=0.013, agent2/loss=0.012, agent4/loss=0.012, agent6/loss=0.013, agent8/loss=0.013, env_step=3456000, len=150, n/ep=60, n/st=9000, rew=1.21]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #192: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #193: 18000it [02:18, 130.15it/s, agent0/loss=0.012, agent10/loss=0.013, agent2/loss=0.012, agent4/loss=0.012, agent6/loss=0.012, agent8/loss=0.012, env_step=3474000, len=150, n/ep=60, n/st=9000, rew=1.19]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #193: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #194: 18000it [02:19, 128.61it/s, agent0/loss=0.011, agent10/loss=0.011, agent2/loss=0.010, agent4/loss=0.012, agent6/loss=0.011, agent8/loss=0.011, env_step=3492000, len=150, n/ep=60, n/st=9000, rew=1.19]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #194: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #195: 18000it [02:18, 130.17it/s, agent0/loss=0.011, agent10/loss=0.011, agent2/loss=0.010, agent4/loss=0.011, agent6/loss=0.011, agent8/loss=0.011, env_step=3510000, len=150, n/ep=60, n/st=9000, rew=1.22]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #195: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #196: 18000it [02:17, 131.08it/s, agent0/loss=0.010, agent10/loss=0.009, agent2/loss=0.009, agent4/loss=0.010, agent6/loss=0.010, agent8/loss=0.010, env_step=3528000, len=150, n/ep=60, n/st=9000, rew=1.25]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #196: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #197: 18000it [02:16, 131.93it/s, agent0/loss=0.010, agent10/loss=0.009, agent2/loss=0.009, agent4/loss=0.009, agent6/loss=0.010, agent8/loss=0.010, env_step=3546000, len=150, n/ep=60, n/st=9000, rew=1.23]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #197: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #198: 18000it [02:17, 130.55it/s, agent0/loss=0.009, agent10/loss=0.008, agent2/loss=0.008, agent4/loss=0.009, agent6/loss=0.009, agent8/loss=0.008, env_step=3564000, len=150, n/ep=60, n/st=9000, rew=1.24]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #198: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #199: 18000it [02:15, 132.81it/s, agent0/loss=0.008, agent10/loss=0.007, agent2/loss=0.007, agent4/loss=0.008, agent6/loss=0.008, agent8/loss=0.008, env_step=3582000, len=150, n/ep=60, n/st=9000, rew=1.21]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #199: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #200: 18000it [02:17, 130.55it/s, agent0/loss=0.006, agent10/loss=0.007, agent2/loss=0.006, agent4/loss=0.007, agent6/loss=0.006, agent8/loss=0.007, env_step=3600000, len=150, n/ep=60, n/st=9000, rew=1.25]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #200: test_reward: 0.300000 ± 0.000000, best_reward: 1.200000 ± 0.000000 in #138\n",
      "\n",
      "==========Result==========\n",
      "{'duration': '27949.89s', 'train_time/model': '18009.67s', 'test_step': 30150, 'test_episode': 201, 'test_time': '211.93s', 'test_speed': '142.27 step/s', 'best_reward': 1.1999999999999997, 'best_result': '1.20 ± 0.00', 'train_step': 3600000, 'train_episode': 24000, 'train_time/collector': '9728.29s', 'train_speed': '129.79 step/s'}\n",
      "\n",
      "(the trained policy can be accessed via policy.policies[agents[0]])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()    \n",
    "\n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = CustomCollector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        #VectorReplayBuffer(100_000, len(train_envs)),\n",
    "        PrioritizedVectorReplayBuffer( 100_000, len(train_envs), alpha=0.6, beta=0.4) , \n",
    "        #ListReplayBuffer(100000)       \n",
    "        exploration_noise=True             \n",
    "    )\n",
    "    test_collector = CustomCollector(policy, test_envs, exploration_noise=True)\n",
    "     \n",
    "    print(\"Buffer Warming Up \")\n",
    "    \n",
    "    for i in range(10):#int(trainer_params['batch_size'] / (300 * 10 ) )):\n",
    "        \n",
    "        train_collector.collect(n_episode=20)#,random=True) #trainer_params['batch_size'] * train_env_num))\n",
    "        #train_collector.collect(n_step=300 * 10)\n",
    "        print(\".\", end=\"\") \n",
    "    \n",
    "    print(\"\\nBuffer Lenght: \", len(train_collector.buffer)/ 150 ) \n",
    "    #train_collector.collect(n_episode=trainer_params['batch_size'])\n",
    "    #test_collector.collect(n_episode=2 )\n",
    "    #test_collector.collect(n_step=trainer_params['batch size'] * train_env_num)\n",
    "    \n",
    "    # ======== tensorboard logging setup =========\n",
    "    #         \n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"Config\", str(Run_Data))\n",
    "    writer.add_text(\"Model\", str(policy.policies['agent0'].model).replace('\\n', '  \\n'))    \n",
    "    \n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    global_step_holder = [0]  \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        print(\"Best Saved\")\n",
    "        torch.save(policy.policies[agents[0]].state_dict(), model_save_path)\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 9939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])  \n",
    "        policy.policies[agents[0]].set_eps(epsilon)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        policy.policies[agents[0]].set_eps(epsilon)\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "        #print(rews)  \n",
    "        global_step_holder[0] += 1      \n",
    "        return rews.mean()#[:,0]\n",
    "\n",
    "\n",
    "    #Define the hook function\n",
    "    def register_activation_hook(module, input, output, layer_name, writer, global_step_holder):\n",
    "        #print(f\"Hook executed for {layer_name} at step {global_step_holder[0]}\")\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # If the output is a tuple, use the first element\n",
    "        writer.add_histogram(f\"activations/{layer_name}\", output, global_step_holder[0])\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function0 = partial(register_activation_hook, layer_name=\"task_embeddings\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook0 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function0)\n",
    "\n",
    "    #Register the hook\n",
    "    # hook_function1 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    # hook1 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function1)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function2 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook2 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function2)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function3 = partial(register_activation_hook, layer_name=\"tasks_info\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook3 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function3)\n",
    "        \n",
    "    #Add Logger Details\n",
    "    def log_gradients(policy, writer, global_step_holder, **kwargs):\n",
    "        for name, param in policy.model.named_parameters():\n",
    "            writer.add_histogram(f\"{name}.grad\", param.grad, global_step_holder[0])    \n",
    "\n",
    "    #Modify the hook definition to pass the writer and global_step_holder\n",
    "    # policy.policies['agent0'].post_optim_hook = partial(log_gradients, writer=writer, global_step_holder=global_step_holder)\n",
    "        \n",
    "   \n",
    "    def condensed_make_dot(var, params=None):\n",
    "        dot = make_dot(var, params)\n",
    "        \n",
    "        # Here's where you'd condense or modify the graph.\n",
    "        # For example, to remove all nodes related to ReLU operations:\n",
    "        # (This is just a conceptual example. You'd modify this to fit your needs.)\n",
    "        nodes_to_remove = [n for n in dot.body if 'Relu' in n]\n",
    "        nodes_to_remove += [n for n in dot.body if 'Accumu' in n]       \n",
    "        for n in nodes_to_remove:\n",
    "            dot.body.remove(n)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    if False:\n",
    "\n",
    "        dummy_input = generate_dummy_observation()    \n",
    "        output = policy.policies['agent0'].model(dummy_input)     \n",
    "        \n",
    "        dot = make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "        # Save as .png\n",
    "        dot.format = 'svg'\n",
    "        dot.render(filename='model_architecture', directory=log_path, cleanup=True)\n",
    "\n",
    "    # dot = condensed_make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "    # # Save as .png\n",
    "    # dot.format = 'svg'\n",
    "    # dot.render(filename='model_architecture_condensed', directory=log_path, cleanup=True)\n",
    "\n",
    "    \n",
    "    # policy.policies[agents[0]].set_eps(0.8)\n",
    "    \n",
    "    # for i in range(int(15000)):  # total step\n",
    "        \n",
    "    #     collect_result = train_collector.collect(n_step=450)\n",
    "\n",
    "    #     # or every 1000 steps, we test it on test_collector\n",
    "    #     if collect_result['rews'].mean() >= 10 or i % 1500 == 0:\n",
    "    #         policy.policies[agents[0]].set_eps(0.0)\n",
    "            \n",
    "    #         result = test_collector.collect(n_episode=1)\n",
    "            \n",
    "    #         if result['rews'].mean() >= 10:\n",
    "    #             print(f'Finished training! Test mean returns: {result[\"rews\"].mean()}')\n",
    "    #             break\n",
    "    #         else:\n",
    "    #             # back to training eps\n",
    "    #             policy.policies[agents[0]].set_eps(0.8)\n",
    "\n",
    "    #     # train policy with a sampled batch data from buffer\n",
    "    #     losses = policy.policies[agents[0]].update(64, train_collector.buffer)\n",
    "    #     print(losses)\n",
    "\n",
    "\n",
    "    # # ======== Step 5: Run the trainer =========\n",
    "    result = offpolicy_trainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,        \n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],\n",
    "        episode_per_test= trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=trainer_params['update_per_step'],\n",
    "        logger=logger,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric,\n",
    "        show_progress = True          \n",
    "        )\n",
    "\n",
    "    writer.close()\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n/ep': 1,\n",
       " 'n/st': 150,\n",
       " 'rews': array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]),\n",
       " 'lens': array([150]),\n",
       " 'idxs': array([0]),\n",
       " 'rew': 0.10000000000000002,\n",
       " 'len': 150.0,\n",
       " 'rew_std': 1.3877787807814457e-17,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "import torch\n",
    "\n",
    "import mUAV_TA.MultiDroneEnvUtils as utils\n",
    "#from Custom_Classes import CustomCollector\n",
    "\n",
    "def _get_env_eval():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    case =  {'case' : 0, 'F1':2, 'F2': 2, \"R1\" : 12, 'R2' : 3, \"Att\" : 4, \"Rec\" : 22}\n",
    "\n",
    "    config = utils.agentEnvOptions( \n",
    "                 render_mode = 'human',                  \n",
    "                 render_speed=1,\n",
    "                 simulation_frame_rate = 0.01, \n",
    "                 action_mode=\"TaskAssign\",\n",
    "                 simulator_module = \"Internal\", \n",
    "                 max_time_steps=200, \n",
    "                 agents= {\"F1\" : 4, \"F2\" : 2, \"R1\" : 6},                 \n",
    "                 tasks= { \"Att\" : 4 , \"Rec\" : 16, \"Hold\" : 4},\n",
    "                 multiple_tasks_per_agent = False,\n",
    "                 multiple_agents_per_task = True,\n",
    "                 random_init_pos=False,\n",
    "                 num_obstacles=0,\n",
    "                 hidden_obstacles = False,\n",
    "                 fail_rate = 0.0,\n",
    "                 info = \"No Info\")\n",
    "   \n",
    "    \n",
    "    env_paralell = MultiUAVEnv()\n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "\n",
    "# Create a new instance of the policy with the same architecture as the saved policy\n",
    "name = 'policy_CustomNetMultiHead_Eval_TBTA_Relative_Representation_01.pth' \n",
    "load_policy_name = f'policy_{name}'\n",
    "\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", name)\n",
    "\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "\n",
    "# Load the saved checkpoint\n",
    "policy_test = policy.policies['agent0']\n",
    "policy_test.load_state_dict(torch.load(model_save_path ))\n",
    "\n",
    "envs = DummyVectorEnv([_get_env_eval for _ in range(1)])\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "#collector = CustomCollector(policy.policies['agent0'], envs, exploration_noise=True)\n",
    "#collector = CustomCollector(policy_test, envs, exploration_noise=False)\n",
    "collector = CustomCollector(policy, envs, exploration_noise=True)\n",
    "\n",
    "#results = collector.collect(n_episode=1)\n",
    "results = collector.collect(n_episode=1)#, gym_reset_kwargs={'seed' :2})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000000000002\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results['rews']\n",
    "print(np.mean(results['rews'][results['rews'] > -10]))\n",
    "\n",
    "\n",
    "#create a function  to print a histogram of the results['rews']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(results['rews'][results['rews'] > -10], bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultiDroneEnv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\GITHUB\\Multi-UAV-TA-gym-env\\Tianshow_Centralized_Training.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m policy_test\u001b[39m.\u001b[39mset_eps(\u001b[39m0.00\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# initialize your environment\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#env = DummyVectorEnv([_get_env for _ in range(1)])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m env \u001b[39m=\u001b[39m MultiDroneEnv(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m env\u001b[39m.\u001b[39mmax_time_steps \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GITHUB/Multi-UAV-TA-gym-env/Tianshow_Centralized_Training.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# simulate the interaction with the environment manually\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MultiDroneEnv' is not defined"
     ]
    }
   ],
   "source": [
    "from turtle import st\n",
    "import torch\n",
    "from tianshou.data import Batch\n",
    "\n",
    "# load policy as in your original code\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "policy_test = policy.policies['agent0']\n",
    "state_saved = torch.load(model_save_path )\n",
    "#print(policy_test)\n",
    "policy_test.load_state_dict(state_saved)\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "# initialize your environment\n",
    "#env = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "env = MultiDroneEnv(None)\n",
    "env.max_time_steps = 50\n",
    "\n",
    "# simulate the interaction with the environment manually\n",
    "for i in range(10):\n",
    "    for episode in range(1):  # simulate 10 episodes\n",
    "        \n",
    "        #env.render_speed = 1\n",
    "        obs, _  = env.reset(seed=episode)         \n",
    "        info         = env.get_initial_state()\n",
    "        \n",
    "        drones = info[\"drones\"]\n",
    "        tasks = info[\"tasks\"]\n",
    "            \n",
    "        done = {0 : False}\n",
    "        truncations = {0 : False}\n",
    "        \n",
    "        episodo_reward = 0\n",
    "        #obs, reward, done, truncations, info = env.step(action)\n",
    "\n",
    "        while not all(done.values()) and not all(truncations.values()):\n",
    "            \n",
    "            agent_id = \"agent\" + str(env.agent_selector._current_agent)\n",
    "            # Create a Batch of observations\n",
    "            obs_batch = Batch(obs=[obs[agent_id]], info=[{}])  # add empty info for each observation\n",
    "            \n",
    "            #print(obs_batch)\n",
    "            # Forward the batch of observations through the policy to get the actions\n",
    "            action = policy_test(obs_batch).act\n",
    "            action = {agent_id : action[0]}\n",
    "        \n",
    "            obs, reward, done, truncations, info = env.step(action)\n",
    "            \n",
    "            episodo_reward += sum(reward.values())/env.n_agents\n",
    "\n",
    "        \n",
    "\n",
    "    print(episodo_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
