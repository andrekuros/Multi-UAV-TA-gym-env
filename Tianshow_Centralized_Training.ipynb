{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomNetMultiHead_TBTA_NOV06_Emb128  \n",
      "Loaded_Model: no  \n",
      "log_path: ./Logs\\dqn\\CustomNetMultiHead_TBTA_NOV06_Emb128231108-171143  \n",
      "train/test_env_num: 10 / 20  \n",
      "model: CustomNetMultiHead  \n",
      "dqn_params: {'discount_factor': 0.98, 'estimation_step': 150, 'target_update_freq': 4500, 'optminizer': 'Adam', 'lr': 5e-05}  \n",
      "trainer_params: {'max_epoch': 500, 'step_per_epoch': 7500, 'step_per_collect': 1500, 'episode_per_test': 20, 'batch_size': 1500, 'update_per_step': 0.0033333333333333335, 'tn_eps_max': 0.8, 'ts_eps_max': 0.0} \n",
      "single_policy: True\n",
      "\n",
      "--------- Env ------------  \n",
      "\n",
      "Rewards Only Final Quality and SQuality\n",
      "F_Rew / 20 > lre\n",
      "random_init_pos      : False\n",
      "max_time_steps       : 150\n",
      "simulation_frame_rate: 0.01\n",
      "Agents               : {'F1': 0, 'F2': 1, 'R1': 0, 'R2': 0}\n",
      "tasks                : {'Att': 10, 'Rec': 0, 'Hold': 0}\n",
      "random_init_pos      : False \n",
      "threats              : []\n",
      "seed                 : -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "from functools import partial\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy, RainbowPolicy\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "\n",
    "#from torchviz import make_dot\n",
    "\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomCollector\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "#from CustomClass_multi_head import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes_simplified import CustomNetSimple\n",
    "#from Custom_Classes_simplified import CustomCollectorSimple\n",
    "#from Custom_Classes_simplified import CustomParallelToAECWrapperSimple\n",
    "\n",
    "from TaskAllocation.RL_Policies.CustomClasses_Transformer_Reduced import CustomNetReduced\n",
    "from TaskAllocation.RL_Policies.CustomClass_MultiHead_Transformer import CustomNetMultiHead\n",
    "\n",
    "from mUAV_TA.MultiDroneEnvUtils import agentEnvOptions\n",
    "\n",
    "from mUAV_TA.DroneEnv import MultiUAVEnv\n",
    "#from tianshou_DQN import train\n",
    "model = \"CustomNetMultiHead\" # \"CustomNet\" or \"CustomNetSimple\" or \"CustomNetReduced\" or \"CustomNetMultiHead\"\n",
    "test_num = \"_TBTA_NOV06_Emb128\"\n",
    "policyModel = \"DQN\"\n",
    "\n",
    "train_env_num = 10\n",
    "test_env_num = 20\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "load_policy_name = f'policy_CustomNetMultiHead_TBTA_NOV06_Emb128_rew2k.pth'\n",
    "save_policy_name = f'policy_{name}'\n",
    "policy_path = \"dqn_Custom\"\n",
    "\n",
    "same_policy = True\n",
    "\n",
    "load_model = False\n",
    "\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", log_name)\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.98, \n",
    "              \"estimation_step\": 150, \n",
    "              \"target_update_freq\": 150 * 30,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 5e-5 }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 500,\n",
    "                  \"step_per_epoch\": 150 * 50,\n",
    "                  \"step_per_collect\": 150 * 10,\n",
    "                  \"episode_per_test\": 20,\n",
    "                  \"batch_size\" : 1500,\n",
    "                  \"update_per_step\": 1 / 300, #Only run after close a Collect (run many times as necessary to meet the value)\n",
    "                  \"tn_eps_max\": 0.80,\n",
    "                  \"ts_eps_max\": 0.0,\n",
    "                  }\n",
    "\n",
    "config_default = agentEnvOptions(                                        \n",
    "                 render_speed=-1,\n",
    "                 simulation_frame_rate = 0.01, \n",
    "                 action_mode=\"TaskAssign\",\n",
    "                 simulator_module = \"Internal\", \n",
    "                 max_time_steps=150, \n",
    "                 agents= {\"F1\" : 0, \"F2\" : 1, \"R1\" : 0, \"R2\" : 0},                 \n",
    "                 tasks= { \"Att\" : 10 , \"Rec\" : 0, \"Hold\" : 0},\n",
    "                 multiple_tasks_per_agent = False,\n",
    "                 multiple_agents_per_task = True,\n",
    "                 random_init_pos=False,\n",
    "                 num_obstacles=0,\n",
    "                 hidden_obstacles = False,\n",
    "                 fail_rate = 0.0,\n",
    "                 threats_list = [],#[(\"T1\", 4), (\"T2\" , 2)],\n",
    "                 fixed_seed = -1,\n",
    "                 info = \"No Info\")    \n",
    "\n",
    "Run_Data = f'''{name}  \n",
    "Loaded_Model: {load_policy_name if load_model else \"no\"}  \n",
    "log_path: {log_path}  \n",
    "train/test_env_num: {train_env_num} / {test_env_num}  \n",
    "model: {model}  \n",
    "dqn_params: {dqn_params}  \n",
    "trainer_params: {trainer_params} \n",
    "single_policy: {same_policy}\n",
    "\n",
    "--------- Env ------------  \n",
    "\n",
    "Rewards Only Final Quality and SQuality\n",
    "F_Rew / 20 > lre\n",
    "random_init_pos      : {config_default.random_init_pos}\n",
    "max_time_steps       : {config_default.max_time_steps}\n",
    "simulation_frame_rate: {config_default.simulation_frame_rate}\n",
    "Agents               : {config_default.agents}\n",
    "tasks                : {config_default.tasks}\n",
    "random_init_pos      : {config_default.random_init_pos} \n",
    "threats              : {config_default.threats_list}\n",
    "seed                 : {config_default.fixed_seed}\n",
    "'''\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "def generate_dummy_observation(batch_size=1, sequence_length=31, feature_dim=12):\n",
    "    # Generate a random tensor with the given shape\n",
    "    dummy_obs = torch.randn(batch_size, sequence_length, feature_dim)\n",
    "\n",
    "    return dummy_obs\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()\n",
    "    agent_name = env.agents[0]  # Get the name of the first agent\n",
    "    \n",
    "    #print(env.observation_space )\n",
    "    agent_observation_space = env.observation_space # assuming 'agent0' is a valid agent name\n",
    "    state_shape_agent_position = agent_observation_space[\"agent_position\"].shape[0]\n",
    "    state_shape_agent_state = agent_observation_space[\"agent_state\"].shape[0]\n",
    "    state_shape_agent_type = agent_observation_space[\"agent_type\"].shape[0]\n",
    "    state_shape_next_free_time = agent_observation_space[\"next_free_time\"].shape[0]\n",
    "    state_shape_position_after_last_task = agent_observation_space[\"position_after_last_task\"].shape[0]       \n",
    "    #state_shape_agent_relay_area = agent_observation_space[\"agent_relay_area\"].shape[0]\n",
    "        \n",
    "    state_shape_agent = (state_shape_agent_position + state_shape_agent_state +\n",
    "                     state_shape_agent_type+ state_shape_next_free_time + state_shape_position_after_last_task #+                     \n",
    "                     #state_shape_agent_relay_area\n",
    "                     )                 \n",
    "\n",
    "    state_shape_task = 31 * 13 #env.observation_space[\"tasks_info\"].shape[0]\n",
    "                  \n",
    "    action_shape = env.action_space[agent_name].shape[0]\n",
    "    #action_shape = env.action_space[agent_name].n\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"          \n",
    "    \n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        if model == \"CustomNet\":        \n",
    "            net = CustomNet(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=device,\n",
    "            ).to(device)\n",
    "        \n",
    "        if model == \"CustomNetSimple\":\n",
    "            net = CustomNetSimple(            \n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "        \n",
    "        if model == \"CustomNetReduced\":\n",
    "            net = CustomNetReduced(            \n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "        \n",
    "        if model == \"CustomNetMultiHead\":\n",
    "            net = CustomNetMultiHead(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "\n",
    "            net2 = CustomNetMultiHead(\n",
    "                state_shape_agent=state_shape_agent,\n",
    "                state_shape_task=state_shape_task,\n",
    "                action_shape=action_shape,\n",
    "                hidden_sizes=[128,128],\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            ).to(device)\n",
    "\n",
    "    \n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "        \n",
    "        optim2 = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )\n",
    "    \n",
    "        if policyModel == \"DQN\":\n",
    "            agent_learn = DQNPolicy(\n",
    "                model=net,\n",
    "                optim=optim,\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "                reward_normalization = False \n",
    "            ) \n",
    "            \n",
    "            agent_learn2 = DQNPolicy(\n",
    "                model=net2,\n",
    "                optim=optim2,\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "            ) \n",
    "\n",
    "        if policyModel == \"Rainbow\":\n",
    "            agent_learn = RainbowPolicy(\n",
    "                model=net.to(device),\n",
    "                optim=optim,\n",
    "                num_atoms= 31,\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "            ) \n",
    "         \n",
    " \n",
    "        if load_model == True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "                   \n",
    "        #print(env.agents)\n",
    "        #agents = [agent_learn for _ in range(len(env.agents))]\n",
    "        \n",
    "        agents = [None for _ in range(len(env.agents))]        \n",
    "        \n",
    "        if not same_policy:\n",
    "\n",
    "            for i,agent in enumerate(env.agents):             \n",
    "                if agent[0] == \"F\":                \n",
    "                    agents[i] = agent_learn2\n",
    "                    #print(\"F\")\n",
    "                else:\n",
    "                    agents[i] = agent_learn\n",
    "                    #print(\"R\")\n",
    "        else:\n",
    "            agents = [agent_learn for _ in range(len(env.agents))]\n",
    "\n",
    "        # print(agents)\n",
    "        # print([o.type for o in agents_obj])\n",
    "\n",
    "\n",
    "        # agent_learn2\n",
    "        \n",
    "    policy = MultiAgentPolicyManager(agents, env)  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    env_paralell = MultiUAVEnv(config=config_default)    \n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "print(Run_Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer Warming Up \n",
      "..........\n",
      "Buffer Lenght:  100.0\n",
      "Best Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 7501it [01:38, 75.96it/s, F2_agent0/loss=37317.528, env_step=7500, len=150, n/ep=10, n/st=1500, rew=475.10]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #1: test_reward: 236.555568 ± 358.367328, best_reward: 236.555568 ± 358.367328 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 7501it [01:42, 73.19it/s, F2_agent0/loss=43241.855, env_step=15000, len=150, n/ep=10, n/st=1500, rew=721.97]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #2: test_reward: 306.295364 ± 324.438667, best_reward: 306.295364 ± 324.438667 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 7501it [01:49, 68.38it/s, F2_agent0/loss=49275.385, env_step=22500, len=150, n/ep=10, n/st=1500, rew=629.67]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #3: test_reward: 814.425608 ± 551.271626, best_reward: 814.425608 ± 551.271626 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 7501it [01:55, 65.19it/s, F2_agent0/loss=54264.221, env_step=30000, len=150, n/ep=10, n/st=1500, rew=722.94]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved\n",
      "Epoch #4: test_reward: 1197.753282 ± 262.788948, best_reward: 1197.753282 ± 262.788948 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 7501it [01:50, 68.17it/s, F2_agent0/loss=63276.336, env_step=37500, len=150, n/ep=10, n/st=1500, rew=770.79]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 1121.004961 ± 422.622615, best_reward: 1197.753282 ± 262.788948 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6:  40%|####      | 3000/7500 [00:24<00:41, 108.10it/s, F2_agent0/loss=64745.086, env_step=39000, len=150, n/ep=10, n/st=1500, rew=908.97]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()    \n",
    "\n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = CustomCollector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        #VectorReplayBuffer(100_000, len(train_envs)),\n",
    "        PrioritizedVectorReplayBuffer( 300_000, len(train_envs), alpha=0.6, beta=0.4) , \n",
    "        #ListReplayBuffer(100000)       \n",
    "        exploration_noise=True             \n",
    "    )\n",
    "    test_collector = CustomCollector(policy, test_envs, exploration_noise=False)\n",
    "     \n",
    "    print(\"Buffer Warming Up \")\n",
    "    \n",
    "    for i in range(10):#int(trainer_params['batch_size'] / (300 * 10 ) )):\n",
    "        \n",
    "        train_collector.collect(n_episode=train_env_num)#,random=True) #trainer_params['batch_size'] * train_env_num))\n",
    "        #train_collector.collect(n_step=300 * 10)\n",
    "        print(\".\", end=\"\") \n",
    "    \n",
    "    print(\"\\nBuffer Lenght: \", len(train_collector.buffer)/ 150 ) \n",
    "    #train_collector.collect(n_episode=trainer_params['batch_size'])\n",
    "    #test_collector.collect(n_episode=2 )\n",
    "    #test_collector.collect(n_step=trainer_params['batch size'] * train_env_num)\n",
    "    \n",
    "    # ======== tensorboard logging setup =========\n",
    "    #         \n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"Config\", str(Run_Data))\n",
    "    if same_policy:\n",
    "        writer.add_text(\"Model\", str(policy.policies[agents[0]].model).replace('\\n', '  \\n'))    \n",
    "    else:\n",
    "         writer.add_text(\"ModelR\", str(policy.policies['R_agent0'].model).replace('\\n', '  \\n'))\n",
    "         writer.add_text(\"ModelF\", str(policy.policies['F_agent0'].model).replace('\\n', '  \\n'))\n",
    "    \n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    global_step_holder = [0]  \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        if same_policy:             \n",
    "            torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \".pth\")\n",
    "            print(\"Best Saved\")\n",
    "        else:\n",
    "            torch.save(policy.policies['R_agent0'].state_dict(), model_save_path + \"R.pth\")\n",
    "            torch.save(policy.policies['F_agent0'].state_dict(), model_save_path + \"F.pth\")\n",
    "            print(\"Bests Saved\")\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 9939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])  \n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "        #print(rews)  \n",
    "        global_step_holder[0] += 1    \n",
    "\n",
    "        #if rews[:,0].mean() != 0:\n",
    "        #    print( rews)\n",
    "        return rews[:,0]\n",
    "\n",
    "\n",
    "    #Define the hook function\n",
    "    def register_activation_hook(module, input, output, layer_name, writer, global_step_holder):\n",
    "        #print(f\"Hook executed for {layer_name} at step {global_step_holder[0]}\")\n",
    "        if isinstance(output, tuple):\n",
    "            output = output[0]  # If the output is a tuple, use the first element\n",
    "        writer.add_histogram(f\"activations/{layer_name}\", output, global_step_holder[0])\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function0 = partial(register_activation_hook, layer_name=\"task_embeddings\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook0 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function0)\n",
    "\n",
    "    #Register the hook\n",
    "    # hook_function1 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    # hook1 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function1)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function2 = partial(register_activation_hook, layer_name=\"attention_output2\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook2 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function2)\n",
    "\n",
    "    #Register the hook\n",
    "    #hook_function3 = partial(register_activation_hook, layer_name=\"tasks_info\", writer=writer, global_step_holder=global_step_holder)    \n",
    "    #hook3 = policy.policies['agent0'].model.task_encoder.register_forward_hook(hook_function3)\n",
    "        \n",
    "    #Add Logger Details\n",
    "    def log_gradients(policy, writer, global_step_holder, **kwargs):\n",
    "        for name, param in policy.model.named_parameters():\n",
    "            writer.add_histogram(f\"{name}.grad\", param.grad, global_step_holder[0])    \n",
    "\n",
    "    #Modify the hook definition to pass the writer and global_step_holder\n",
    "    # policy.policies['agent0'].post_optim_hook = partial(log_gradients, writer=writer, global_step_holder=global_step_holder)\n",
    "        \n",
    "   \n",
    "    def condensed_make_dot(var, params=None):\n",
    "        dot = make_dot(var, params)\n",
    "        \n",
    "        # Here's where you'd condense or modify the graph.\n",
    "        # For example, to remove all nodes related to ReLU operations:\n",
    "        # (This is just a conceptual example. You'd modify this to fit your needs.)\n",
    "        nodes_to_remove = [n for n in dot.body if 'Relu' in n]\n",
    "        nodes_to_remove += [n for n in dot.body if 'Accumu' in n]       \n",
    "        for n in nodes_to_remove:\n",
    "            dot.body.remove(n)\n",
    "\n",
    "        return dot\n",
    "\n",
    "    if False:\n",
    "\n",
    "        dummy_input = generate_dummy_observation()    \n",
    "        output = policy.policies['agent0'].model(dummy_input)     \n",
    "        \n",
    "        dot = make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "        # Save as .png\n",
    "        dot.format = 'svg'\n",
    "        dot.render(filename='model_architecture', directory=log_path, cleanup=True)\n",
    "\n",
    "    # dot = condensed_make_dot(output[0], params=dict(policy.policies['agent0'].model.named_parameters()))\n",
    "    # # Save as .png\n",
    "    # dot.format = 'svg'\n",
    "    # dot.render(filename='model_architecture_condensed', directory=log_path, cleanup=True)\n",
    "\n",
    "    \n",
    "    # policy.policies[agents[0]].set_eps(0.8)\n",
    "    \n",
    "    # for i in range(int(15000)):  # total step\n",
    "        \n",
    "    #     collect_result = train_collector.collect(n_step=450)\n",
    "\n",
    "    #     # or every 1000 steps, we test it on test_collector\n",
    "    #     if collect_result['rews'].mean() >= 10 or i % 1500 == 0:\n",
    "    #         policy.policies[agents[0]].set_eps(0.0)\n",
    "            \n",
    "    #         result = test_collector.collect(n_episode=1)\n",
    "            \n",
    "    #         if result['rews'].mean() >= 10:\n",
    "    #             print(f'Finished training! Test mean returns: {result[\"rews\"].mean()}')\n",
    "    #             break\n",
    "    #         else:\n",
    "    #             # back to training eps\n",
    "    #             policy.policies[agents[0]].set_eps(0.8)\n",
    "\n",
    "    #     # train policy with a sampled batch data from buffer\n",
    "    #     losses = policy.policies[agents[0]].update(64, train_collector.buffer)\n",
    "    #     print(losses)\n",
    "\n",
    "\n",
    "    # # ======== Step 5: Run the trainer =========\n",
    "    result = offpolicy_trainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,        \n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],\n",
    "        episode_per_test= trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=trainer_params['update_per_step'],\n",
    "        logger=logger,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric,\n",
    "        show_progress = True \n",
    "               \n",
    "        )\n",
    "\n",
    "    writer.close()\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import TensorboardLogger\n",
    "import torch\n",
    "\n",
    "import mUAV_TA.MultiDroneEnvUtils as utils\n",
    "#from Custom_Classes import CustomCollector\n",
    "\n",
    "def _get_env_eval():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"\n",
    "    case =  {'case' : 0, 'F1':0, 'F2': 0, \"R1\" : 6, 'R2' : 0, \"Att\" : 0, \"Rec\" : 16, \"Hold\" : 4}\n",
    "\n",
    "    config_default = agentEnvOptions(                                        \n",
    "                 render_speed=-1,\n",
    "                 simulation_frame_rate = 0.01, \n",
    "                 action_mode=\"TaskAssign\",\n",
    "                 simulator_module = \"Internal\", \n",
    "                 max_time_steps=150, \n",
    "                 agents= {\"F1\" : 4, \"F2\" : 4, \"R1\" : 4},                 \n",
    "                 tasks= { \"Att\" : 0 , \"Rec\" : 16, \"Hold\" : 4},\n",
    "                 multiple_tasks_per_agent = False,\n",
    "                 multiple_agents_per_task = True,\n",
    "                 random_init_pos=False,\n",
    "                 num_obstacles=0,\n",
    "                 hidden_obstacles = False,\n",
    "                 fail_rate = 0.0,\n",
    "                 threats_list = [(\"T1\", 4), (\"T2\" , 2)],\n",
    "                 fixed_seed = -1,\n",
    "                 info = \"No Info\")  \n",
    "   \n",
    "    \n",
    "    env_paralell = MultiUAVEnv(config = config_default)\n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "\n",
    "\n",
    "# Create a new instance of the policy with the same architecture as the saved policy\n",
    "name = 'CustomNetMultiHead_Eval_TBTA_OCT01.pth' \n",
    "load_policy_name = f'policy_{name}'\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn\", name)\n",
    "\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "\n",
    "# Load the saved checkpoint\n",
    "policy_test = policy.policies[\"R1_agent0\"]\n",
    "policy_test.load_state_dict(torch.load(model_save_path + \".pth\" ))\n",
    "\n",
    "envs = DummyVectorEnv([_get_env_eval for _ in range(1)])\n",
    "#policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "#collector = CustomCollector(policy.policies['agent0'], envs, exploration_noise=True)\n",
    "#collector = CustomCollector(policy_test, envs, exploration_noise=False)\n",
    "collector = CustomCollector(policy, envs, exploration_noise=True)\n",
    "\n",
    "#results = collector.collect(n_episode=1)\n",
    "results = collector.collect(n_episode=10)#, gym_reset_kwargs={'seed' :2})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['rews']\n",
    "print(np.mean(results['rews'][results['rews'] > -10]))\n",
    "\n",
    "\n",
    "#create a function  to print a histogram of the results['rews']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(results['rews'][results['rews'] > -10], bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import st\n",
    "import torch\n",
    "from tianshou.data import Batch\n",
    "\n",
    "# load policy as in your original code\n",
    "policy, optim, _ = _get_agents()\n",
    "model_save_path = os.path.join(\"dqn_Custom\", save_policy_name)        \n",
    "policy_test = policy.policies['agent0']\n",
    "state_saved = torch.load(model_save_path )\n",
    "#print(policy_test)\n",
    "policy_test.load_state_dict(state_saved)\n",
    "policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "# initialize your environment\n",
    "#env = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "env = MultiDroneEnv(None)\n",
    "env.max_time_steps = 50\n",
    "\n",
    "# simulate the interaction with the environment manually\n",
    "for i in range(10):\n",
    "    for episode in range(1):  # simulate 10 episodes\n",
    "        \n",
    "        #env.render_speed = 1\n",
    "        obs, _  = env.reset(seed=episode)         \n",
    "        info         = env.get_initial_state()\n",
    "        \n",
    "        drones = info[\"drones\"]\n",
    "        tasks = info[\"tasks\"]\n",
    "            \n",
    "        done = {0 : False}\n",
    "        truncations = {0 : False}\n",
    "        \n",
    "        episodo_reward = 0\n",
    "        #obs, reward, done, truncations, info = env.step(action)\n",
    "\n",
    "        while not all(done.values()) and not all(truncations.values()):\n",
    "            \n",
    "            agent_id = \"agent\" + str(env.agent_selector._current_agent)\n",
    "            # Create a Batch of observations\n",
    "            obs_batch = Batch(obs=[obs[agent_id]], info=[{}])  # add empty info for each observation\n",
    "            \n",
    "            #print(obs_batch)\n",
    "            # Forward the batch of observations through the policy to get the actions\n",
    "            action = policy_test(obs_batch).act\n",
    "            action = {agent_id : action[0]}\n",
    "        \n",
    "            obs, reward, done, truncations, info = env.step(action)\n",
    "            \n",
    "            episodo_reward += sum(reward.values())/env.n_agents\n",
    "\n",
    "        \n",
    "\n",
    "    print(episodo_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
