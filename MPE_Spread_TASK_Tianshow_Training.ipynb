{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import utils\n",
    "import os\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "import json\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Tianshow_Centralized_Training\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.env.pettingzoo_env_parallel import PettingZooParallelEnv\n",
    "\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy, RainbowPolicy\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from pettingzoo.sisl import pursuit_v4\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "#import Mods.TaskSpreadEnv as TaskSpreadEnv\n",
    "\n",
    "from TaskAllocation.RL_Policies.DNN_Spread import DNN_Spread\n",
    "from TaskAllocation.RL_Policies.MPE_Task_MultiHead import MPE_Task_MultiHead\n",
    "\n",
    "#import Mods.TaskPursuitEnv as TaskPursuitEnv\n",
    "import Mods.ActionLoggerWrapper as ActionLoggerWrapper\n",
    "import Mods.VDNPolicy as VDNPolicy\n",
    "import Mods.PettingZooParallelEnv2 as PettingZooParallelEnv2\n",
    "import Mods.CollectorMA as CollectorMA\n",
    "\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomCollector\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "# Add specific modification to tianshou\n",
    "import wandb\n",
    "from tianshou.utils import WandbLogger\n",
    "from tianshou.utils.logger.base import LOG_DATA_TYPE\n",
    "\n",
    "def new_write(self, step_type: str, step: int, data: LOG_DATA_TYPE) -> None:\n",
    "    data[step_type] = step\n",
    "    wandb.log(data)\n",
    "    \n",
    "WandbLogger.write = new_write \n",
    "\n",
    "from pettingzoo.utils import wrappers\n",
    "import gym\n",
    "\n",
    "class ActionLoggerWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ActionLoggerWrapper, self).__init__(env)\n",
    "        self.actions = []\n",
    "\n",
    "    def step(self, action):\n",
    "        self.actions.append(action)\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self, **kwargs):      \n",
    "        if self.actions:\n",
    "            # Convert all actions to numpy arrays and standardize their shapes\n",
    "            formatted_actions = [np.array(a).flatten() for a in self.actions]\n",
    "            flattened_actions = np.concatenate(formatted_actions)\n",
    "\n",
    "            try:\n",
    "                # Compute the histogram\n",
    "                hist_data, bin_edges = np.histogram(flattened_actions, bins='auto')\n",
    "\n",
    "                # Log the actions as a histogram to wandb\n",
    "                wandb.log({\"actions_histogram\": wandb.Histogram(np_histogram=(hist_data, bin_edges))})\n",
    "            except Exception as e:\n",
    "                pass#print(\"Error in logging histogram:\", e)\n",
    "\n",
    "            self.actions = []\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "#from tianshou_DQN import train\n",
    "model  =  \"DNN_Spread\" #\"DNN_Spread\"#\"MPE_Task_MultiHead\" # #\"CNN_ATT_SISL\" #\"MultiHead_SISL\" \n",
    "test_num  =  \"_Desk_01_8feat\"\n",
    "policyModel  =  \"DQN\"\n",
    "\n",
    "train_env_num = 10\n",
    "test_env_num = 10\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn_sisl\", log_name)\n",
    "\n",
    "#policy\n",
    "load_policy_name = f'policy_MPE_Task_MultiHead_Desk_01_8feat240112-145703_29_BestRew.pth'\n",
    "save_policy_name = f'policy_{log_name}'\n",
    "policy_path = \"vdn_Spread\"\n",
    "\n",
    "Policy_Config = {\n",
    "    \"same_policy\" : True,\n",
    "    \"load_model\" : False,\n",
    "    \"freeze_CNN\" : False     \n",
    "                }\n",
    "\n",
    "Spread_Config = {\n",
    "    \"N\": 3,                      # Default = 3\n",
    "    \"local_ratio\": 0.5,          # Default = 0.5\n",
    "    \"max_cycles\": 25,            # Default = 25\n",
    "    \"continuous_actions\": False, # Default = False\n",
    "    \"render_mode\": None          # Default = None \n",
    "}\n",
    "\n",
    "max_cycles = Spread_Config[\"max_cycles\"]\n",
    "n_agents = Spread_Config[\"N\"]\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.98, \n",
    "              \"estimation_step\": 5, \n",
    "              \"target_update_freq\": 1000,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 0.0001 }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 500,\n",
    "                  \"step_per_epoch\": 2000 * max_cycles,\n",
    "                  \"step_per_collect\": 250 * max_cycles, #6250\n",
    "                  \"episode_per_test\": 50,\n",
    "                  \"batch_size\" :  256,\n",
    "                  \"update_per_step\": 1 / 30, #Only run after close a Collect (run many times as necessary to meet the value)\n",
    "                  \"tn_eps_max\": 0.30,\n",
    "                  \"ts_eps_max\": 0.01,\n",
    "                  \"warmup_size\" : 1\n",
    "                  }\n",
    "\n",
    "\n",
    "runConfig = dqn_params\n",
    "runConfig.update(Policy_Config)\n",
    "runConfig.update(trainer_params) \n",
    "runConfig.update(Spread_Config)\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()       \n",
    "    agent_observation_space = env.observation_space.shape\n",
    "   \n",
    "    action_shape = env.action_space\n",
    "    \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  \n",
    "\n",
    "    agents = []        \n",
    "    \n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        policies_number = 1\n",
    "    else:\n",
    "        policies_number = 3#len(env.agents)\n",
    "\n",
    "    for _ in range(policies_number):                   \n",
    "\n",
    "        if model == \"DNN_Spread\":\n",
    "            net = DNN_Spread(\n",
    "                obs_shape=agent_observation_space[0],                \n",
    "                action_shape=5,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                \n",
    "            ).to(device)\n",
    "\n",
    "        if model == \"VDN_Spread\":\n",
    "            net = DNN_Spread(\n",
    "                obs_shape=agent_observation_space[0],                \n",
    "                action_shape=5,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                \n",
    "            ).to(device)\n",
    "\n",
    "        if model == \"MPE_Task_MultiHead\":\n",
    "            net = MPE_Task_MultiHead(                \n",
    "                num_tasks=Spread_Config['N'] * 2 + 5,\n",
    "                num_features_per_task = 2,#6 + 2 + 1,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                \n",
    "            ).to(device)\n",
    "\n",
    "        optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0 , amsgrad= True, eps=1e-06 )                \n",
    "\n",
    "        if policyModel == \"DQN\":\n",
    "            agent_learn = DQNPolicy(\n",
    "                model=net,\n",
    "                optim=optim,\n",
    "                action_space = action_shape,\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "                reward_normalization = False,\n",
    "                clip_loss_grad = False \n",
    "            ) \n",
    "        \n",
    "        if policyModel == \"VDN\":\n",
    "            agent_learn = DQNPolicy(\n",
    "                model=net,\n",
    "                optim=optim,\n",
    "                action_space = action_shape,\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "                reward_normalization = False,\n",
    "                clip_loss_grad = False,                \n",
    "            ) \n",
    "\n",
    "        if Policy_Config[\"load_model\"] is True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "                   \n",
    "        #print(env.agents)\n",
    "        #agents = [agent_learn for _ in range(len(env.agents))]\n",
    "        \n",
    "        agents.append(agent_learn)\n",
    "\n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        agents = [agents[0] for _ in range(len(env.agents))]\n",
    "    else:\n",
    "        for _ in range(len(env.agents) - policies_number):\n",
    "            agents.append(agents[0])\n",
    "\n",
    "    if policyModel == \"DQN\":\n",
    "        policy = MultiAgentPolicyManager(policies = agents, env=env)  \n",
    "\n",
    "    if policyModel == \"VDN\":\n",
    "        policy = VDNPolicy.VDNMAPolicy(policies = agents, env=env, device=\"cuda\" if torch.cuda.is_available() else \"cpu\" )  \n",
    "\n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "def _get_env(test=False):\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    # env_paralell = MultiUAVEnv()  \n",
    "    #env = pursuit_v4.env()    \n",
    "    #env = TaskSpreadEnv.env(\n",
    "    # env = simple_spread_v3.parallel_env(\n",
    "    env = simple_spread_v3.env(\n",
    "        max_cycles=Spread_Config[\"max_cycles\"],\n",
    "        local_ratio=Spread_Config[\"local_ratio\"],\n",
    "        N=Spread_Config[\"N\"],\n",
    "        continuous_actions=Spread_Config[\"continuous_actions\"],\n",
    "        render_mode=\" human\" #Spread_Config[\"render_mode\"]\n",
    "    )    \n",
    "    \n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    # env = CustomParallelToAECWrapper(env_paralell)\n",
    "    env = ActionLoggerWrapper(env)\n",
    "    env = PettingZooEnv(env) \n",
    "    # env = PettingZooParallelEnv(env)\n",
    "       \n",
    "    return  env\n",
    "\n",
    "# print(json.dumps(runConfig, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python310\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "d:\\Python310\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer Warming Up \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find Tianshow_Centralized_Training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrekuros\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "d:\\Python310\\lib\\site-packages\\wandb\\sdk\\lib\\ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\GITHUB\\SiSL Eval\\Multi-UAV-TA-gym-env\\wandb\\run-20240121_163028-DNN_Spread_Desk_01_8feat240121-163013</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrekuros/Spread_Eval01/runs/DNN_Spread_Desk_01_8feat240121-163013' target=\"_blank\">DNN_Spread_Desk_01_8feat240121-163013</a></strong> to <a href='https://wandb.ai/andrekuros/Spread_Eval01' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrekuros/Spread_Eval01' target=\"_blank\">https://wandb.ai/andrekuros/Spread_Eval01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrekuros/Spread_Eval01/runs/DNN_Spread_Desk_01_8feat240121-163013' target=\"_blank\">https://wandb.ai/andrekuros/Spread_Eval01/runs/DNN_Spread_Desk_01_8feat240121-163013</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  0\n",
      "Bests Saved Rew 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 25001it [00:40, 620.97it/s, agent_0/loss=0.180, agent_1/loss=0.787, agent_2/loss=0.848, env_step=25000, len=75, n/ep=80, n/st=6250, rew=-35.01]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bests Saved Rew 6\n",
      "Epoch #1: test_reward: -28.572341 ± 8.322666, best_reward: -28.572341 ± 8.322666 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 25001it [00:38, 648.82it/s, agent_0/loss=0.168, agent_1/loss=0.830, agent_2/loss=0.803, env_step=50000, len=75, n/ep=80, n/st=6250, rew=-28.69]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  10\n",
      "Epoch #2: test_reward: -29.140289 ± 9.330465, best_reward: -28.572341 ± 8.322666 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 25001it [00:37, 664.50it/s, agent_0/loss=0.154, agent_1/loss=0.707, agent_2/loss=0.704, env_step=75000, len=75, n/ep=90, n/st=6250, rew=-27.78]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bests Saved Rew 16\n",
      "Epoch #3: test_reward: -24.448605 ± 6.144603, best_reward: -24.448605 ± 6.144603 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 25001it [00:38, 652.36it/s, agent_0/loss=0.141, agent_1/loss=0.697, agent_2/loss=0.739, env_step=100000, len=75, n/ep=80, n/st=6250, rew=-25.50]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  20\n",
      "Epoch #4: test_reward: -26.471742 ± 6.515872, best_reward: -24.448605 ± 6.144603 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 25001it [00:38, 657.51it/s, agent_0/loss=0.140, agent_1/loss=0.698, agent_2/loss=0.733, env_step=125000, len=75, n/ep=80, n/st=6250, rew=-25.26]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bests Saved Rew 26\n",
      "Epoch #5: test_reward: -23.074004 ± 5.092964, best_reward: -23.074004 ± 5.092964 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 25001it [00:37, 660.20it/s, agent_0/loss=0.118, agent_1/loss=0.589, agent_2/loss=0.655, env_step=150000, len=75, n/ep=90, n/st=6250, rew=-24.29]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  30\n",
      "Epoch #6: test_reward: -27.943002 ± 7.241477, best_reward: -23.074004 ± 5.092964 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 25001it [00:37, 661.16it/s, agent_0/loss=0.119, agent_1/loss=0.639, agent_2/loss=0.668, env_step=175000, len=75, n/ep=80, n/st=6250, rew=-23.57]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: -24.236488 ± 5.728032, best_reward: -23.074004 ± 5.092964 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 25001it [00:38, 652.15it/s, agent_0/loss=0.121, agent_1/loss=0.667, agent_2/loss=0.683, env_step=200000, len=75, n/ep=80, n/st=6250, rew=-23.41]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  40\n",
      "Epoch #8: test_reward: -24.687556 ± 6.703183, best_reward: -23.074004 ± 5.092964 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 25001it [00:37, 670.20it/s, agent_0/loss=0.122, agent_1/loss=0.743, agent_2/loss=0.727, env_step=225000, len=75, n/ep=90, n/st=6250, rew=-23.25]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -24.428342 ± 5.932659, best_reward: -23.074004 ± 5.092964 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 25001it [00:38, 649.47it/s, agent_0/loss=0.135, agent_1/loss=0.760, agent_2/loss=0.811, env_step=250000, len=75, n/ep=80, n/st=6250, rew=-23.80]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  50\n",
      "Epoch #10: test_reward: -24.491059 ± 6.133425, best_reward: -23.074004 ± 5.092964 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 25001it [00:38, 647.73it/s, agent_0/loss=0.138, agent_1/loss=0.871, agent_2/loss=0.916, env_step=275000, len=75, n/ep=80, n/st=6250, rew=-23.03]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bests Saved Rew 56\n",
      "Epoch #11: test_reward: -22.860642 ± 5.997847, best_reward: -22.860642 ± 5.997847 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 25001it [00:37, 663.49it/s, agent_0/loss=0.129, agent_1/loss=0.785, agent_2/loss=0.849, env_step=300000, len=75, n/ep=90, n/st=6250, rew=-23.04]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  60\n",
      "Bests Saved Rew 61\n",
      "Epoch #12: test_reward: -22.547580 ± 5.349286, best_reward: -22.547580 ± 5.349286 in #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 25001it [00:37, 664.61it/s, agent_0/loss=0.127, agent_1/loss=0.788, agent_2/loss=0.925, env_step=325000, len=75, n/ep=80, n/st=6250, rew=-21.68]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: -23.351046 ± 5.653278, best_reward: -22.547580 ± 5.349286 in #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 25001it [00:38, 652.90it/s, agent_0/loss=0.129, agent_1/loss=0.853, agent_2/loss=0.889, env_step=350000, len=75, n/ep=80, n/st=6250, rew=-21.71]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  70\n",
      "Bests Saved Rew 71\n",
      "Epoch #14: test_reward: -21.657522 ± 5.314194, best_reward: -21.657522 ± 5.314194 in #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 25001it [00:38, 657.56it/s, agent_0/loss=0.134, agent_1/loss=0.818, agent_2/loss=0.855, env_step=375000, len=75, n/ep=90, n/st=6250, rew=-21.47]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bests Saved Rew 76\n",
      "Epoch #15: test_reward: -20.037465 ± 5.031871, best_reward: -20.037465 ± 5.031871 in #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 25001it [00:38, 653.11it/s, agent_0/loss=0.136, agent_1/loss=0.859, agent_2/loss=0.903, env_step=400000, len=75, n/ep=80, n/st=6250, rew=-22.07]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  80\n",
      "Epoch #16: test_reward: -21.103815 ± 4.685271, best_reward: -20.037465 ± 5.031871 in #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 25001it [00:36, 688.88it/s, agent_0/loss=0.134, agent_1/loss=0.866, agent_2/loss=0.912, env_step=425000, len=75, n/ep=80, n/st=6250, rew=-22.43]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: -21.055746 ± 5.953920, best_reward: -20.037465 ± 5.031871 in #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 25001it [00:37, 665.12it/s, agent_0/loss=0.130, agent_1/loss=0.805, agent_2/loss=0.882, env_step=450000, len=75, n/ep=90, n/st=6250, rew=-22.04]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  90\n",
      "Bests Saved Rew 91\n",
      "Epoch #18: test_reward: -19.561938 ± 5.183604, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 25001it [00:37, 670.93it/s, agent_0/loss=0.131, agent_1/loss=0.881, agent_2/loss=0.836, env_step=475000, len=75, n/ep=80, n/st=6250, rew=-21.44]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: -21.307860 ± 4.100875, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 25001it [00:36, 680.09it/s, agent_0/loss=0.122, agent_1/loss=0.827, agent_2/loss=0.857, env_step=500000, len=75, n/ep=80, n/st=6250, rew=-22.16]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  100\n",
      "Epoch #20: test_reward: -20.711333 ± 4.120550, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 25001it [00:36, 677.90it/s, agent_0/loss=0.131, agent_1/loss=0.859, agent_2/loss=0.858, env_step=525000, len=75, n/ep=90, n/st=6250, rew=-20.86]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: -21.650793 ± 4.815897, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 25001it [00:37, 674.05it/s, agent_0/loss=0.122, agent_1/loss=0.870, agent_2/loss=0.842, env_step=550000, len=75, n/ep=80, n/st=6250, rew=-21.88]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  110\n",
      "Epoch #22: test_reward: -21.774877 ± 5.281846, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 25001it [00:36, 678.73it/s, agent_0/loss=0.123, agent_1/loss=0.931, agent_2/loss=0.848, env_step=575000, len=75, n/ep=80, n/st=6250, rew=-21.70]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: -20.082534 ± 5.141089, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 25001it [00:36, 687.10it/s, agent_0/loss=0.116, agent_1/loss=0.890, agent_2/loss=0.838, env_step=600000, len=75, n/ep=90, n/st=6250, rew=-21.26]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  120\n",
      "Epoch #24: test_reward: -19.950928 ± 4.970826, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 25001it [00:37, 669.02it/s, agent_0/loss=0.116, agent_1/loss=0.894, agent_2/loss=0.829, env_step=625000, len=75, n/ep=80, n/st=6250, rew=-23.22]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #25: test_reward: -20.891955 ± 5.464428, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 25001it [00:36, 676.59it/s, agent_0/loss=0.116, agent_1/loss=0.955, agent_2/loss=0.789, env_step=650000, len=75, n/ep=80, n/st=6250, rew=-21.77]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  130\n",
      "Epoch #26: test_reward: -22.086153 ± 4.086262, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 25001it [00:36, 676.26it/s, agent_0/loss=0.116, agent_1/loss=0.994, agent_2/loss=0.801, env_step=675000, len=75, n/ep=90, n/st=6250, rew=-22.72]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #27: test_reward: -20.821669 ± 5.587397, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 25001it [00:37, 674.52it/s, agent_0/loss=0.110, agent_1/loss=1.119, agent_2/loss=0.805, env_step=700000, len=75, n/ep=80, n/st=6250, rew=-21.02]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  140\n",
      "Epoch #28: test_reward: -20.953780 ± 5.842889, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 25001it [00:36, 690.41it/s, agent_0/loss=0.114, agent_1/loss=1.118, agent_2/loss=0.825, env_step=725000, len=75, n/ep=80, n/st=6250, rew=-20.86]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: -23.927776 ± 6.109648, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 25001it [00:36, 689.15it/s, agent_0/loss=0.107, agent_1/loss=1.108, agent_2/loss=0.861, env_step=750000, len=75, n/ep=90, n/st=6250, rew=-21.62]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  150\n",
      "Epoch #30: test_reward: -20.705076 ± 5.036365, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 25001it [00:37, 672.48it/s, agent_0/loss=0.108, agent_1/loss=1.238, agent_2/loss=0.881, env_step=775000, len=75, n/ep=80, n/st=6250, rew=-22.96]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: -23.315482 ± 5.462823, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 25001it [00:36, 678.85it/s, agent_0/loss=0.110, agent_1/loss=1.291, agent_2/loss=0.860, env_step=800000, len=75, n/ep=80, n/st=6250, rew=-23.21]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  160\n",
      "Epoch #32: test_reward: -21.086562 ± 5.963281, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 25001it [00:36, 677.43it/s, agent_0/loss=0.109, agent_1/loss=1.403, agent_2/loss=0.873, env_step=825000, len=75, n/ep=90, n/st=6250, rew=-22.57]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: -23.077714 ± 6.709586, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 25001it [00:37, 672.55it/s, agent_0/loss=0.105, agent_1/loss=1.561, agent_2/loss=0.857, env_step=850000, len=75, n/ep=80, n/st=6250, rew=-23.57]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  170\n",
      "Epoch #34: test_reward: -21.089815 ± 4.615624, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 25001it [00:37, 668.57it/s, agent_0/loss=0.110, agent_1/loss=1.572, agent_2/loss=0.838, env_step=875000, len=75, n/ep=80, n/st=6250, rew=-21.66]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #35: test_reward: -23.301956 ± 5.028620, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 25001it [00:37, 669.19it/s, agent_0/loss=0.106, agent_1/loss=1.570, agent_2/loss=0.857, env_step=900000, len=75, n/ep=90, n/st=6250, rew=-22.18]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  180\n",
      "Epoch #36: test_reward: -21.493766 ± 5.055444, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 25001it [00:36, 680.06it/s, agent_0/loss=0.110, agent_1/loss=1.663, agent_2/loss=0.843, env_step=925000, len=75, n/ep=80, n/st=6250, rew=-21.53]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: -21.102559 ± 5.439577, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 25001it [00:36, 688.72it/s, agent_0/loss=0.103, agent_1/loss=1.792, agent_2/loss=0.838, env_step=950000, len=75, n/ep=80, n/st=6250, rew=-22.31]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  190\n",
      "Epoch #38: test_reward: -20.139896 ± 4.921473, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 25001it [00:36, 680.51it/s, agent_0/loss=0.106, agent_1/loss=1.817, agent_2/loss=0.796, env_step=975000, len=75, n/ep=90, n/st=6250, rew=-20.83]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: -21.244649 ± 5.641709, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 25001it [00:36, 676.34it/s, agent_0/loss=0.110, agent_1/loss=1.792, agent_2/loss=0.787, env_step=1000000, len=75, n/ep=80, n/st=6250, rew=-22.54]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  200\n",
      "Epoch #40: test_reward: -23.103881 ± 5.572133, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 25001it [00:37, 670.52it/s, agent_0/loss=0.108, agent_1/loss=1.849, agent_2/loss=0.775, env_step=1025000, len=75, n/ep=80, n/st=6250, rew=-21.83]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: -20.922868 ± 5.166278, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 25001it [00:36, 681.04it/s, agent_0/loss=0.107, agent_1/loss=1.770, agent_2/loss=0.740, env_step=1050000, len=75, n/ep=90, n/st=6250, rew=-22.25]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  210\n",
      "Epoch #42: test_reward: -21.397510 ± 5.965227, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 25001it [00:37, 669.71it/s, agent_0/loss=0.107, agent_1/loss=1.759, agent_2/loss=0.731, env_step=1075000, len=75, n/ep=80, n/st=6250, rew=-21.18]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: -20.983980 ± 5.653256, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 25001it [00:37, 674.85it/s, agent_0/loss=0.103, agent_1/loss=1.718, agent_2/loss=0.744, env_step=1100000, len=75, n/ep=80, n/st=6250, rew=-22.29]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  220\n",
      "Epoch #44: test_reward: -22.073666 ± 5.714171, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 25001it [00:37, 672.75it/s, agent_0/loss=0.109, agent_1/loss=1.625, agent_2/loss=0.748, env_step=1125000, len=75, n/ep=90, n/st=6250, rew=-21.39]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #45: test_reward: -22.655006 ± 5.617088, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #46: 25001it [00:36, 676.38it/s, agent_0/loss=0.107, agent_1/loss=1.798, agent_2/loss=0.701, env_step=1150000, len=75, n/ep=80, n/st=6250, rew=-22.12]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  230\n",
      "Epoch #46: test_reward: -23.201281 ± 5.510550, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #47: 25001it [00:37, 670.03it/s, agent_0/loss=0.105, agent_1/loss=1.766, agent_2/loss=0.666, env_step=1175000, len=75, n/ep=80, n/st=6250, rew=-22.78]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #47: test_reward: -21.929433 ± 5.803890, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #48: 25001it [00:37, 666.70it/s, agent_0/loss=0.100, agent_1/loss=1.732, agent_2/loss=0.715, env_step=1200000, len=75, n/ep=90, n/st=6250, rew=-22.49]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  240\n",
      "Epoch #48: test_reward: -20.910840 ± 5.231448, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #49: 25001it [00:36, 682.67it/s, agent_0/loss=0.105, agent_1/loss=1.848, agent_2/loss=0.704, env_step=1225000, len=75, n/ep=80, n/st=6250, rew=-23.16]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #49: test_reward: -22.473139 ± 6.554896, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #50: 25001it [00:37, 661.17it/s, agent_0/loss=0.111, agent_1/loss=1.753, agent_2/loss=0.686, env_step=1250000, len=75, n/ep=80, n/st=6250, rew=-22.04]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  250\n",
      "Epoch #50: test_reward: -21.638234 ± 5.077276, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #51: 25001it [00:37, 673.27it/s, agent_0/loss=0.103, agent_1/loss=1.749, agent_2/loss=0.718, env_step=1275000, len=75, n/ep=90, n/st=6250, rew=-21.13]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #51: test_reward: -22.896583 ± 6.075582, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #52: 25001it [00:36, 682.74it/s, agent_0/loss=0.106, agent_1/loss=1.864, agent_2/loss=0.698, env_step=1300000, len=75, n/ep=80, n/st=6250, rew=-22.05]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  260\n",
      "Epoch #52: test_reward: -23.180570 ± 4.760844, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #53: 25001it [00:37, 670.72it/s, agent_0/loss=0.107, agent_1/loss=1.860, agent_2/loss=0.707, env_step=1325000, len=75, n/ep=80, n/st=6250, rew=-21.94]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #53: test_reward: -23.907511 ± 5.511402, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #54: 25001it [00:37, 666.76it/s, agent_0/loss=0.103, agent_1/loss=1.758, agent_2/loss=0.739, env_step=1350000, len=75, n/ep=90, n/st=6250, rew=-21.52]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  270\n",
      "Epoch #54: test_reward: -21.161816 ± 5.607961, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #55: 25001it [00:36, 681.19it/s, agent_0/loss=0.104, agent_1/loss=1.833, agent_2/loss=0.669, env_step=1375000, len=75, n/ep=80, n/st=6250, rew=-21.41]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #55: test_reward: -21.716060 ± 5.009540, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #56: 25001it [00:37, 669.37it/s, agent_0/loss=0.104, agent_1/loss=1.805, agent_2/loss=0.689, env_step=1400000, len=75, n/ep=80, n/st=6250, rew=-23.16]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  280\n",
      "Epoch #56: test_reward: -23.080426 ± 5.984202, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #57: 25001it [00:37, 667.33it/s, agent_0/loss=0.105, agent_1/loss=1.838, agent_2/loss=0.653, env_step=1425000, len=75, n/ep=90, n/st=6250, rew=-23.66]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #57: test_reward: -19.588593 ± 4.880962, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #58: 25001it [00:36, 686.43it/s, agent_0/loss=0.106, agent_1/loss=1.883, agent_2/loss=0.654, env_step=1450000, len=75, n/ep=80, n/st=6250, rew=-22.00]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  290\n",
      "Epoch #58: test_reward: -21.763866 ± 4.722534, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #59: 25001it [00:37, 672.96it/s, agent_0/loss=0.107, agent_1/loss=1.859, agent_2/loss=0.646, env_step=1475000, len=75, n/ep=80, n/st=6250, rew=-21.42]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #59: test_reward: -20.152047 ± 5.189259, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #60: 25001it [00:37, 674.93it/s, agent_0/loss=0.101, agent_1/loss=1.829, agent_2/loss=0.636, env_step=1500000, len=75, n/ep=90, n/st=6250, rew=-21.82]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  300\n",
      "Epoch #60: test_reward: -23.926611 ± 6.714333, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #61: 25001it [00:37, 674.75it/s, agent_0/loss=0.101, agent_1/loss=1.680, agent_2/loss=0.641, env_step=1525000, len=75, n/ep=80, n/st=6250, rew=-23.05]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #61: test_reward: -21.860168 ± 6.518977, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #62: 25001it [00:37, 672.13it/s, agent_0/loss=0.107, agent_1/loss=1.903, agent_2/loss=0.637, env_step=1550000, len=75, n/ep=80, n/st=6250, rew=-23.36]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  310\n",
      "Epoch #62: test_reward: -22.172437 ± 5.124099, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #63: 25001it [00:37, 674.48it/s, agent_0/loss=0.108, agent_1/loss=1.803, agent_2/loss=0.664, env_step=1575000, len=75, n/ep=90, n/st=6250, rew=-22.24]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #63: test_reward: -20.525994 ± 6.012541, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #64: 25001it [00:37, 664.50it/s, agent_0/loss=0.110, agent_1/loss=1.870, agent_2/loss=0.656, env_step=1600000, len=75, n/ep=80, n/st=6250, rew=-22.77]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  320\n",
      "Epoch #64: test_reward: -22.564471 ± 5.859548, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #65: 25001it [00:37, 659.30it/s, agent_0/loss=0.108, agent_1/loss=1.817, agent_2/loss=0.645, env_step=1625000, len=75, n/ep=80, n/st=6250, rew=-23.30]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #65: test_reward: -21.608489 ± 5.482060, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #66: 25001it [00:36, 688.00it/s, agent_0/loss=0.106, agent_1/loss=1.712, agent_2/loss=0.653, env_step=1650000, len=75, n/ep=90, n/st=6250, rew=-22.67]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  330\n",
      "Epoch #66: test_reward: -21.834525 ± 4.423151, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #67: 25001it [00:36, 679.81it/s, agent_0/loss=0.111, agent_1/loss=1.759, agent_2/loss=0.654, env_step=1675000, len=75, n/ep=80, n/st=6250, rew=-21.49]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #67: test_reward: -23.775690 ± 6.963774, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #68: 25001it [00:37, 663.22it/s, agent_0/loss=0.109, agent_1/loss=1.667, agent_2/loss=0.676, env_step=1700000, len=75, n/ep=80, n/st=6250, rew=-21.18]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  340\n",
      "Epoch #68: test_reward: -21.231197 ± 4.948625, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #69: 25001it [00:37, 665.69it/s, agent_0/loss=0.108, agent_1/loss=1.776, agent_2/loss=0.687, env_step=1725000, len=75, n/ep=90, n/st=6250, rew=-23.03]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #69: test_reward: -21.734304 ± 7.300340, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #70: 25001it [00:36, 675.70it/s, agent_0/loss=0.108, agent_1/loss=1.678, agent_2/loss=0.666, env_step=1750000, len=75, n/ep=80, n/st=6250, rew=-21.72]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  350\n",
      "Epoch #70: test_reward: -21.955084 ± 5.828222, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #71: 25001it [00:36, 675.70it/s, agent_0/loss=0.115, agent_1/loss=1.701, agent_2/loss=0.705, env_step=1775000, len=75, n/ep=80, n/st=6250, rew=-23.18]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #71: test_reward: -20.704151 ± 4.883298, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #72: 25001it [00:37, 668.75it/s, agent_0/loss=0.115, agent_1/loss=1.716, agent_2/loss=0.691, env_step=1800000, len=75, n/ep=90, n/st=6250, rew=-22.15]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  360\n",
      "Epoch #72: test_reward: -22.047434 ± 5.602587, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #73: 25001it [00:37, 671.69it/s, agent_0/loss=0.116, agent_1/loss=1.713, agent_2/loss=0.716, env_step=1825000, len=75, n/ep=80, n/st=6250, rew=-22.35]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #73: test_reward: -24.100353 ± 6.809801, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #74: 25001it [00:37, 661.71it/s, agent_0/loss=0.116, agent_1/loss=1.716, agent_2/loss=0.717, env_step=1850000, len=75, n/ep=80, n/st=6250, rew=-22.82]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  370\n",
      "Epoch #74: test_reward: -22.801630 ± 5.824508, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #75: 25001it [00:36, 676.21it/s, agent_0/loss=0.121, agent_1/loss=1.782, agent_2/loss=0.738, env_step=1875000, len=75, n/ep=90, n/st=6250, rew=-21.89]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #75: test_reward: -23.525403 ± 5.675313, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #76: 25001it [00:37, 673.12it/s, agent_0/loss=0.119, agent_1/loss=1.761, agent_2/loss=0.707, env_step=1900000, len=75, n/ep=80, n/st=6250, rew=-21.58]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  380\n",
      "Epoch #76: test_reward: -22.242692 ± 6.203020, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #77: 25001it [00:37, 674.32it/s, agent_0/loss=0.118, agent_1/loss=1.807, agent_2/loss=0.684, env_step=1925000, len=75, n/ep=80, n/st=6250, rew=-21.19]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #77: test_reward: -21.399269 ± 6.814529, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #78: 25001it [00:36, 688.90it/s, agent_0/loss=0.121, agent_1/loss=1.777, agent_2/loss=0.720, env_step=1950000, len=75, n/ep=90, n/st=6250, rew=-23.46]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  390\n",
      "Epoch #78: test_reward: -21.615475 ± 5.346808, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #79: 25001it [00:36, 676.27it/s, agent_0/loss=0.118, agent_1/loss=1.877, agent_2/loss=0.743, env_step=1975000, len=75, n/ep=80, n/st=6250, rew=-21.84]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #79: test_reward: -22.094981 ± 5.881762, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #80: 25001it [00:37, 673.24it/s, agent_0/loss=0.121, agent_1/loss=1.818, agent_2/loss=0.752, env_step=2000000, len=75, n/ep=80, n/st=6250, rew=-23.12]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  400\n",
      "Epoch #80: test_reward: -21.632960 ± 5.229812, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #81: 25001it [00:37, 675.01it/s, agent_0/loss=0.116, agent_1/loss=1.926, agent_2/loss=0.754, env_step=2025000, len=75, n/ep=90, n/st=6250, rew=-21.21]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #81: test_reward: -21.850786 ± 5.097189, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #82: 25001it [00:37, 669.82it/s, agent_0/loss=0.122, agent_1/loss=1.914, agent_2/loss=0.731, env_step=2050000, len=75, n/ep=80, n/st=6250, rew=-21.51]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  410\n",
      "Epoch #82: test_reward: -23.500082 ± 5.033359, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #83: 25001it [00:37, 667.28it/s, agent_0/loss=0.119, agent_1/loss=2.078, agent_2/loss=0.770, env_step=2075000, len=75, n/ep=80, n/st=6250, rew=-22.55]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #83: test_reward: -21.320193 ± 5.809093, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #84: 25001it [00:36, 677.91it/s, agent_0/loss=0.120, agent_1/loss=2.109, agent_2/loss=0.801, env_step=2100000, len=75, n/ep=90, n/st=6250, rew=-23.04]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  420\n",
      "Epoch #84: test_reward: -22.374945 ± 5.444317, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #85: 25001it [00:37, 667.94it/s, agent_0/loss=0.117, agent_1/loss=2.065, agent_2/loss=0.753, env_step=2125000, len=75, n/ep=80, n/st=6250, rew=-22.35]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #85: test_reward: -22.091433 ± 4.872588, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #86: 25001it [00:37, 668.60it/s, agent_0/loss=0.118, agent_1/loss=2.036, agent_2/loss=0.743, env_step=2150000, len=75, n/ep=80, n/st=6250, rew=-21.42]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  430\n",
      "Epoch #86: test_reward: -22.041180 ± 6.161554, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #87: 25001it [00:36, 680.31it/s, agent_0/loss=0.124, agent_1/loss=2.098, agent_2/loss=0.730, env_step=2175000, len=75, n/ep=90, n/st=6250, rew=-21.92]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #87: test_reward: -21.782127 ± 5.650386, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #88: 25001it [00:36, 676.36it/s, agent_0/loss=0.116, agent_1/loss=2.129, agent_2/loss=0.768, env_step=2200000, len=75, n/ep=80, n/st=6250, rew=-22.59]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  440\n",
      "Epoch #88: test_reward: -21.730274 ± 5.141314, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #89: 25001it [00:36, 680.51it/s, agent_0/loss=0.124, agent_1/loss=2.280, agent_2/loss=0.788, env_step=2225000, len=75, n/ep=80, n/st=6250, rew=-23.30]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #89: test_reward: -21.677077 ± 5.379996, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #90: 25001it [00:36, 686.20it/s, agent_0/loss=0.117, agent_1/loss=2.271, agent_2/loss=0.765, env_step=2250000, len=75, n/ep=90, n/st=6250, rew=-21.22]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  450\n",
      "Epoch #90: test_reward: -22.465733 ± 4.883785, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #91: 25001it [00:36, 678.31it/s, agent_0/loss=0.114, agent_1/loss=2.062, agent_2/loss=0.777, env_step=2275000, len=75, n/ep=80, n/st=6250, rew=-22.87]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #91: test_reward: -22.295109 ± 5.724512, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #92: 25001it [00:37, 671.32it/s, agent_0/loss=0.117, agent_1/loss=2.158, agent_2/loss=0.716, env_step=2300000, len=75, n/ep=80, n/st=6250, rew=-22.41]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  460\n",
      "Epoch #92: test_reward: -22.503671 ± 5.858489, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #93: 25001it [00:36, 678.38it/s, agent_0/loss=0.114, agent_1/loss=2.104, agent_2/loss=0.746, env_step=2325000, len=75, n/ep=90, n/st=6250, rew=-22.36]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #93: test_reward: -21.133923 ± 3.976560, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #94: 25001it [00:37, 675.41it/s, agent_0/loss=0.115, agent_1/loss=2.353, agent_2/loss=0.735, env_step=2350000, len=75, n/ep=80, n/st=6250, rew=-21.94]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  470\n",
      "Epoch #94: test_reward: -22.688392 ± 5.153698, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #95: 25001it [00:37, 673.08it/s, agent_0/loss=0.111, agent_1/loss=2.180, agent_2/loss=0.736, env_step=2375000, len=75, n/ep=80, n/st=6250, rew=-21.31]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #95: test_reward: -22.271677 ± 6.458102, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #96: 25001it [00:37, 662.75it/s, agent_0/loss=0.111, agent_1/loss=2.298, agent_2/loss=0.702, env_step=2400000, len=75, n/ep=90, n/st=6250, rew=-20.73]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  480\n",
      "Epoch #96: test_reward: -21.832145 ± 6.143395, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #97: 25001it [00:37, 671.18it/s, agent_0/loss=0.107, agent_1/loss=2.352, agent_2/loss=0.720, env_step=2425000, len=75, n/ep=80, n/st=6250, rew=-22.10]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #97: test_reward: -22.791206 ± 5.067688, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #98: 25001it [00:36, 685.64it/s, agent_0/loss=0.115, agent_1/loss=2.440, agent_2/loss=0.770, env_step=2450000, len=75, n/ep=80, n/st=6250, rew=-22.90]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  490\n",
      "Epoch #98: test_reward: -23.749663 ± 5.306764, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #99: 25001it [00:36, 680.23it/s, agent_0/loss=0.115, agent_1/loss=2.368, agent_2/loss=0.814, env_step=2475000, len=75, n/ep=90, n/st=6250, rew=-22.63]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #99: test_reward: -22.175370 ± 6.229390, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #100: 25001it [00:37, 669.19it/s, agent_0/loss=0.112, agent_1/loss=2.397, agent_2/loss=0.798, env_step=2500000, len=75, n/ep=80, n/st=6250, rew=-22.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  500\n",
      "Epoch #100: test_reward: -22.160959 ± 5.917115, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #101: 25001it [00:37, 671.76it/s, agent_0/loss=0.110, agent_1/loss=2.398, agent_2/loss=0.806, env_step=2525000, len=75, n/ep=80, n/st=6250, rew=-22.17]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #101: test_reward: -22.299464 ± 4.975260, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #102: 25001it [00:37, 673.38it/s, agent_0/loss=0.116, agent_1/loss=2.380, agent_2/loss=0.797, env_step=2550000, len=75, n/ep=90, n/st=6250, rew=-22.47]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  510\n",
      "Epoch #102: test_reward: -21.613463 ± 5.250341, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #103: 25001it [00:36, 676.70it/s, agent_0/loss=0.107, agent_1/loss=2.453, agent_2/loss=0.828, env_step=2575000, len=75, n/ep=80, n/st=6250, rew=-22.58]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #103: test_reward: -20.862072 ± 5.382030, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #104: 25001it [00:37, 663.01it/s, agent_0/loss=0.115, agent_1/loss=2.579, agent_2/loss=0.825, env_step=2600000, len=75, n/ep=80, n/st=6250, rew=-23.06]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  520\n",
      "Epoch #104: test_reward: -20.817407 ± 5.952690, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #105: 25001it [00:37, 673.95it/s, agent_0/loss=0.115, agent_1/loss=2.673, agent_2/loss=0.866, env_step=2625000, len=75, n/ep=90, n/st=6250, rew=-22.44]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #105: test_reward: -22.356222 ± 5.965334, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #106: 25001it [00:37, 667.91it/s, agent_0/loss=0.116, agent_1/loss=2.706, agent_2/loss=0.836, env_step=2650000, len=75, n/ep=80, n/st=6250, rew=-22.86]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  530\n",
      "Epoch #106: test_reward: -22.309251 ± 5.619907, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #107: 25001it [00:37, 666.12it/s, agent_0/loss=0.118, agent_1/loss=2.800, agent_2/loss=0.835, env_step=2675000, len=75, n/ep=80, n/st=6250, rew=-22.68]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #107: test_reward: -23.946846 ± 6.704443, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #108: 25001it [00:37, 672.95it/s, agent_0/loss=0.109, agent_1/loss=2.752, agent_2/loss=0.829, env_step=2700000, len=75, n/ep=90, n/st=6250, rew=-23.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  540\n",
      "Epoch #108: test_reward: -21.884476 ± 5.328155, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #109: 25001it [00:37, 666.14it/s, agent_0/loss=0.115, agent_1/loss=2.610, agent_2/loss=0.814, env_step=2725000, len=75, n/ep=80, n/st=6250, rew=-21.95]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #109: test_reward: -22.899843 ± 6.067396, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #110: 25001it [00:38, 655.61it/s, agent_0/loss=0.118, agent_1/loss=2.811, agent_2/loss=0.824, env_step=2750000, len=75, n/ep=80, n/st=6250, rew=-23.79]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  550\n",
      "Epoch #110: test_reward: -23.386083 ± 5.861976, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #111: 25001it [00:36, 676.15it/s, agent_0/loss=0.118, agent_1/loss=2.724, agent_2/loss=0.815, env_step=2775000, len=75, n/ep=90, n/st=6250, rew=-21.62]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #111: test_reward: -21.753446 ± 6.331835, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #112: 25001it [00:37, 668.69it/s, agent_0/loss=0.119, agent_1/loss=2.959, agent_2/loss=0.862, env_step=2800000, len=75, n/ep=80, n/st=6250, rew=-22.76]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  560\n",
      "Epoch #112: test_reward: -22.294198 ± 6.491027, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #113: 25001it [00:37, 663.43it/s, agent_0/loss=0.122, agent_1/loss=2.925, agent_2/loss=0.828, env_step=2825000, len=75, n/ep=80, n/st=6250, rew=-23.10]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #113: test_reward: -21.493673 ± 6.099589, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #114: 25001it [00:37, 673.76it/s, agent_0/loss=0.116, agent_1/loss=2.984, agent_2/loss=0.824, env_step=2850000, len=75, n/ep=90, n/st=6250, rew=-21.57]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  570\n",
      "Epoch #114: test_reward: -22.953397 ± 5.744012, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #115: 25001it [00:36, 675.95it/s, agent_0/loss=0.112, agent_1/loss=3.140, agent_2/loss=0.834, env_step=2875000, len=75, n/ep=80, n/st=6250, rew=-22.55]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #115: test_reward: -23.154501 ± 4.862374, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #116: 25001it [00:36, 687.53it/s, agent_0/loss=0.115, agent_1/loss=3.324, agent_2/loss=0.841, env_step=2900000, len=75, n/ep=80, n/st=6250, rew=-22.46]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  580\n",
      "Epoch #116: test_reward: -22.136703 ± 6.888936, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #117: 25001it [00:37, 671.63it/s, agent_0/loss=0.116, agent_1/loss=3.292, agent_2/loss=0.811, env_step=2925000, len=75, n/ep=90, n/st=6250, rew=-22.74]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #117: test_reward: -19.935038 ± 5.200613, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #118: 25001it [00:37, 670.18it/s, agent_0/loss=0.111, agent_1/loss=3.542, agent_2/loss=0.831, env_step=2950000, len=75, n/ep=80, n/st=6250, rew=-21.98]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  590\n",
      "Epoch #118: test_reward: -23.818289 ± 5.820777, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #119: 25001it [00:37, 671.58it/s, agent_0/loss=0.112, agent_1/loss=3.623, agent_2/loss=0.853, env_step=2975000, len=75, n/ep=80, n/st=6250, rew=-21.60]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #119: test_reward: -22.318479 ± 5.303616, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #120: 25001it [00:37, 672.80it/s, agent_0/loss=0.115, agent_1/loss=3.819, agent_2/loss=0.871, env_step=3000000, len=75, n/ep=90, n/st=6250, rew=-21.33]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  600\n",
      "Epoch #120: test_reward: -22.230720 ± 5.117157, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #121: 25001it [00:36, 678.52it/s, agent_0/loss=0.118, agent_1/loss=4.298, agent_2/loss=0.877, env_step=3025000, len=75, n/ep=80, n/st=6250, rew=-22.69]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #121: test_reward: -20.376609 ± 5.047527, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #122: 25001it [00:36, 677.50it/s, agent_0/loss=0.109, agent_1/loss=4.389, agent_2/loss=0.876, env_step=3050000, len=75, n/ep=80, n/st=6250, rew=-23.08]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  610\n",
      "Epoch #122: test_reward: -22.170391 ± 4.954196, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #123: 25001it [00:37, 668.10it/s, agent_0/loss=0.112, agent_1/loss=4.632, agent_2/loss=0.889, env_step=3075000, len=75, n/ep=90, n/st=6250, rew=-20.48]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #123: test_reward: -20.643321 ± 5.237300, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #124: 25001it [00:37, 669.28it/s, agent_0/loss=0.119, agent_1/loss=4.592, agent_2/loss=0.918, env_step=3100000, len=75, n/ep=80, n/st=6250, rew=-22.51]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  620\n",
      "Epoch #124: test_reward: -22.348237 ± 5.507128, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #125: 25001it [00:37, 668.32it/s, agent_0/loss=0.109, agent_1/loss=4.808, agent_2/loss=0.874, env_step=3125000, len=75, n/ep=80, n/st=6250, rew=-23.09]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #125: test_reward: -23.678939 ± 5.836737, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #126: 25001it [00:37, 671.10it/s, agent_0/loss=0.110, agent_1/loss=4.853, agent_2/loss=0.946, env_step=3150000, len=75, n/ep=90, n/st=6250, rew=-21.92]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  630\n",
      "Epoch #126: test_reward: -21.120788 ± 5.424585, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #127: 25001it [00:37, 670.76it/s, agent_0/loss=0.114, agent_1/loss=4.637, agent_2/loss=0.976, env_step=3175000, len=75, n/ep=80, n/st=6250, rew=-22.45]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #127: test_reward: -23.290054 ± 5.004513, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #128: 25001it [00:36, 683.80it/s, agent_0/loss=0.111, agent_1/loss=4.847, agent_2/loss=0.948, env_step=3200000, len=75, n/ep=80, n/st=6250, rew=-21.62]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  640\n",
      "Epoch #128: test_reward: -23.850110 ± 5.684213, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #129: 25001it [00:37, 675.05it/s, agent_0/loss=0.116, agent_1/loss=4.887, agent_2/loss=0.921, env_step=3225000, len=75, n/ep=90, n/st=6250, rew=-23.20]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #129: test_reward: -23.583745 ± 6.246853, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #130: 25001it [00:37, 659.83it/s, agent_0/loss=0.118, agent_1/loss=5.366, agent_2/loss=1.004, env_step=3250000, len=75, n/ep=80, n/st=6250, rew=-22.87]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  650\n",
      "Epoch #130: test_reward: -21.162242 ± 4.912563, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #131: 25001it [00:37, 665.10it/s, agent_0/loss=0.117, agent_1/loss=4.884, agent_2/loss=0.982, env_step=3275000, len=75, n/ep=80, n/st=6250, rew=-23.41]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #131: test_reward: -23.893773 ± 5.526943, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #132: 25001it [00:37, 674.86it/s, agent_0/loss=0.113, agent_1/loss=4.861, agent_2/loss=1.016, env_step=3300000, len=75, n/ep=90, n/st=6250, rew=-22.96]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  660\n",
      "Epoch #132: test_reward: -20.926925 ± 5.491453, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #133: 25001it [00:36, 675.92it/s, agent_0/loss=0.117, agent_1/loss=4.754, agent_2/loss=1.070, env_step=3325000, len=75, n/ep=80, n/st=6250, rew=-22.46]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #133: test_reward: -20.994067 ± 5.556536, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #134: 25001it [00:36, 677.83it/s, agent_0/loss=0.123, agent_1/loss=5.196, agent_2/loss=1.057, env_step=3350000, len=75, n/ep=80, n/st=6250, rew=-23.39]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  670\n",
      "Epoch #134: test_reward: -21.783523 ± 5.895973, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #135: 25001it [00:37, 675.26it/s, agent_0/loss=0.124, agent_1/loss=5.191, agent_2/loss=1.146, env_step=3375000, len=75, n/ep=90, n/st=6250, rew=-22.64]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #135: test_reward: -22.322557 ± 6.427406, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #136: 25001it [00:36, 679.28it/s, agent_0/loss=0.122, agent_1/loss=5.061, agent_2/loss=1.168, env_step=3400000, len=75, n/ep=80, n/st=6250, rew=-23.41]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  680\n",
      "Epoch #136: test_reward: -22.151269 ± 6.290540, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #137: 25001it [00:37, 666.12it/s, agent_0/loss=0.123, agent_1/loss=5.193, agent_2/loss=1.192, env_step=3425000, len=75, n/ep=80, n/st=6250, rew=-23.05]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #137: test_reward: -21.940273 ± 5.758359, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #138: 25001it [00:37, 673.77it/s, agent_0/loss=0.126, agent_1/loss=5.287, agent_2/loss=1.290, env_step=3450000, len=75, n/ep=90, n/st=6250, rew=-24.75]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  690\n",
      "Epoch #138: test_reward: -22.822360 ± 5.012565, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #139: 25001it [00:37, 675.55it/s, agent_0/loss=0.131, agent_1/loss=5.323, agent_2/loss=1.329, env_step=3475000, len=75, n/ep=80, n/st=6250, rew=-22.86]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #139: test_reward: -22.461543 ± 6.074702, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #140: 25001it [00:36, 676.62it/s, agent_0/loss=0.125, agent_1/loss=5.403, agent_2/loss=1.376, env_step=3500000, len=75, n/ep=80, n/st=6250, rew=-22.56]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  700\n",
      "Epoch #140: test_reward: -22.497828 ± 4.818070, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #141: 25001it [00:37, 669.65it/s, agent_0/loss=0.122, agent_1/loss=5.045, agent_2/loss=1.318, env_step=3525000, len=75, n/ep=90, n/st=6250, rew=-23.12]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #141: test_reward: -23.281082 ± 7.269137, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #142: 25001it [00:37, 668.88it/s, agent_0/loss=0.128, agent_1/loss=4.839, agent_2/loss=1.323, env_step=3550000, len=75, n/ep=80, n/st=6250, rew=-23.11]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  710\n",
      "Epoch #142: test_reward: -22.800541 ± 5.412067, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #143: 25001it [00:36, 685.22it/s, agent_0/loss=0.129, agent_1/loss=4.855, agent_2/loss=1.357, env_step=3575000, len=75, n/ep=80, n/st=6250, rew=-22.76]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #143: test_reward: -21.215246 ± 5.459678, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #144: 25001it [00:37, 675.61it/s, agent_0/loss=0.121, agent_1/loss=5.068, agent_2/loss=1.295, env_step=3600000, len=75, n/ep=90, n/st=6250, rew=-22.92]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  720\n",
      "Epoch #144: test_reward: -23.257903 ± 5.418807, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #145: 25001it [00:36, 678.07it/s, agent_0/loss=0.115, agent_1/loss=5.152, agent_2/loss=1.354, env_step=3625000, len=75, n/ep=80, n/st=6250, rew=-22.39]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #145: test_reward: -21.788221 ± 5.058685, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #146: 25001it [00:37, 668.35it/s, agent_0/loss=0.126, agent_1/loss=5.308, agent_2/loss=1.268, env_step=3650000, len=75, n/ep=80, n/st=6250, rew=-21.73]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  730\n",
      "Epoch #146: test_reward: -22.973689 ± 5.846022, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #147: 25001it [00:37, 673.59it/s, agent_0/loss=0.122, agent_1/loss=5.149, agent_2/loss=1.302, env_step=3675000, len=75, n/ep=90, n/st=6250, rew=-22.67]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #147: test_reward: -22.716581 ± 5.952240, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #148: 25001it [00:37, 671.78it/s, agent_0/loss=0.123, agent_1/loss=5.382, agent_2/loss=1.296, env_step=3700000, len=75, n/ep=80, n/st=6250, rew=-22.70]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  740\n",
      "Epoch #148: test_reward: -23.956130 ± 6.340063, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #149: 25001it [00:37, 664.68it/s, agent_0/loss=0.121, agent_1/loss=5.159, agent_2/loss=1.343, env_step=3725000, len=75, n/ep=80, n/st=6250, rew=-22.84]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #149: test_reward: -23.175443 ± 5.645475, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #150: 25001it [00:37, 665.75it/s, agent_0/loss=0.120, agent_1/loss=5.501, agent_2/loss=1.281, env_step=3750000, len=75, n/ep=90, n/st=6250, rew=-24.36]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  750\n",
      "Epoch #150: test_reward: -24.431230 ± 8.080755, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #151: 25001it [00:38, 655.57it/s, agent_0/loss=0.128, agent_1/loss=5.537, agent_2/loss=1.351, env_step=3775000, len=75, n/ep=80, n/st=6250, rew=-23.28]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #151: test_reward: -22.569840 ± 6.104083, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #152: 25001it [00:37, 662.65it/s, agent_0/loss=0.125, agent_1/loss=5.635, agent_2/loss=1.438, env_step=3800000, len=75, n/ep=80, n/st=6250, rew=-22.50]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  760\n",
      "Epoch #152: test_reward: -22.759193 ± 6.747307, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #153: 25001it [00:37, 665.19it/s, agent_0/loss=0.137, agent_1/loss=6.482, agent_2/loss=1.343, env_step=3825000, len=75, n/ep=90, n/st=6250, rew=-22.57]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #153: test_reward: -23.085709 ± 5.100723, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #154: 25001it [00:37, 666.79it/s, agent_0/loss=0.129, agent_1/loss=6.545, agent_2/loss=1.352, env_step=3850000, len=75, n/ep=80, n/st=6250, rew=-22.38]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  770\n",
      "Epoch #154: test_reward: -22.523286 ± 5.477719, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #155: 25001it [00:36, 676.75it/s, agent_0/loss=0.130, agent_1/loss=6.460, agent_2/loss=1.356, env_step=3875000, len=75, n/ep=80, n/st=6250, rew=-24.05]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #155: test_reward: -22.235737 ± 4.646235, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #156: 25001it [00:36, 681.99it/s, agent_0/loss=0.128, agent_1/loss=6.687, agent_2/loss=1.248, env_step=3900000, len=75, n/ep=90, n/st=6250, rew=-24.23]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  780\n",
      "Epoch #156: test_reward: -23.160760 ± 5.670305, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #157: 25001it [00:37, 670.46it/s, agent_0/loss=0.129, agent_1/loss=6.930, agent_2/loss=1.312, env_step=3925000, len=75, n/ep=80, n/st=6250, rew=-23.86]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #157: test_reward: -22.360173 ± 6.459101, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #158: 25001it [00:36, 682.03it/s, agent_0/loss=0.132, agent_1/loss=7.079, agent_2/loss=1.379, env_step=3950000, len=75, n/ep=80, n/st=6250, rew=-22.20]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  790\n",
      "Epoch #158: test_reward: -21.011265 ± 5.462896, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #159: 25001it [00:36, 683.85it/s, agent_0/loss=0.129, agent_1/loss=7.193, agent_2/loss=1.309, env_step=3975000, len=75, n/ep=90, n/st=6250, rew=-21.60]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #159: test_reward: -22.224223 ± 5.991949, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #160: 25001it [00:36, 676.66it/s, agent_0/loss=0.131, agent_1/loss=7.228, agent_2/loss=1.304, env_step=4000000, len=75, n/ep=80, n/st=6250, rew=-23.89]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  800\n",
      "Epoch #160: test_reward: -22.825863 ± 6.544934, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #161: 25001it [00:36, 678.04it/s, agent_0/loss=0.126, agent_1/loss=7.111, agent_2/loss=1.236, env_step=4025000, len=75, n/ep=80, n/st=6250, rew=-24.23]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #161: test_reward: -24.218172 ± 5.898534, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #162: 25001it [00:37, 675.24it/s, agent_0/loss=0.129, agent_1/loss=7.565, agent_2/loss=1.175, env_step=4050000, len=75, n/ep=90, n/st=6250, rew=-22.40]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  810\n",
      "Epoch #162: test_reward: -22.590332 ± 5.523637, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #163: 25001it [00:37, 675.70it/s, agent_0/loss=0.125, agent_1/loss=7.486, agent_2/loss=1.178, env_step=4075000, len=75, n/ep=80, n/st=6250, rew=-22.74]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #163: test_reward: -21.645664 ± 5.494630, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #164: 25001it [00:37, 666.00it/s, agent_0/loss=0.121, agent_1/loss=7.660, agent_2/loss=1.218, env_step=4100000, len=75, n/ep=80, n/st=6250, rew=-22.72]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  820\n",
      "Epoch #164: test_reward: -23.130785 ± 5.564605, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #165: 25001it [00:38, 657.00it/s, agent_0/loss=0.116, agent_1/loss=7.315, agent_2/loss=1.221, env_step=4125000, len=75, n/ep=90, n/st=6250, rew=-22.81]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #165: test_reward: -22.892066 ± 5.645934, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #166: 25001it [00:37, 669.55it/s, agent_0/loss=0.118, agent_1/loss=7.445, agent_2/loss=1.201, env_step=4150000, len=75, n/ep=80, n/st=6250, rew=-22.53]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  830\n",
      "Epoch #166: test_reward: -21.178503 ± 5.452819, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #167: 25001it [00:37, 667.23it/s, agent_0/loss=0.121, agent_1/loss=7.645, agent_2/loss=1.253, env_step=4175000, len=75, n/ep=80, n/st=6250, rew=-23.96]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #167: test_reward: -24.061956 ± 6.822286, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #168: 25001it [00:37, 660.24it/s, agent_0/loss=0.118, agent_1/loss=7.298, agent_2/loss=1.208, env_step=4200000, len=75, n/ep=90, n/st=6250, rew=-21.69]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  840\n",
      "Epoch #168: test_reward: -22.813340 ± 5.929744, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #169: 25001it [00:34, 734.18it/s, agent_0/loss=0.119, agent_1/loss=6.682, agent_2/loss=1.234, env_step=4225000, len=75, n/ep=80, n/st=6250, rew=-22.69]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #169: test_reward: -23.143089 ± 5.675607, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #170: 25001it [00:35, 696.96it/s, agent_0/loss=0.116, agent_1/loss=6.550, agent_2/loss=1.291, env_step=4250000, len=75, n/ep=80, n/st=6250, rew=-23.14]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  850\n",
      "Epoch #170: test_reward: -24.282327 ± 6.039897, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #171: 25001it [00:34, 714.61it/s, agent_0/loss=0.124, agent_1/loss=6.653, agent_2/loss=1.325, env_step=4275000, len=75, n/ep=90, n/st=6250, rew=-23.20]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #171: test_reward: -23.277258 ± 6.548444, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #172: 25001it [00:35, 699.63it/s, agent_0/loss=0.113, agent_1/loss=5.995, agent_2/loss=1.352, env_step=4300000, len=75, n/ep=80, n/st=6250, rew=-22.56]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  860\n",
      "Epoch #172: test_reward: -23.023154 ± 7.765280, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #173: 25001it [00:36, 678.20it/s, agent_0/loss=0.111, agent_1/loss=6.150, agent_2/loss=1.367, env_step=4325000, len=75, n/ep=80, n/st=6250, rew=-22.15]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #173: test_reward: -22.331586 ± 5.741615, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #174: 25001it [00:36, 681.47it/s, agent_0/loss=0.115, agent_1/loss=5.975, agent_2/loss=1.481, env_step=4350000, len=75, n/ep=90, n/st=6250, rew=-23.53]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  870\n",
      "Epoch #174: test_reward: -23.539597 ± 6.763303, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #175: 25001it [00:36, 676.85it/s, agent_0/loss=0.121, agent_1/loss=5.685, agent_2/loss=1.541, env_step=4375000, len=75, n/ep=80, n/st=6250, rew=-23.51]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #175: test_reward: -23.847809 ± 5.551110, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #176: 25001it [00:36, 689.69it/s, agent_0/loss=0.121, agent_1/loss=5.481, agent_2/loss=1.613, env_step=4400000, len=75, n/ep=80, n/st=6250, rew=-23.63]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  880\n",
      "Epoch #176: test_reward: -21.343905 ± 5.167458, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #177: 25001it [00:36, 680.02it/s, agent_0/loss=0.119, agent_1/loss=5.309, agent_2/loss=1.754, env_step=4425000, len=75, n/ep=90, n/st=6250, rew=-24.16]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #177: test_reward: -24.802887 ± 7.153807, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #178: 25001it [00:36, 679.31it/s, agent_0/loss=0.128, agent_1/loss=5.465, agent_2/loss=1.847, env_step=4450000, len=75, n/ep=80, n/st=6250, rew=-25.10]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  890\n",
      "Epoch #178: test_reward: -22.948554 ± 6.224735, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #179: 25001it [00:36, 679.42it/s, agent_0/loss=0.123, agent_1/loss=5.412, agent_2/loss=1.847, env_step=4475000, len=75, n/ep=80, n/st=6250, rew=-24.61]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #179: test_reward: -23.665119 ± 5.830498, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #180: 25001it [00:36, 678.98it/s, agent_0/loss=0.125, agent_1/loss=5.372, agent_2/loss=1.849, env_step=4500000, len=75, n/ep=90, n/st=6250, rew=-24.22]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  900\n",
      "Epoch #180: test_reward: -22.156512 ± 4.930647, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #181: 25001it [00:35, 709.15it/s, agent_0/loss=0.126, agent_1/loss=5.097, agent_2/loss=2.011, env_step=4525000, len=75, n/ep=80, n/st=6250, rew=-23.76]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #181: test_reward: -22.908979 ± 5.341692, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #182: 25001it [00:34, 720.48it/s, agent_0/loss=0.123, agent_1/loss=5.253, agent_2/loss=2.050, env_step=4550000, len=75, n/ep=80, n/st=6250, rew=-22.44]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  910\n",
      "Epoch #182: test_reward: -21.590033 ± 5.640022, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #183: 25001it [00:34, 726.00it/s, agent_0/loss=0.125, agent_1/loss=5.377, agent_2/loss=2.224, env_step=4575000, len=75, n/ep=90, n/st=6250, rew=-21.92]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #183: test_reward: -22.638071 ± 5.079784, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #184: 25001it [00:34, 718.80it/s, agent_0/loss=0.128, agent_1/loss=5.154, agent_2/loss=2.182, env_step=4600000, len=75, n/ep=80, n/st=6250, rew=-22.04]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  920\n",
      "Epoch #184: test_reward: -22.356080 ± 5.859852, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #185: 25001it [00:33, 743.47it/s, agent_0/loss=0.125, agent_1/loss=4.994, agent_2/loss=2.106, env_step=4625000, len=75, n/ep=80, n/st=6250, rew=-23.19]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #185: test_reward: -23.394141 ± 8.197020, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #186: 25001it [00:36, 677.20it/s, agent_0/loss=0.123, agent_1/loss=5.066, agent_2/loss=2.400, env_step=4650000, len=75, n/ep=90, n/st=6250, rew=-24.24]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  930\n",
      "Epoch #186: test_reward: -22.561078 ± 5.936533, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #187: 25001it [00:35, 708.58it/s, agent_0/loss=0.120, agent_1/loss=5.038, agent_2/loss=2.480, env_step=4675000, len=75, n/ep=80, n/st=6250, rew=-21.59]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #187: test_reward: -22.354056 ± 5.188515, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #188: 25001it [00:33, 753.24it/s, agent_0/loss=0.128, agent_1/loss=4.859, agent_2/loss=2.640, env_step=4700000, len=75, n/ep=80, n/st=6250, rew=-23.25]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  940\n",
      "Epoch #188: test_reward: -22.272031 ± 6.582320, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #189: 25001it [00:32, 761.78it/s, agent_0/loss=0.122, agent_1/loss=4.770, agent_2/loss=2.707, env_step=4725000, len=75, n/ep=90, n/st=6250, rew=-22.39]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #189: test_reward: -23.494139 ± 5.575799, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #190: 25001it [00:34, 714.63it/s, agent_0/loss=0.119, agent_1/loss=4.757, agent_2/loss=2.936, env_step=4750000, len=75, n/ep=80, n/st=6250, rew=-23.31]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  950\n",
      "Epoch #190: test_reward: -23.258517 ± 5.620782, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #191: 25001it [00:35, 701.74it/s, agent_0/loss=0.116, agent_1/loss=4.910, agent_2/loss=2.830, env_step=4775000, len=75, n/ep=80, n/st=6250, rew=-22.71]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #191: test_reward: -23.487002 ± 5.705182, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #192: 25001it [00:35, 708.93it/s, agent_0/loss=0.125, agent_1/loss=4.770, agent_2/loss=2.777, env_step=4800000, len=75, n/ep=90, n/st=6250, rew=-22.91]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  960\n",
      "Epoch #192: test_reward: -21.173844 ± 4.548594, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #193: 25001it [00:35, 698.90it/s, agent_0/loss=0.123, agent_1/loss=5.052, agent_2/loss=2.841, env_step=4825000, len=75, n/ep=80, n/st=6250, rew=-22.65]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #193: test_reward: -23.227503 ± 6.364078, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #194: 25001it [00:36, 691.10it/s, agent_0/loss=0.118, agent_1/loss=4.575, agent_2/loss=2.763, env_step=4850000, len=75, n/ep=80, n/st=6250, rew=-22.70]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  970\n",
      "Epoch #194: test_reward: -23.013107 ± 5.150618, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #195: 25001it [00:35, 698.75it/s, agent_0/loss=0.119, agent_1/loss=4.977, agent_2/loss=2.674, env_step=4875000, len=75, n/ep=90, n/st=6250, rew=-22.52]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #195: test_reward: -23.121834 ± 5.327790, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #196: 25001it [00:36, 692.41it/s, agent_0/loss=0.125, agent_1/loss=4.853, agent_2/loss=2.784, env_step=4900000, len=75, n/ep=80, n/st=6250, rew=-23.61]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  980\n",
      "Epoch #196: test_reward: -22.511536 ± 5.108747, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #197: 25001it [00:35, 698.48it/s, agent_0/loss=0.123, agent_1/loss=4.440, agent_2/loss=2.851, env_step=4925000, len=75, n/ep=80, n/st=6250, rew=-23.73]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #197: test_reward: -23.071806 ± 5.645987, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #198: 25001it [00:35, 696.19it/s, agent_0/loss=0.116, agent_1/loss=4.821, agent_2/loss=2.920, env_step=4950000, len=75, n/ep=90, n/st=6250, rew=-23.51]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  990\n",
      "Epoch #198: test_reward: -21.721226 ± 5.458633, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #199: 25001it [00:32, 758.14it/s, agent_0/loss=0.115, agent_1/loss=4.304, agent_2/loss=2.848, env_step=4975000, len=75, n/ep=80, n/st=6250, rew=-22.76]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #199: test_reward: -21.405403 ± 5.999276, best_reward: -19.561938 ± 5.183604 in #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #200: 25001it [00:35, 707.83it/s, agent_0/loss=0.120, agent_1/loss=5.008, agent_2/loss=2.681, env_step=5000000, len=75, n/ep=80, n/st=6250, rew=-22.40]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  1000\n",
      "Epoch #200: test_reward: -22.016923 ± 4.210466, best_reward: -19.561938 ± 5.183604 in #18\n",
      "\n",
      "==========Result==========\n",
      "{'duration': '7746.96s', 'train_time/model': '5057.65s', 'test_step': 753750, 'test_episode': 10050, 'test_time': '352.16s', 'test_speed': '2140.39 step/s', 'best_reward': -19.561938321409354, 'best_result': '-19.56 ± 5.18', 'train_step': 5000000, 'train_episode': 66660, 'train_time/collector': '2337.16s', 'train_speed': '676.15 step/s'}\n",
      "\n",
      "(the trained policy can be accessed via policy.policies[agents[0]])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "   \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()    \n",
    "\n",
    "    if False:\n",
    "        agents_buffers_training = {agent : \n",
    "                           PrioritizedVectorReplayBuffer( 300_000, \n",
    "                                                          len(train_envs), \n",
    "                                                          alpha=0.6, \n",
    "                                                          beta=0.4) \n",
    "                                                          for agent in agents\n",
    "                         }\n",
    "        agents_buffers_test = {agent : \n",
    "                           PrioritizedVectorReplayBuffer( 300_000, \n",
    "                                                          len(train_envs), \n",
    "                                                          alpha=0.6, \n",
    "                                                          beta=0.4) \n",
    "                                                          for agent in agents\n",
    "                         }\n",
    "    \n",
    "        # ======== Step 3: Collector setup =========\n",
    "        train_collector = CollectorMA.CollectorMA(\n",
    "            policy,\n",
    "            train_envs,\n",
    "            agents_buffers_training,                        \n",
    "            exploration_noise=True             \n",
    "        )\n",
    "        test_collector = CollectorMA.CollectorMA(policy, test_envs, agents_buffers_test, exploration_noise=True)\n",
    "\n",
    "    if True:\n",
    "         # ======== Step 3: Collector setup =========\n",
    "        train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        # VectorReplayBuffer(300_000, len(train_envs)),\n",
    "        PrioritizedVectorReplayBuffer( 300_000, len(train_envs), alpha=0.6, beta=0.4) , \n",
    "        #ListReplayBuffer(100000)       \n",
    "        # buffer = StateMemoryVectorReplayBuffer(\n",
    "        #         300_000,\n",
    "        #         len(train_envs),  # Assuming train_envs is your vectorized environment\n",
    "        #         memory_size=10,                \n",
    "        #     ),\n",
    "        exploration_noise=True             \n",
    "        )\n",
    "        test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "        \n",
    "    print(\"Buffer Warming Up \")    \n",
    "    for i in range(trainer_params[\"warmup_size\"]):#int(trainer_params['batch_size'] / (300 * 10 ) )):\n",
    "        \n",
    "        train_collector.collect(n_episode=train_env_num)#,random=True) #trainer_params['batch_size'] * train_env_num))\n",
    "        #train_collector.collect(n_step=300 * 10)\n",
    "        print(\".\", end=\"\") \n",
    "    \n",
    "    # len_buffer = len(train_collector.buffer) / (Spread_Config[\"max_cycles\"] * Spread_Config[\"N\"])\n",
    "    # print(\"\\nBuffer Lenght: \", len_buffer ) \n",
    "    \n",
    "    info = { \"Buffer\"  : \"PriorizedReplayBuffer\", \" Warmup_ep\" : runConfig[\"warmup_size\"]}\n",
    "    # ======== tensorboard logging setup =========                       \n",
    "    logger = WandbLogger(\n",
    "        train_interval = runConfig[\"max_cycles\"] * runConfig[\"N\"] ,\n",
    "        test_interval = 1,#runConfig[\"max_cycles\"] * runConfig[\"n_pursuers\"],\n",
    "        update_interval = runConfig[\"max_cycles\"],\n",
    "        save_interval = 1,\n",
    "        write_flush = True,\n",
    "        project = \"Spread_Eval01\",\n",
    "        name = log_name,\n",
    "        entity = None,\n",
    "        run_id = log_name,\n",
    "        config = runConfig,\n",
    "        monitor_gym = True )\n",
    "    \n",
    "    writer = SummaryWriter(log_path)    \n",
    "    writer.add_text(\"args\", str(runConfig))    \n",
    "    logger.load(writer)\n",
    "\n",
    "    \n",
    "    global_step_holder = [0] \n",
    "    \n",
    "    \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        \n",
    "        if Policy_Config[\"same_policy\"]:\n",
    "            torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_BestRew.pth\")\n",
    "            print(\"Best Saved Rew\" , str(global_step_holder[0]))\n",
    "        \n",
    "        else:\n",
    "            for n,agent in enumerate(agents):\n",
    "                torch.save(policy.policies[agent].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_\" + agent + \".pth\")\n",
    "            \n",
    "            print(\"Bests Saved Rew\" , str(global_step_holder[0]))\n",
    "        \n",
    "    def save_test_best_fn(policy):                \n",
    "        \n",
    "        if Policy_Config[\"same_policy\"]:\n",
    "            torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_BestLen.pth\")\n",
    "            print(\"Best Saved Length\" , str(global_step_holder[0]))\n",
    "        \n",
    "        else:\n",
    "            for n,agent in enumerate(agents):\n",
    "                torch.save(policy.policies[agent].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_\" + agent + \".pth\")\n",
    "            \n",
    "            print(\"Best Saved Length\" , str(global_step_holder[0]))\n",
    "        \n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 99999939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])          \n",
    "        if Policy_Config[\"same_policy\"]:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            for agent in agents:\n",
    "                policy.policies[agent].set_eps(epsilon)\n",
    "                \n",
    "        \n",
    "        # if env_step % 500 == 0:\n",
    "            # logger.write(\"train/env_step\", env_step, {\"train/eps\": eps})\n",
    "\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "               \n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        if Policy_Config[\"same_policy\"]:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:            \n",
    "            for agent in agents:                             \n",
    "                 policy.policies[agent].set_eps(epsilon)\n",
    "                \n",
    "        \n",
    "        if global_step_holder[0] % 10 == 0:\n",
    "            \n",
    "            if Policy_Config[\"same_policy\"]:\n",
    "                torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_Step.pth\")\n",
    "                print(\"Steps Policy Saved \" , str(global_step_holder[0]))\n",
    "            \n",
    "            else:\n",
    "                for n,agent in enumerate(agents):\n",
    "                    torch.save(policy.policies[agent].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_\" + agent + \"Step\" + str(global_step_holder[0]) + \".pth\")\n",
    "                \n",
    "                print(\"Steps Policy Saved \" , str(global_step_holder[0]))\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "                \n",
    "        global_step_holder[0] +=1 \n",
    "        # print(rews)\n",
    "        return rews\n",
    "\n",
    "\n",
    "    # # ======== Step 5: Run the trainer =========\n",
    "    offPolicyTrainer = OffpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,        \n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],        \n",
    "        episode_per_test= trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        # save_test_best_fn=save_test_best_fn,\n",
    "        update_per_step=trainer_params['update_per_step'],\n",
    "        logger=logger,\n",
    "        test_in_train=True,\n",
    "        reward_metric=reward_metric,\n",
    "        show_progress = True \n",
    "               \n",
    "        )\n",
    "    \n",
    "    result = offPolicyTrainer.run()\n",
    "    writer.close()\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
