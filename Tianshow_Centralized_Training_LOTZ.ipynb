{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import config\n",
    "import os\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "import json\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \".\\Tianshow_Centralized_Training\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy, RainbowPolicy\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from LOTZ.LOTZ_env import LeadingOnesTrailingZerosEnv\n",
    "\n",
    "# from TaskAllocation.RL_Policies.MultiHead_SISL import MultiHead_SISL\n",
    "from TaskAllocation.RL_Policies.DNN_LOTZ import DNN_LOTZ\n",
    "from TaskAllocation.RL_Policies.MultiHead_LOTZ import MultiHead_LOTZ\n",
    "from TaskAllocation.RL_Policies.ATT_LOTZ import ATT_LOTZ\n",
    "# from TaskAllocation.RL_Policies.CNN_SISL import CNN_SISL\n",
    "\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomCollector\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "\n",
    "#----------------------------------#\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from typing import Any\n",
    "from gymnasium import spaces\n",
    "\n",
    "def _reset(self, *args: Any, **kwargs: Any) -> tuple[dict, dict]:\n",
    "        self.env.reset(*args, **kwargs)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = self.env.last()\n",
    "\n",
    "        if isinstance(observation, dict) and \"action_mask\" in observation:\n",
    "            observation_dict = {\n",
    "                \"agent_id\": self.env.agent_selection,\n",
    "                \"obs\": observation[\"observation\"],\n",
    "                \"mask\": [obm == 1 for obm in observation[\"action_mask\"]],\n",
    "            }\n",
    "        else:\n",
    "            if isinstance(self.action_space, spaces.Discrete):\n",
    "                observation_dict = {\n",
    "                    \"agent_id\": self.env.agent_selection,\n",
    "                    \"obs\": observation,\n",
    "                    \"mask\": [True] * self.env.action_space(self.env.agent_selection).n,\n",
    "                }\n",
    "            else:\n",
    "                observation_dict = {\n",
    "                    \"agent_id\": self.env.agent_selection,\n",
    "                    \"obs\": observation,\n",
    "                }\n",
    "\n",
    "        return observation_dict, info\n",
    "    \n",
    "PettingZooEnv.reset = _reset\n",
    "    \n",
    "# --- Add specific modification to tianshou -----#\n",
    "import wandb\n",
    "from tianshou.utils import WandbLogger\n",
    "from tianshou.utils.logger.base import LOG_DATA_TYPE\n",
    "def new_write(self, step_type: str, step: int, data: LOG_DATA_TYPE) -> None:\n",
    "    data[step_type] = step\n",
    "    wandb.log(data)\n",
    "WandbLogger.write = new_write \n",
    "\n",
    "#from tianshou_DQN import train\n",
    "project = \"LOTZ_Eval\"\n",
    "model  =  \"MultiHead_LOTZ\" #\"MultiHead_SISL\" \n",
    "test_num  =  \"_NOV01\"\n",
    "policyModel  =  \"DQN\"\n",
    "\n",
    "train_env_num = 20\n",
    "test_env_num = 20\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "load_policy_name = f'policy_MultiHead_LOTZ_NOV01.pth'\n",
    "save_policy_name = f'policy_{name}'\n",
    "policy_path = \"policy_LOTZ\"\n",
    "\n",
    "same_policy = True\n",
    "load_model = False\n",
    "\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn_sisl\", log_name)\n",
    "\n",
    "LOTZ_Config = {\n",
    "    \"string_length\": 128,\n",
    "    \"n_agents\": 2,\n",
    "    \"seed\": 0,\n",
    "    \"m_steps\": 256,\n",
    "    \"sp\": 0\n",
    "}\n",
    "\n",
    "max_cycles = LOTZ_Config[\"m_steps\"]\n",
    "n_agents = 2\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.95, \n",
    "              \"estimation_step\": 3, \n",
    "              \"target_update_freq\": 100 * max_cycles,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 1e-3 }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 500,\n",
    "                  \"step_per_epoch\": 250 * max_cycles,\n",
    "                  \"step_per_collect\": max_cycles * 50,\n",
    "                  \"episode_per_test\": 20,\n",
    "                  \"batch_size\" : max_cycles * 10 ,\n",
    "                  \"update_per_step\": 1 / (max_cycles * 5), #Only run after close a Collect (run many times as necessary to meet the value)\n",
    "                  \"tn_eps_max\": 0.15,\n",
    "                  \"ts_eps_max\": 0.0,\n",
    "                  }\n",
    "\n",
    "runConfig = dqn_params\n",
    "runConfig.update(trainer_params) \n",
    "runConfig.update(LOTZ_Config)\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()       \n",
    "    agent_observation_space = env.observation_space\n",
    "   \n",
    "    action_shape = env.action_space\n",
    "\n",
    "    print(\"Action_Shape: \", action_shape)\n",
    "    print(\"agent_observation_space: \", agent_observation_space)\n",
    "    \n",
    "    device=\"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"          \n",
    "    \n",
    "    if agent_learn is None:      \n",
    "        \n",
    "        if model == \"MultiHead_LOTZ\":\n",
    "            net = MultiHead_LOTZ(\n",
    "                obs_shape=agent_observation_space,                \n",
    "                action_shape=action_shape, \n",
    "                max_len = LOTZ_Config[\"string_length\"],\n",
    "                device=device\n",
    "                \n",
    "            ).to(device)\n",
    "\n",
    "        if model == \"DNN_LOTZ\":\n",
    "            net = DNN_LOTZ(\n",
    "                obs_shape=agent_observation_space,                \n",
    "                action_shape=action_shape,                \n",
    "                device=device\n",
    "                \n",
    "            ).to(device)\n",
    "\n",
    "        if model == \"ATT_LOTZ\":\n",
    "            net = ATT_LOTZ(\n",
    "                obs_shape=agent_observation_space,                \n",
    "                action_shape=action_shape,                \n",
    "                device=device\n",
    "                \n",
    "            ).to(device)\n",
    "           \n",
    "\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= False )                \n",
    "    \n",
    "        if policyModel == \"DQN\":\n",
    "            agent_learn = DQNPolicy(\n",
    "                model=net,\n",
    "                optim=optim,\n",
    "                action_space = action_shape,\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "                reward_normalization = False,\n",
    "                clip_loss_grad = False \n",
    "            ) \n",
    "                     \n",
    "\n",
    "        if policyModel == \"Rainbow\":\n",
    "            agent_learn = RainbowPolicy(\n",
    "                model=net.to(device),\n",
    "                optim=optim,\n",
    "                action_space = action_shape,\n",
    "                num_atoms= 5,\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "            ) \n",
    "         \n",
    " \n",
    "        if load_model is True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "                   \n",
    "        #print(env.agents)\n",
    "        #agents = [agent_learn for _ in range(len(env.agents))]\n",
    "        \n",
    "        agents = [agent_learn for _ in range(len(env.agents))]\n",
    "\n",
    "        \n",
    "    policy = MultiAgentPolicyManager(policies = agents, env=env)  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "def _get_env():\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    # env_paralell = MultiUAVEnv()  \n",
    "    # env = pursuit_v4.env()\n",
    "    env = LeadingOnesTrailingZerosEnv(\n",
    "        string_length= LOTZ_Config[\"string_length\"], \n",
    "        n_agents=LOTZ_Config[\"n_agents\"], \n",
    "        seed=LOTZ_Config[\"seed\"], \n",
    "        m_steps = LOTZ_Config[\"m_steps\"], \n",
    "        sp = LOTZ_Config[\"sp\"] )\n",
    "    \n",
    "    #env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    # env = CustomParallelToAECWrapper(env_paralell)\n",
    "    \n",
    "    return PettingZooEnv(env)\n",
    "    # return env\n",
    "\n",
    "print(json.dumps(runConfig, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "   \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()    \n",
    "\n",
    "    agentsBuffer = PrioritizedVectorReplayBuffer( 300_000, len(train_envs), alpha=0.6, beta=0.4)  \n",
    "    # ======== Step 3: Collector setup =========\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        # VectorReplayBuffer(300_000, len(train_envs)),\n",
    "        agentsBuffer,\n",
    "        #ListReplayBuffer(100000)       \n",
    "        exploration_noise=True             \n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=False)\n",
    "     \n",
    "    print(\"Buffer Warming Up \")    \n",
    "    for i in range(1):#int(trainer_params['batch_size'] / (300 * 10 ) )):\n",
    "        \n",
    "        train_collector.collect(n_episode=train_env_num)#,random=True) #trainer_params['batch_size'] * train_env_num))\n",
    "        #train_collector.collect(n_step=300 * 10)\n",
    "        print(\".\", end=\"\") \n",
    "    \n",
    "    len_buffer = len(train_collector.buffer) #/ (SISL_Config[\"max_cycles\"] * SISL_Config[\"n_pursuers\"])\n",
    "    print(\"\\nBuffer Lenght: \", len_buffer ) \n",
    "    \n",
    "    info = { \"Buffer\"  : \"ReplayBuffer\", \" Warmup_ep\" : len_buffer}\n",
    "    # ======== tensorboard logging setup =========                       \n",
    "    logger = WandbLogger(\n",
    "        train_interval = runConfig[\"m_steps\"] * runConfig[\"n_agents\"] ,\n",
    "        test_interval = 1,#runConfig[\"max_cycles\"] * runConfig[\"n_pursuers\"],\n",
    "        update_interval = runConfig[\"m_steps\"],\n",
    "        save_interval = 1,\n",
    "        write_flush = True,\n",
    "        project = project,\n",
    "        name = log_name,\n",
    "        entity = None,\n",
    "        run_id = log_name,\n",
    "        config = runConfig,\n",
    "        monitor_gym = True )\n",
    "    \n",
    "    writer = SummaryWriter(log_path)    \n",
    "    writer.add_text(\"args\", str(runConfig))    \n",
    "    logger.load(writer)\n",
    "    \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        \n",
    "        torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \".pth\")\n",
    "        print(\"Best Saved\")\n",
    "        \n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 99999939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])          \n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "        \n",
    "        # if env_step % 500 == 0:\n",
    "            # logger.write(\"train/env_step\", env_step, {\"train/eps\": eps})\n",
    "\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        if same_policy:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            policy.policies['R_agent0'].set_eps(epsilon)\n",
    "            policy.policies['F_agent0'].set_eps(epsilon)\n",
    "\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "        #print(rews)\n",
    "        return rews#[:, 1]\n",
    "\n",
    "    # # ======== Step 5: Run the trainer =========\n",
    "    offPolicyTrainer = OffpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,  \n",
    "        buffer= agentsBuffer,      \n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],        \n",
    "        episode_per_test= trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=trainer_params['update_per_step'],\n",
    "        logger=logger,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric,\n",
    "        show_progress = True \n",
    "               \n",
    "        )\n",
    "    \n",
    "    result = offPolicyTrainer.run()\n",
    "    writer.close()\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"MultiHead_LOTZ\"\n",
    "policyModel = \"DQN\"\n",
    "# Create a new instance of the policy with the same architecture as the saved policy\n",
    "name = 'policy_MultiHead_LOTZ_NOV01.pth' \n",
    "\n",
    "# policy, optim, _ = get_agents()\n",
    "# model_load_path = os.path.join(\"policy_LOTZ\", name)        \n",
    "\n",
    "# Load the saved checkpoint\n",
    "policy_test = policy.policies['agent0']\n",
    "#policy_test.load_state_dict(torch.load(model_load_path ))\n",
    "\n",
    "envs = DummyVectorEnv([_get_env for _ in range(1)])\n",
    "#policy_test.eval()\n",
    "policy_test.set_eps(0.00)\n",
    "\n",
    "#collector = CustomCollector(policy.policies['agent0'], envs, exploration_noise=True)\n",
    "#collector = CustomCollector(policy_test, envs, exploration_noise=False)\n",
    "collector = CustomCollector(policy, envs, exploration_noise=False)\n",
    "\n",
    "#results = collector.collect(n_episode=1)\n",
    "results = collector.collect(n_episode=10)# render=0.01,)#, gym_reset_kwargs={'seed' :2})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['rews']\n",
    "print(np.mean(results['rews'][results['rews'] > -10]))\n",
    "\n",
    "\n",
    "#create a function  to print a histogram of the results['rews']\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(results['rews'][results['rews'] > -10], bins=100)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
