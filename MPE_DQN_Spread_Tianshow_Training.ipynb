{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import utils\n",
    "import os\n",
    "import datetime\n",
    "from typing import Optional, Tuple\n",
    "import json\n",
    "\n",
    "\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"Tianshow_Centralized_Training\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tianshou.data import Collector, VectorReplayBuffer, PrioritizedVectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "# from tianshou.env.pettingzoo_env_parallel import PettingZooParallelEnv\n",
    "\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy, RainbowPolicy\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from pettingzoo.sisl import pursuit_v4\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "import Mods.TaskSpreadEnv as TaskSpreadEnv\n",
    "\n",
    "from TaskAllocation.RL_Policies.DNN_Spread import DNN_Spread\n",
    "from TaskAllocation.RL_Policies.MPE_Task_MultiHead import MPE_Task_MultiHead\n",
    "\n",
    "#import Mods.TaskPursuitEnv as TaskPursuitEnv\n",
    "import Mods.ActionLoggerWrapper as ActionLoggerWrapper\n",
    "# import Mods.VDNPolicy as VDNPolicy\n",
    "# import Mods.PettingZooParallelEnv2 as PettingZooParallelEnv2\n",
    "# import Mods.CollectorMA as CollectorMA\n",
    "\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomNet\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomCollector\n",
    "from TaskAllocation.RL_Policies.Custom_Classes import CustomParallelToAECWrapper\n",
    "\n",
    "# Add specific modification to tianshou\n",
    "import wandb\n",
    "from tianshou.utils import WandbLogger\n",
    "from tianshou.utils.logger.base import LOG_DATA_TYPE\n",
    "\n",
    "def new_write(self, step_type: str, step: int, data: LOG_DATA_TYPE) -> None:\n",
    "    data[step_type] = step\n",
    "    wandb.log(data)\n",
    "    \n",
    "WandbLogger.write = new_write \n",
    "\n",
    "from pettingzoo.utils import wrappers\n",
    "import gym\n",
    "\n",
    "class ActionLoggerWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ActionLoggerWrapper, self).__init__(env)\n",
    "        self.actions = []\n",
    "\n",
    "    def step(self, action):\n",
    "        self.actions.append(action)\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self, **kwargs):      \n",
    "        if self.actions:\n",
    "            # Convert all actions to numpy arrays and standardize their shapes\n",
    "            formatted_actions = [np.array(a).flatten() for a in self.actions]\n",
    "            flattened_actions = np.concatenate(formatted_actions)\n",
    "\n",
    "            try:\n",
    "                # Compute the histogram\n",
    "                hist_data, bin_edges = np.histogram(flattened_actions, bins='auto')\n",
    "\n",
    "                # Log the actions as a histogram to wandb\n",
    "                wandb.log({\"actions_histogram\": wandb.Histogram(np_histogram=(hist_data, bin_edges))})\n",
    "            except Exception as e:\n",
    "                pass#print(\"Error in logging histogram:\", e)\n",
    "\n",
    "            self.actions = []\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "#from tianshou_DQN import train\n",
    "model  =  \"MPE_Task_MultiHead\" #\"DNN_Spread\"#\"MPE_Task_MultiHead\" # #\"CNN_ATT_SISL\" #\"MultiHead_SISL\" \n",
    "test_num  =  \"_Desk_01_4feat_land_alli\"\n",
    "policyModel  =  \"DQN\"\n",
    "\n",
    "train_env_num = 10\n",
    "test_env_num = 10\n",
    "\n",
    "name = model + test_num\n",
    "\n",
    "# log\n",
    "now = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "log_name = name + str(now)\n",
    "log_path = os.path.join('./', \"Logs\", \"dqn_sisl\", log_name)\n",
    "\n",
    "#policy\n",
    "load_policy_name = f'policy_MPE_Task_MultiHead_Desk_01_8feat240112-145703_29_BestRew.pth'\n",
    "save_policy_name = f'policy_{log_name}'\n",
    "policy_path = \"dqn_Spread\"\n",
    "\n",
    "Policy_Config = {\n",
    "    \"same_policy\" : True,\n",
    "    \"load_model\" : False,\n",
    "    \"freeze_CNN\" : False     \n",
    "                }\n",
    "\n",
    "Spread_Config = {\n",
    "    \"N\": 3,                      # Default = 3\n",
    "    \"local_ratio\": 0.5,          # Default = 0.5\n",
    "    \"max_cycles\": 25,            # Default = 25\n",
    "    \"continuous_actions\": False, # Default = False\n",
    "    \"render_mode\": None          # Default = None \n",
    "}\n",
    "\n",
    "max_cycles = Spread_Config[\"max_cycles\"]\n",
    "n_agents = Spread_Config[\"N\"]\n",
    "\n",
    "dqn_params = {\"discount_factor\": 0.99, \n",
    "              \"estimation_step\": 3, \n",
    "              \"target_update_freq\": 2400, #* max_cycles,\n",
    "              \"optminizer\": \"Adam\",\n",
    "              \"lr\": 0.00001 }\n",
    "\n",
    "trainer_params = {\"max_epoch\": 200,\n",
    "                  \"step_per_epoch\": 20000,#1000 * max_cycles,\n",
    "                  \"step_per_collect\": 800,#20 * max_cycles,\n",
    "                  \"episode_per_test\": 50,\n",
    "                  \"batch_size\" :  1024,\n",
    "                  \"update_per_step\": 0.0125,#1 / 75, #Only run after close a Collect (run many times as necessary to meet the value)\n",
    "                  \"tn_eps_max\": 0.15,\n",
    "                  \"ts_eps_max\": 0.0,\n",
    "                  \"warmup_size\" : 10\n",
    "                  }\n",
    "\n",
    "\n",
    "runConfig = dqn_params\n",
    "runConfig.update(Policy_Config)\n",
    "runConfig.update(trainer_params) \n",
    "runConfig.update(Spread_Config)\n",
    "\n",
    "model_load_path = os.path.join(policy_path, load_policy_name)  \n",
    "model_save_path = os.path.join(policy_path, save_policy_name)        \n",
    "os.makedirs(os.path.join(policy_path), exist_ok=True)  \n",
    "os.makedirs(os.path.join(log_path), exist_ok=True)\n",
    "\n",
    "def _get_agents(\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    "    policy_load_path = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    \n",
    "    env = _get_env()       \n",
    "    agent_observation_space = env.observation_space.shape\n",
    "   \n",
    "    action_shape = env.action_space\n",
    "    \n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  \n",
    "\n",
    "    agents = []        \n",
    "    \n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        policies_number = 1\n",
    "    else:\n",
    "        policies_number = 3#len(env.agents)\n",
    "\n",
    "    for _ in range(policies_number):                   \n",
    "\n",
    "        if model == \"DNN_Spread\":\n",
    "            net = DNN_Spread(\n",
    "                obs_shape=agent_observation_space[0],                \n",
    "                action_shape=5,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                \n",
    "            ).to(device)\n",
    "\n",
    "        if model == \"VDN_Spread\":\n",
    "            net = DNN_Spread(\n",
    "                obs_shape=agent_observation_space[0],                \n",
    "                action_shape=5,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                \n",
    "            ).to(device)\n",
    "\n",
    "        if model == \"MPE_Task_MultiHead\":\n",
    "            net = MPE_Task_MultiHead(                \n",
    "                num_tasks=Spread_Config['N'] * 2 + 5,\n",
    "                num_features_per_task = 8,#6 + 2 + 1,                \n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "                \n",
    "            ).to(device)\n",
    "\n",
    "        optim = torch.optim.Adam(net.parameters(), lr=dqn_params[\"lr\"], weight_decay=0.0, amsgrad= True )                \n",
    "\n",
    "        if policyModel == \"DQN\":\n",
    "            agent_learn = DQNPolicy(\n",
    "                model=net,\n",
    "                optim=optim,\n",
    "                action_space = action_shape,\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "                reward_normalization = False,\n",
    "                clip_loss_grad = False \n",
    "            ) \n",
    "        \n",
    "        if policyModel == \"VDN\":\n",
    "            agent_learn = DQNPolicy(\n",
    "                model=net,\n",
    "                optim=optim,\n",
    "                action_space = action_shape,\n",
    "                discount_factor= dqn_params[\"discount_factor\"],\n",
    "                estimation_step=dqn_params[\"estimation_step\"],\n",
    "                target_update_freq=dqn_params[\"target_update_freq\"],\n",
    "                reward_normalization = False,\n",
    "                clip_loss_grad = False,                \n",
    "            ) \n",
    "\n",
    "        if Policy_Config[\"load_model\"] is True:\n",
    "            # Load the saved checkpoint             \n",
    "            agent_learn.load_state_dict(torch.load(model_load_path))\n",
    "            print(f'Loaded-> {model_load_path}')\n",
    "                   \n",
    "        #print(env.agents)\n",
    "        #agents = [agent_learn for _ in range(len(env.agents))]\n",
    "        \n",
    "        agents.append(agent_learn)\n",
    "\n",
    "    if Policy_Config[\"same_policy\"]:\n",
    "        agents = [agents[0] for _ in range(len(env.agents))]\n",
    "    else:\n",
    "        for _ in range(len(env.agents) - policies_number):\n",
    "            agents.append(agents[0])\n",
    "\n",
    "    # policy = VDNPolicy.VDNMAPolicy(policies = agents, env=env, device=\"cuda\" if torch.cuda.is_available() else \"cpu\" )  \n",
    "    policy = MultiAgentPolicyManager(policies = agents, env=env )  \n",
    "        \n",
    "    return policy, optim, env.agents\n",
    "\n",
    "def _get_env(test=False):\n",
    "    \"\"\"This function is needed to provide callables for DummyVectorEnv.\"\"\"   \n",
    "    # env_paralell = MultiUAVEnv()  \n",
    "    #env = pursuit_v4.env()    \n",
    "    env = TaskSpreadEnv.env(\n",
    "    # env = simple_spread_v3.parallel_env(\n",
    "    # env = simple_spread_v3.env(\n",
    "        max_cycles=Spread_Config[\"max_cycles\"],\n",
    "        local_ratio=Spread_Config[\"local_ratio\"],\n",
    "        N=Spread_Config[\"N\"],\n",
    "        continuous_actions=Spread_Config[\"continuous_actions\"],\n",
    "        render_mode=\" human\" #Spread_Config[\"render_mode\"]\n",
    "    )    \n",
    "    \n",
    "    # env = parallel_to_aec_wrapper(env_paralell)    \n",
    "    # env = CustomParallelToAECWrapper(env_paralell)\n",
    "    # env = ActionLoggerWrapper(env)\n",
    "    env = PettingZooEnv(env) \n",
    "    # env = PettingZooParallelEnv(env)\n",
    "       \n",
    "    return  env\n",
    "\n",
    "# print(json.dumps(runConfig, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer Warming Up \n",
      "........."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find Tianshow_Centralized_Training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mandrekuros\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "c:\\Python310\\lib\\site-packages\\wandb\\sdk\\lib\\ipython.py:77: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\GITHUB\\Multi-UAV-TA-gym-env\\wandb\\run-20240124_205230-MPE_Task_MultiHead_Desk_01_4feat_land_alli240124-205209</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/andrekuros/Spread_Eval01/runs/MPE_Task_MultiHead_Desk_01_4feat_land_alli240124-205209' target=\"_blank\">MPE_Task_MultiHead_Desk_01_4feat_land_alli240124-205209</a></strong> to <a href='https://wandb.ai/andrekuros/Spread_Eval01' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/andrekuros/Spread_Eval01' target=\"_blank\">https://wandb.ai/andrekuros/Spread_Eval01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/andrekuros/Spread_Eval01/runs/MPE_Task_MultiHead_Desk_01_4feat_land_alli240124-205209' target=\"_blank\">https://wandb.ai/andrekuros/Spread_Eval01/runs/MPE_Task_MultiHead_Desk_01_4feat_land_alli240124-205209</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  0\n",
      "Best Saved Rew 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 20001it [00:37, 531.27it/s, agent_0/loss=0.259, agent_1/loss=0.271, agent_2/loss=0.264, env_step=20000, len=75, n/ep=10, n/st=800, rew=-16.28]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved Rew 27\n",
      "Epoch #1: test_reward: -18.851344 ± 7.151752, best_reward: -18.851344 ± 7.151752 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 20001it [00:36, 551.23it/s, agent_0/loss=0.135, agent_1/loss=0.140, agent_2/loss=0.141, env_step=40000, len=75, n/ep=10, n/st=800, rew=-19.20]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -23.535522 ± 8.176556, best_reward: -18.851344 ± 7.151752 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 20001it [00:34, 576.14it/s, agent_0/loss=0.156, agent_1/loss=0.160, agent_2/loss=0.160, env_step=60000, len=75, n/ep=20, n/st=800, rew=-20.06]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved Rew 79\n",
      "Epoch #3: test_reward: -17.444675 ± 7.063890, best_reward: -17.444675 ± 7.063890 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 20001it [00:34, 574.44it/s, agent_0/loss=0.165, agent_1/loss=0.165, agent_2/loss=0.171, env_step=80000, len=75, n/ep=10, n/st=800, rew=-20.97]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -19.836096 ± 7.138915, best_reward: -17.444675 ± 7.063890 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 20001it [00:34, 578.14it/s, agent_0/loss=0.160, agent_1/loss=0.167, agent_2/loss=0.164, env_step=100000, len=75, n/ep=10, n/st=800, rew=-21.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  130\n",
      "Best Saved Rew 131\n",
      "Epoch #5: test_reward: -17.360558 ± 5.491127, best_reward: -17.360558 ± 5.491127 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 20001it [00:35, 568.57it/s, agent_0/loss=0.146, agent_1/loss=0.147, agent_2/loss=0.153, env_step=120000, len=75, n/ep=20, n/st=800, rew=-17.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: -18.490801 ± 5.972650, best_reward: -17.360558 ± 5.491127 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 20001it [00:34, 574.33it/s, agent_0/loss=0.143, agent_1/loss=0.149, agent_2/loss=0.150, env_step=140000, len=75, n/ep=10, n/st=800, rew=-17.77]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved Rew 183\n",
      "Epoch #7: test_reward: -17.147465 ± 7.686320, best_reward: -17.147465 ± 7.686320 in #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 20001it [00:35, 556.92it/s, agent_0/loss=0.107, agent_1/loss=0.113, agent_2/loss=0.115, env_step=160000, len=75, n/ep=10, n/st=800, rew=-18.75]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved Rew 209\n",
      "Epoch #8: test_reward: -16.940000 ± 7.225419, best_reward: -16.940000 ± 7.225419 in #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 20001it [00:34, 573.79it/s, agent_0/loss=0.108, agent_1/loss=0.114, agent_2/loss=0.116, env_step=180000, len=75, n/ep=20, n/st=800, rew=-14.61]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: -18.958657 ± 7.317113, best_reward: -16.940000 ± 7.225419 in #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 20001it [00:34, 576.10it/s, agent_0/loss=0.143, agent_1/loss=0.151, agent_2/loss=0.155, env_step=200000, len=75, n/ep=10, n/st=800, rew=-20.63]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  260\n",
      "Epoch #10: test_reward: -17.918890 ± 8.159875, best_reward: -16.940000 ± 7.225419 in #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #11: 20001it [00:38, 513.82it/s, agent_0/loss=0.115, agent_1/loss=0.117, agent_2/loss=0.124, env_step=220000, len=75, n/ep=10, n/st=800, rew=-15.20]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved Rew 287\n",
      "Epoch #11: test_reward: -15.156786 ± 6.711757, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #12: 20001it [00:35, 568.82it/s, agent_0/loss=0.117, agent_1/loss=0.124, agent_2/loss=0.128, env_step=240000, len=75, n/ep=20, n/st=800, rew=-16.70]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #12: test_reward: -17.662780 ± 8.321141, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #13: 20001it [00:35, 570.75it/s, agent_0/loss=0.141, agent_1/loss=0.156, agent_2/loss=0.158, env_step=260000, len=75, n/ep=10, n/st=800, rew=-21.10]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #13: test_reward: -17.120500 ± 8.186881, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #14: 20001it [00:35, 567.66it/s, agent_0/loss=0.129, agent_1/loss=0.154, agent_2/loss=0.154, env_step=280000, len=75, n/ep=10, n/st=800, rew=-19.32]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #14: test_reward: -18.598816 ± 9.579704, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #15: 20001it [00:35, 557.57it/s, agent_0/loss=0.129, agent_1/loss=0.152, agent_2/loss=0.154, env_step=300000, len=75, n/ep=20, n/st=800, rew=-18.29]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  390\n",
      "Epoch #15: test_reward: -18.526315 ± 7.616661, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #16: 20001it [00:35, 560.49it/s, agent_0/loss=0.126, agent_1/loss=0.142, agent_2/loss=0.147, env_step=320000, len=75, n/ep=10, n/st=800, rew=-14.96]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #16: test_reward: -18.597491 ± 10.846791, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #17: 20001it [00:34, 579.09it/s, agent_0/loss=0.143, agent_1/loss=0.168, agent_2/loss=0.170, env_step=340000, len=75, n/ep=10, n/st=800, rew=-24.06]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #17: test_reward: -17.577299 ± 7.701587, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #18: 20001it [00:34, 578.32it/s, agent_0/loss=0.124, agent_1/loss=0.164, agent_2/loss=0.156, env_step=360000, len=75, n/ep=20, n/st=800, rew=-17.59]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #18: test_reward: -17.382879 ± 8.189764, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #19: 20001it [00:34, 578.41it/s, agent_0/loss=0.117, agent_1/loss=0.148, agent_2/loss=0.156, env_step=380000, len=75, n/ep=10, n/st=800, rew=-15.40]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #19: test_reward: -19.563680 ± 7.581879, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #20: 20001it [00:34, 579.74it/s, agent_0/loss=0.137, agent_1/loss=0.170, agent_2/loss=0.188, env_step=400000, len=75, n/ep=10, n/st=800, rew=-20.37]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  520\n",
      "Epoch #20: test_reward: -18.892167 ± 8.200212, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #21: 20001it [00:34, 575.12it/s, agent_0/loss=0.138, agent_1/loss=0.184, agent_2/loss=0.190, env_step=420000, len=75, n/ep=20, n/st=800, rew=-19.53]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #21: test_reward: -18.989800 ± 6.713043, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #22: 20001it [00:34, 573.58it/s, agent_0/loss=0.125, agent_1/loss=0.173, agent_2/loss=0.168, env_step=440000, len=75, n/ep=10, n/st=800, rew=-18.62]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #22: test_reward: -16.510295 ± 7.361907, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #23: 20001it [00:35, 562.30it/s, agent_0/loss=0.142, agent_1/loss=0.180, agent_2/loss=0.185, env_step=460000, len=75, n/ep=10, n/st=800, rew=-17.73]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #23: test_reward: -15.326543 ± 5.879495, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #24: 20001it [00:34, 577.68it/s, agent_0/loss=0.137, agent_1/loss=0.174, agent_2/loss=0.182, env_step=480000, len=75, n/ep=20, n/st=800, rew=-15.60]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #24: test_reward: -16.227349 ± 8.288442, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #25: 20001it [00:34, 572.71it/s, agent_0/loss=0.128, agent_1/loss=0.170, agent_2/loss=0.168, env_step=500000, len=75, n/ep=10, n/st=800, rew=-17.06]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  650\n",
      "Epoch #25: test_reward: -17.463149 ± 9.819872, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #26: 20001it [00:34, 577.89it/s, agent_0/loss=0.165, agent_1/loss=0.222, agent_2/loss=0.211, env_step=520000, len=75, n/ep=10, n/st=800, rew=-14.63]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #26: test_reward: -15.597110 ± 6.576033, best_reward: -15.156786 ± 6.711757 in #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #27: 20001it [00:34, 579.52it/s, agent_0/loss=0.140, agent_1/loss=0.191, agent_2/loss=0.187, env_step=540000, len=75, n/ep=20, n/st=800, rew=-13.50]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved Rew 703\n",
      "Epoch #27: test_reward: -12.611545 ± 6.474710, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #28: 20001it [00:34, 574.25it/s, agent_0/loss=0.142, agent_1/loss=0.198, agent_2/loss=0.203, env_step=560000, len=75, n/ep=10, n/st=800, rew=-13.54]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #28: test_reward: -14.277900 ± 7.311873, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #29: 20001it [00:34, 578.19it/s, agent_0/loss=0.168, agent_1/loss=0.234, agent_2/loss=0.240, env_step=580000, len=75, n/ep=10, n/st=800, rew=-15.36]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #29: test_reward: -15.086500 ± 8.043775, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #30: 20001it [00:34, 576.20it/s, agent_0/loss=0.163, agent_1/loss=0.234, agent_2/loss=0.228, env_step=600000, len=75, n/ep=20, n/st=800, rew=-15.51]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  780\n",
      "Epoch #30: test_reward: -12.884762 ± 5.883568, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #31: 20001it [00:34, 584.18it/s, agent_0/loss=0.160, agent_1/loss=0.215, agent_2/loss=0.217, env_step=620000, len=75, n/ep=10, n/st=800, rew=-15.43]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #31: test_reward: -16.853491 ± 8.635331, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #32: 20001it [00:34, 583.17it/s, agent_0/loss=0.156, agent_1/loss=0.212, agent_2/loss=0.213, env_step=640000, len=75, n/ep=10, n/st=800, rew=-21.21]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #32: test_reward: -15.216094 ± 7.270135, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #33: 20001it [00:34, 583.23it/s, agent_0/loss=0.168, agent_1/loss=0.238, agent_2/loss=0.231, env_step=660000, len=75, n/ep=20, n/st=800, rew=-14.86]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #33: test_reward: -16.351363 ± 8.226359, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #34: 20001it [00:33, 591.79it/s, agent_0/loss=0.148, agent_1/loss=0.228, agent_2/loss=0.227, env_step=680000, len=75, n/ep=10, n/st=800, rew=-14.02]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #34: test_reward: -13.404053 ± 8.663321, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #35: 20001it [00:34, 585.25it/s, agent_0/loss=0.140, agent_1/loss=0.201, agent_2/loss=0.216, env_step=700000, len=75, n/ep=10, n/st=800, rew=-13.23]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  910\n",
      "Epoch #35: test_reward: -15.330479 ± 7.854981, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #36: 20001it [00:34, 587.36it/s, agent_0/loss=0.171, agent_1/loss=0.241, agent_2/loss=0.243, env_step=720000, len=75, n/ep=20, n/st=800, rew=-15.86]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #36: test_reward: -14.901518 ± 7.355478, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #37: 20001it [00:34, 585.35it/s, agent_0/loss=0.179, agent_1/loss=0.223, agent_2/loss=0.237, env_step=740000, len=75, n/ep=10, n/st=800, rew=-14.75]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #37: test_reward: -14.567665 ± 7.925979, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #38: 20001it [00:34, 581.54it/s, agent_0/loss=0.161, agent_1/loss=0.205, agent_2/loss=0.228, env_step=760000, len=75, n/ep=10, n/st=800, rew=-14.96]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #38: test_reward: -12.825742 ± 7.057582, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #39: 20001it [00:33, 588.83it/s, agent_0/loss=0.209, agent_1/loss=0.277, agent_2/loss=0.282, env_step=780000, len=75, n/ep=20, n/st=800, rew=-13.49]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #39: test_reward: -13.748420 ± 6.252888, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #40: 20001it [00:34, 579.34it/s, agent_0/loss=0.190, agent_1/loss=0.251, agent_2/loss=0.259, env_step=800000, len=75, n/ep=10, n/st=800, rew=-14.85]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  1040\n",
      "Epoch #40: test_reward: -15.414245 ± 10.057166, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #41: 20001it [00:34, 582.98it/s, agent_0/loss=0.177, agent_1/loss=0.249, agent_2/loss=0.251, env_step=820000, len=75, n/ep=10, n/st=800, rew=-14.53]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #41: test_reward: -13.092808 ± 6.957101, best_reward: -12.611545 ± 6.474710 in #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #42: 20001it [00:34, 586.29it/s, agent_0/loss=0.203, agent_1/loss=0.289, agent_2/loss=0.282, env_step=840000, len=75, n/ep=20, n/st=800, rew=-12.93]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved Rew 1093\n",
      "Epoch #42: test_reward: -11.043515 ± 5.017869, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #43: 20001it [00:33, 591.35it/s, agent_0/loss=0.190, agent_1/loss=0.270, agent_2/loss=0.280, env_step=860000, len=75, n/ep=10, n/st=800, rew=-11.09]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #43: test_reward: -13.043082 ± 6.166365, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #44: 20001it [00:34, 579.47it/s, agent_0/loss=0.187, agent_1/loss=0.254, agent_2/loss=0.275, env_step=880000, len=75, n/ep=10, n/st=800, rew=-16.35]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #44: test_reward: -13.551426 ± 7.014577, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #45: 20001it [00:34, 585.50it/s, agent_0/loss=0.208, agent_1/loss=0.285, agent_2/loss=0.290, env_step=900000, len=75, n/ep=20, n/st=800, rew=-14.46]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  1170\n",
      "Epoch #45: test_reward: -16.762025 ± 7.104546, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #46: 20001it [00:34, 585.89it/s, agent_0/loss=0.207, agent_1/loss=0.291, agent_2/loss=0.278, env_step=920000, len=75, n/ep=10, n/st=800, rew=-15.12]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #46: test_reward: -13.689791 ± 6.640342, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #47: 20001it [00:34, 583.61it/s, agent_0/loss=0.202, agent_1/loss=0.293, agent_2/loss=0.285, env_step=940000, len=75, n/ep=10, n/st=800, rew=-13.73]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #47: test_reward: -14.328737 ± 6.985687, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #48: 20001it [00:34, 586.48it/s, agent_0/loss=0.196, agent_1/loss=0.303, agent_2/loss=0.286, env_step=960000, len=75, n/ep=20, n/st=800, rew=-15.12]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #48: test_reward: -13.199630 ± 6.175133, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #49: 20001it [00:34, 578.98it/s, agent_0/loss=0.221, agent_1/loss=0.318, agent_2/loss=0.316, env_step=980000, len=75, n/ep=10, n/st=800, rew=-17.98]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #49: test_reward: -15.764013 ± 6.969645, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #50: 20001it [00:34, 585.31it/s, agent_0/loss=0.211, agent_1/loss=0.301, agent_2/loss=0.307, env_step=1000000, len=75, n/ep=10, n/st=800, rew=-16.71]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  1300\n",
      "Epoch #50: test_reward: -13.316969 ± 6.643016, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #51: 20001it [00:34, 587.83it/s, agent_0/loss=0.220, agent_1/loss=0.305, agent_2/loss=0.304, env_step=1020000, len=75, n/ep=20, n/st=800, rew=-14.78]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #51: test_reward: -13.986399 ± 6.091827, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #52: 20001it [00:34, 584.94it/s, agent_0/loss=0.237, agent_1/loss=0.342, agent_2/loss=0.344, env_step=1040000, len=75, n/ep=10, n/st=800, rew=-14.50]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #52: test_reward: -13.206723 ± 6.875438, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #53: 20001it [00:34, 586.10it/s, agent_0/loss=0.237, agent_1/loss=0.314, agent_2/loss=0.319, env_step=1060000, len=75, n/ep=10, n/st=800, rew=-15.47]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #53: test_reward: -16.096437 ± 9.024568, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #54: 20001it [00:34, 583.16it/s, agent_0/loss=0.234, agent_1/loss=0.329, agent_2/loss=0.340, env_step=1080000, len=75, n/ep=20, n/st=800, rew=-16.06]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #54: test_reward: -14.127627 ± 6.931253, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #55: 20001it [00:35, 564.22it/s, agent_0/loss=0.264, agent_1/loss=0.373, agent_2/loss=0.377, env_step=1100000, len=75, n/ep=10, n/st=800, rew=-14.55]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  1430\n",
      "Epoch #55: test_reward: -12.420759 ± 5.118742, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #56: 20001it [00:36, 552.67it/s, agent_0/loss=0.252, agent_1/loss=0.384, agent_2/loss=0.386, env_step=1120000, len=75, n/ep=10, n/st=800, rew=-12.44]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #56: test_reward: -16.496498 ± 8.534962, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #57: 20001it [00:37, 538.25it/s, agent_0/loss=0.247, agent_1/loss=0.359, agent_2/loss=0.374, env_step=1140000, len=75, n/ep=20, n/st=800, rew=-13.23]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #57: test_reward: -12.580334 ± 7.167077, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #58: 20001it [00:34, 578.55it/s, agent_0/loss=0.268, agent_1/loss=0.406, agent_2/loss=0.384, env_step=1160000, len=75, n/ep=10, n/st=800, rew=-10.44]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #58: test_reward: -11.322626 ± 5.939105, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #59: 20001it [00:34, 580.65it/s, agent_0/loss=0.264, agent_1/loss=0.354, agent_2/loss=0.344, env_step=1180000, len=75, n/ep=10, n/st=800, rew=-15.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #59: test_reward: -12.887982 ± 7.731156, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #60: 20001it [00:34, 572.23it/s, agent_0/loss=0.265, agent_1/loss=0.377, agent_2/loss=0.367, env_step=1200000, len=75, n/ep=20, n/st=800, rew=-13.15]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  1560\n",
      "Epoch #60: test_reward: -14.635018 ± 7.950731, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #61: 20001it [00:34, 581.01it/s, agent_0/loss=0.289, agent_1/loss=0.376, agent_2/loss=0.378, env_step=1220000, len=75, n/ep=10, n/st=800, rew=-12.50]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #61: test_reward: -13.767555 ± 6.603706, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #62: 20001it [00:34, 572.83it/s, agent_0/loss=0.296, agent_1/loss=0.398, agent_2/loss=0.414, env_step=1240000, len=75, n/ep=10, n/st=800, rew=-13.21]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #62: test_reward: -13.518346 ± 6.330955, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #63: 20001it [00:34, 575.68it/s, agent_0/loss=0.268, agent_1/loss=0.385, agent_2/loss=0.399, env_step=1260000, len=75, n/ep=20, n/st=800, rew=-13.59]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #63: test_reward: -13.100167 ± 6.763961, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #64: 20001it [00:35, 563.29it/s, agent_0/loss=0.271, agent_1/loss=0.372, agent_2/loss=0.386, env_step=1280000, len=75, n/ep=10, n/st=800, rew=-12.68]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #64: test_reward: -12.554373 ± 6.169741, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #65: 20001it [00:34, 572.88it/s, agent_0/loss=0.311, agent_1/loss=0.409, agent_2/loss=0.431, env_step=1300000, len=75, n/ep=10, n/st=800, rew=-12.38]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  1690\n",
      "Epoch #65: test_reward: -13.805037 ± 7.867755, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #66: 20001it [00:34, 579.46it/s, agent_0/loss=0.298, agent_1/loss=0.427, agent_2/loss=0.410, env_step=1320000, len=75, n/ep=20, n/st=800, rew=-15.57]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #66: test_reward: -13.270596 ± 6.317469, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #67: 20001it [00:34, 578.21it/s, agent_0/loss=0.275, agent_1/loss=0.387, agent_2/loss=0.403, env_step=1340000, len=75, n/ep=10, n/st=800, rew=-14.89]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #67: test_reward: -13.578004 ± 7.348245, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #68: 20001it [00:35, 571.34it/s, agent_0/loss=0.304, agent_1/loss=0.429, agent_2/loss=0.440, env_step=1360000, len=75, n/ep=10, n/st=800, rew=-13.26]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #68: test_reward: -13.123690 ± 5.494127, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #69: 20001it [00:35, 570.80it/s, agent_0/loss=0.289, agent_1/loss=0.409, agent_2/loss=0.427, env_step=1380000, len=75, n/ep=20, n/st=800, rew=-14.82]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #69: test_reward: -13.412743 ± 6.241280, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #70: 20001it [00:34, 582.02it/s, agent_0/loss=0.308, agent_1/loss=0.453, agent_2/loss=0.443, env_step=1400000, len=75, n/ep=10, n/st=800, rew=-15.45]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  1820\n",
      "Epoch #70: test_reward: -12.738128 ± 7.822553, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #71: 20001it [00:35, 570.03it/s, agent_0/loss=0.312, agent_1/loss=0.423, agent_2/loss=0.445, env_step=1420000, len=75, n/ep=10, n/st=800, rew=-9.93]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #71: test_reward: -12.433844 ± 5.221982, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #72: 20001it [00:34, 576.08it/s, agent_0/loss=0.316, agent_1/loss=0.428, agent_2/loss=0.438, env_step=1440000, len=75, n/ep=20, n/st=800, rew=-12.22]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #72: test_reward: -13.664111 ± 6.810185, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #73: 20001it [00:34, 578.32it/s, agent_0/loss=0.305, agent_1/loss=0.427, agent_2/loss=0.419, env_step=1460000, len=75, n/ep=10, n/st=800, rew=-10.58]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #73: test_reward: -11.837762 ± 5.825406, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #74: 20001it [00:35, 567.33it/s, agent_0/loss=0.304, agent_1/loss=0.447, agent_2/loss=0.444, env_step=1480000, len=75, n/ep=10, n/st=800, rew=-16.61]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #74: test_reward: -11.730436 ± 6.199725, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #75: 20001it [00:34, 581.44it/s, agent_0/loss=0.306, agent_1/loss=0.425, agent_2/loss=0.462, env_step=1500000, len=75, n/ep=20, n/st=800, rew=-13.15]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  1950\n",
      "Epoch #75: test_reward: -14.404898 ± 6.896298, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #76: 20001it [00:35, 564.71it/s, agent_0/loss=0.304, agent_1/loss=0.465, agent_2/loss=0.467, env_step=1520000, len=75, n/ep=10, n/st=800, rew=-10.92]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #76: test_reward: -12.951003 ± 6.204547, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #77: 20001it [00:39, 507.02it/s, agent_0/loss=0.295, agent_1/loss=0.469, agent_2/loss=0.428, env_step=1540000, len=75, n/ep=10, n/st=800, rew=-14.69]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #77: test_reward: -11.953252 ± 5.656091, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #78: 20001it [00:35, 565.00it/s, agent_0/loss=0.310, agent_1/loss=0.461, agent_2/loss=0.458, env_step=1560000, len=75, n/ep=20, n/st=800, rew=-14.43]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #78: test_reward: -13.674564 ± 8.085698, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #79: 20001it [00:35, 565.58it/s, agent_0/loss=0.307, agent_1/loss=0.471, agent_2/loss=0.430, env_step=1580000, len=75, n/ep=10, n/st=800, rew=-13.92]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #79: test_reward: -13.660847 ± 7.462176, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #80: 20001it [00:35, 567.13it/s, agent_0/loss=0.327, agent_1/loss=0.484, agent_2/loss=0.444, env_step=1600000, len=75, n/ep=10, n/st=800, rew=-12.19]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  2080\n",
      "Epoch #80: test_reward: -14.741714 ± 9.330006, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #81: 20001it [00:35, 570.10it/s, agent_0/loss=0.352, agent_1/loss=0.485, agent_2/loss=0.503, env_step=1620000, len=75, n/ep=20, n/st=800, rew=-11.28]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #81: test_reward: -13.712855 ± 7.230678, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #82: 20001it [00:35, 563.10it/s, agent_0/loss=0.354, agent_1/loss=0.475, agent_2/loss=0.474, env_step=1640000, len=75, n/ep=10, n/st=800, rew=-14.06]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #82: test_reward: -12.702710 ± 7.643124, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #83: 20001it [00:35, 567.22it/s, agent_0/loss=0.344, agent_1/loss=0.463, agent_2/loss=0.478, env_step=1660000, len=75, n/ep=10, n/st=800, rew=-9.59]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #83: test_reward: -11.854295 ± 5.935349, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #84: 20001it [00:35, 569.56it/s, agent_0/loss=0.343, agent_1/loss=0.450, agent_2/loss=0.510, env_step=1680000, len=75, n/ep=20, n/st=800, rew=-10.10]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #84: test_reward: -11.751647 ± 6.056054, best_reward: -11.043515 ± 5.017869 in #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #85: 20001it [00:35, 570.90it/s, agent_0/loss=0.344, agent_1/loss=0.446, agent_2/loss=0.473, env_step=1700000, len=75, n/ep=10, n/st=800, rew=-12.55]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  2210\n",
      "Best Saved Rew 2211\n",
      "Epoch #85: test_reward: -10.866622 ± 6.004604, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #86: 20001it [00:35, 570.93it/s, agent_0/loss=0.325, agent_1/loss=0.420, agent_2/loss=0.451, env_step=1720000, len=75, n/ep=10, n/st=800, rew=-10.62]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #86: test_reward: -11.443381 ± 4.749487, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #87: 20001it [00:34, 573.84it/s, agent_0/loss=0.334, agent_1/loss=0.452, agent_2/loss=0.462, env_step=1740000, len=75, n/ep=20, n/st=800, rew=-14.40]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #87: test_reward: -11.832654 ± 5.780268, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #88: 20001it [00:35, 566.68it/s, agent_0/loss=0.349, agent_1/loss=0.475, agent_2/loss=0.503, env_step=1760000, len=75, n/ep=10, n/st=800, rew=-8.63]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #88: test_reward: -12.576067 ± 6.637888, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #89: 20001it [00:36, 546.60it/s, agent_0/loss=0.331, agent_1/loss=0.473, agent_2/loss=0.506, env_step=1780000, len=75, n/ep=10, n/st=800, rew=-12.78]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #89: test_reward: -11.882980 ± 6.178765, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #90: 20001it [00:34, 571.84it/s, agent_0/loss=0.345, agent_1/loss=0.460, agent_2/loss=0.490, env_step=1800000, len=75, n/ep=20, n/st=800, rew=-10.21]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  2340\n",
      "Epoch #90: test_reward: -11.643105 ± 5.622509, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #91: 20001it [00:35, 567.87it/s, agent_0/loss=0.332, agent_1/loss=0.447, agent_2/loss=0.466, env_step=1820000, len=75, n/ep=10, n/st=800, rew=-11.53]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #91: test_reward: -13.220094 ± 6.703675, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #92: 20001it [00:35, 557.87it/s, agent_0/loss=0.347, agent_1/loss=0.503, agent_2/loss=0.487, env_step=1840000, len=75, n/ep=10, n/st=800, rew=-12.07]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #92: test_reward: -12.248351 ± 5.544505, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #93: 20001it [00:35, 569.67it/s, agent_0/loss=0.342, agent_1/loss=0.468, agent_2/loss=0.459, env_step=1860000, len=75, n/ep=20, n/st=800, rew=-12.33]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #93: test_reward: -11.564010 ± 5.980436, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #94: 20001it [00:35, 570.92it/s, agent_0/loss=0.329, agent_1/loss=0.453, agent_2/loss=0.470, env_step=1880000, len=75, n/ep=10, n/st=800, rew=-12.47]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #94: test_reward: -11.476759 ± 6.470915, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #95: 20001it [00:34, 575.06it/s, agent_0/loss=0.348, agent_1/loss=0.490, agent_2/loss=0.488, env_step=1900000, len=75, n/ep=10, n/st=800, rew=-13.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  2470\n",
      "Epoch #95: test_reward: -11.327754 ± 5.715237, best_reward: -10.866622 ± 6.004604 in #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #96: 20001it [00:35, 570.68it/s, agent_0/loss=0.350, agent_1/loss=0.481, agent_2/loss=0.480, env_step=1920000, len=75, n/ep=20, n/st=800, rew=-11.81]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved Rew 2497\n",
      "Epoch #96: test_reward: -10.839567 ± 4.527871, best_reward: -10.839567 ± 4.527871 in #96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #97: 20001it [00:34, 573.84it/s, agent_0/loss=0.352, agent_1/loss=0.460, agent_2/loss=0.446, env_step=1940000, len=75, n/ep=10, n/st=800, rew=-14.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #97: test_reward: -11.980600 ± 6.465296, best_reward: -10.839567 ± 4.527871 in #96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #98: 20001it [00:35, 570.85it/s, agent_0/loss=0.356, agent_1/loss=0.444, agent_2/loss=0.456, env_step=1960000, len=75, n/ep=10, n/st=800, rew=-10.06]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #98: test_reward: -12.297905 ± 5.084365, best_reward: -10.839567 ± 4.527871 in #96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #99: 20001it [00:35, 568.82it/s, agent_0/loss=0.329, agent_1/loss=0.443, agent_2/loss=0.447, env_step=1980000, len=75, n/ep=20, n/st=800, rew=-12.48]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #99: test_reward: -11.533498 ± 7.055826, best_reward: -10.839567 ± 4.527871 in #96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #100: 20001it [00:34, 573.31it/s, agent_0/loss=0.342, agent_1/loss=0.447, agent_2/loss=0.470, env_step=2000000, len=75, n/ep=10, n/st=800, rew=-8.83]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  2600\n",
      "Best Saved Rew 2601\n",
      "Epoch #100: test_reward: -9.272048 ± 5.842043, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #101: 20001it [00:34, 573.45it/s, agent_0/loss=0.312, agent_1/loss=0.421, agent_2/loss=0.442, env_step=2020000, len=75, n/ep=10, n/st=800, rew=-7.14]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #101: test_reward: -11.314835 ± 4.963106, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #102: 20001it [00:34, 576.58it/s, agent_0/loss=0.321, agent_1/loss=0.430, agent_2/loss=0.433, env_step=2040000, len=75, n/ep=20, n/st=800, rew=-11.05]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #102: test_reward: -11.365619 ± 4.896710, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #103: 20001it [00:35, 570.26it/s, agent_0/loss=0.349, agent_1/loss=0.414, agent_2/loss=0.448, env_step=2060000, len=75, n/ep=10, n/st=800, rew=-13.35]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #103: test_reward: -10.964331 ± 5.869108, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #104: 20001it [00:34, 572.50it/s, agent_0/loss=0.349, agent_1/loss=0.408, agent_2/loss=0.438, env_step=2080000, len=75, n/ep=10, n/st=800, rew=-13.29]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #104: test_reward: -11.765620 ± 6.491712, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #105: 20001it [00:35, 569.42it/s, agent_0/loss=0.327, agent_1/loss=0.412, agent_2/loss=0.414, env_step=2100000, len=75, n/ep=20, n/st=800, rew=-10.76]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  2730\n",
      "Epoch #105: test_reward: -9.967134 ± 4.971542, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #106: 20001it [00:34, 571.80it/s, agent_0/loss=0.305, agent_1/loss=0.401, agent_2/loss=0.393, env_step=2120000, len=75, n/ep=10, n/st=800, rew=-8.45]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #106: test_reward: -11.740309 ± 6.586666, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #107: 20001it [00:35, 562.97it/s, agent_0/loss=0.325, agent_1/loss=0.388, agent_2/loss=0.400, env_step=2140000, len=75, n/ep=10, n/st=800, rew=-9.95]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #107: test_reward: -11.305383 ± 6.895594, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #108: 20001it [00:36, 554.81it/s, agent_0/loss=0.323, agent_1/loss=0.388, agent_2/loss=0.413, env_step=2160000, len=75, n/ep=20, n/st=800, rew=-12.44]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #108: test_reward: -10.001176 ± 5.083861, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #109: 20001it [00:34, 578.86it/s, agent_0/loss=0.332, agent_1/loss=0.423, agent_2/loss=0.423, env_step=2180000, len=75, n/ep=10, n/st=800, rew=-9.39]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #109: test_reward: -10.610229 ± 4.671438, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #110: 20001it [00:34, 575.34it/s, agent_0/loss=0.339, agent_1/loss=0.415, agent_2/loss=0.418, env_step=2200000, len=75, n/ep=10, n/st=800, rew=-10.39]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  2860\n",
      "Epoch #110: test_reward: -10.973770 ± 4.962072, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #111: 20001it [00:34, 572.26it/s, agent_0/loss=0.336, agent_1/loss=0.406, agent_2/loss=0.409, env_step=2220000, len=75, n/ep=20, n/st=800, rew=-10.66]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #111: test_reward: -11.098862 ± 4.817202, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #112: 20001it [00:35, 569.59it/s, agent_0/loss=0.304, agent_1/loss=0.389, agent_2/loss=0.389, env_step=2240000, len=75, n/ep=10, n/st=800, rew=-9.93]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #112: test_reward: -10.095122 ± 5.231123, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #113: 20001it [00:34, 576.28it/s, agent_0/loss=0.289, agent_1/loss=0.347, agent_2/loss=0.393, env_step=2260000, len=75, n/ep=10, n/st=800, rew=-11.93]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #113: test_reward: -10.267037 ± 5.264308, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #114: 20001it [00:34, 575.78it/s, agent_0/loss=0.295, agent_1/loss=0.350, agent_2/loss=0.382, env_step=2280000, len=75, n/ep=20, n/st=800, rew=-11.56]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #114: test_reward: -10.058566 ± 4.642512, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #115: 20001it [00:34, 572.45it/s, agent_0/loss=0.300, agent_1/loss=0.376, agent_2/loss=0.371, env_step=2300000, len=75, n/ep=10, n/st=800, rew=-9.52]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  2990\n",
      "Epoch #115: test_reward: -9.878284 ± 4.198153, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #116: 20001it [00:34, 575.03it/s, agent_0/loss=0.289, agent_1/loss=0.370, agent_2/loss=0.374, env_step=2320000, len=75, n/ep=10, n/st=800, rew=-10.28]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #116: test_reward: -9.808738 ± 7.049243, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #117: 20001it [00:37, 530.87it/s, agent_0/loss=0.278, agent_1/loss=0.386, agent_2/loss=0.412, env_step=2340000, len=75, n/ep=20, n/st=800, rew=-9.99]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #117: test_reward: -10.222614 ± 4.715020, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #118: 20001it [00:35, 563.28it/s, agent_0/loss=0.277, agent_1/loss=0.367, agent_2/loss=0.381, env_step=2360000, len=75, n/ep=10, n/st=800, rew=-8.91]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #118: test_reward: -11.179644 ± 4.670053, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #119: 20001it [00:35, 562.07it/s, agent_0/loss=0.282, agent_1/loss=0.363, agent_2/loss=0.344, env_step=2380000, len=75, n/ep=10, n/st=800, rew=-8.58]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #119: test_reward: -9.575688 ± 4.451048, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #120: 20001it [00:35, 562.79it/s, agent_0/loss=0.275, agent_1/loss=0.360, agent_2/loss=0.348, env_step=2400000, len=75, n/ep=20, n/st=800, rew=-9.37]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  3120\n",
      "Epoch #120: test_reward: -10.312085 ± 7.130776, best_reward: -9.272048 ± 5.842043 in #100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #121: 20001it [00:34, 572.18it/s, agent_0/loss=0.260, agent_1/loss=0.334, agent_2/loss=0.341, env_step=2420000, len=75, n/ep=10, n/st=800, rew=-8.47]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Saved Rew 3147\n",
      "Epoch #121: test_reward: -8.012583 ± 3.209611, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #122: 20001it [00:34, 575.42it/s, agent_0/loss=0.264, agent_1/loss=0.375, agent_2/loss=0.342, env_step=2440000, len=75, n/ep=10, n/st=800, rew=-10.42]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #122: test_reward: -10.185394 ± 4.011883, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #123: 20001it [00:34, 580.62it/s, agent_0/loss=0.269, agent_1/loss=0.343, agent_2/loss=0.349, env_step=2460000, len=75, n/ep=20, n/st=800, rew=-11.11]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #123: test_reward: -9.183181 ± 5.313994, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #124: 20001it [00:34, 572.41it/s, agent_0/loss=0.270, agent_1/loss=0.328, agent_2/loss=0.347, env_step=2480000, len=75, n/ep=10, n/st=800, rew=-8.43]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #124: test_reward: -11.421088 ± 6.141855, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #125: 20001it [00:34, 572.79it/s, agent_0/loss=0.259, agent_1/loss=0.307, agent_2/loss=0.317, env_step=2500000, len=75, n/ep=10, n/st=800, rew=-10.12]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  3250\n",
      "Epoch #125: test_reward: -10.524530 ± 5.310665, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #126: 20001it [00:37, 534.99it/s, agent_0/loss=0.263, agent_1/loss=0.331, agent_2/loss=0.324, env_step=2520000, len=75, n/ep=20, n/st=800, rew=-10.78]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #126: test_reward: -9.112960 ± 5.022667, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #127: 20001it [00:35, 561.87it/s, agent_0/loss=0.267, agent_1/loss=0.333, agent_2/loss=0.331, env_step=2540000, len=75, n/ep=10, n/st=800, rew=-11.61]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #127: test_reward: -9.726134 ± 5.166072, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #128: 20001it [00:33, 596.20it/s, agent_0/loss=0.266, agent_1/loss=0.344, agent_2/loss=0.324, env_step=2560000, len=75, n/ep=10, n/st=800, rew=-8.04]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #128: test_reward: -9.050146 ± 4.193676, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #129: 20001it [00:33, 589.95it/s, agent_0/loss=0.267, agent_1/loss=0.329, agent_2/loss=0.324, env_step=2580000, len=75, n/ep=20, n/st=800, rew=-8.36]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #129: test_reward: -8.901389 ± 4.845243, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #130: 20001it [00:33, 590.08it/s, agent_0/loss=0.261, agent_1/loss=0.341, agent_2/loss=0.352, env_step=2600000, len=75, n/ep=10, n/st=800, rew=-10.37]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  3380\n",
      "Epoch #130: test_reward: -9.470214 ± 5.040372, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #131: 20001it [00:33, 593.81it/s, agent_0/loss=0.239, agent_1/loss=0.321, agent_2/loss=0.323, env_step=2620000, len=75, n/ep=10, n/st=800, rew=-10.67]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #131: test_reward: -9.677211 ± 4.215299, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #132: 20001it [00:33, 592.42it/s, agent_0/loss=0.232, agent_1/loss=0.312, agent_2/loss=0.295, env_step=2640000, len=75, n/ep=20, n/st=800, rew=-11.39]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #132: test_reward: -8.674048 ± 4.302035, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #133: 20001it [00:36, 547.89it/s, agent_0/loss=0.285, agent_1/loss=0.323, agent_2/loss=0.327, env_step=2660000, len=75, n/ep=10, n/st=800, rew=-7.58]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #133: test_reward: -8.785231 ± 4.109295, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #134: 20001it [00:34, 579.96it/s, agent_0/loss=0.243, agent_1/loss=0.303, agent_2/loss=0.291, env_step=2680000, len=75, n/ep=10, n/st=800, rew=-8.57]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #134: test_reward: -9.564940 ± 4.172010, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #135: 20001it [00:34, 579.56it/s, agent_0/loss=0.230, agent_1/loss=0.298, agent_2/loss=0.281, env_step=2700000, len=75, n/ep=20, n/st=800, rew=-10.22]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps Policy Saved  3510\n",
      "Epoch #135: test_reward: -9.604478 ± 4.373172, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #136: 20001it [00:35, 565.82it/s, agent_0/loss=0.226, agent_1/loss=0.281, agent_2/loss=0.301, env_step=2720000, len=75, n/ep=10, n/st=800, rew=-9.63]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #136: test_reward: -9.403795 ± 4.078298, best_reward: -8.012583 ± 3.209611 in #121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #137:  40%|####      | 8000/20000 [00:14<00:23, 520.73it/s, agent_0/loss=0.242, agent_1/loss=0.301, agent_2/loss=0.325, env_step=2728000, len=75, n/ep=10, n/st=800, rew=-8.08] "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "                        \n",
    "    torch.set_grad_enabled(True) \n",
    "   \n",
    "    # ======== Step 1: Environment setup =========\n",
    "    train_envs = DummyVectorEnv([_get_env for _ in range(train_env_num)])\n",
    "    test_envs = DummyVectorEnv([_get_env for _ in range(test_env_num)]) \n",
    "\n",
    "    # seed\n",
    "    seed = 0\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    train_envs.seed(seed)\n",
    "    test_envs.seed(seed)\n",
    "\n",
    "    # ======== Step 2: Agent setup =========\n",
    "    policy, optim, agents = _get_agents()    \n",
    "\n",
    "    if False:\n",
    "        agents_buffers_training = {agent : \n",
    "                           PrioritizedVectorReplayBuffer( 30_000, \n",
    "                                                          len(train_envs), \n",
    "                                                          alpha=0.6, \n",
    "                                                          beta=0.4) \n",
    "                                                          for agent in agents\n",
    "                         }\n",
    "        agents_buffers_test = {agent : \n",
    "                           PrioritizedVectorReplayBuffer( 30_000, \n",
    "                                                          len(train_envs), \n",
    "                                                          alpha=0.6, \n",
    "                                                          beta=0.4) \n",
    "                                                          for agent in agents\n",
    "                         }\n",
    "    \n",
    "        # ======== Step 3: Collector setup =========\n",
    "        train_collector = CollectorMA.CollectorMA(\n",
    "            policy,\n",
    "            train_envs,\n",
    "            agents_buffers_training,                        \n",
    "            exploration_noise=True             \n",
    "        )\n",
    "        test_collector = CollectorMA.CollectorMA(policy, test_envs, agents_buffers_test, exploration_noise=True)\n",
    "\n",
    "    if True:\n",
    "         # ======== Step 3: Collector setup =========\n",
    "        train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        # VectorReplayBuffer(300_000, len(train_envs)),\n",
    "        PrioritizedVectorReplayBuffer( 30_000, len(train_envs), alpha=0.6, beta=0.4) , \n",
    "        #ListReplayBuffer(100000)       \n",
    "        # buffer = StateMemoryVectorReplayBuffer(\n",
    "        #         300_000,\n",
    "        #         len(train_envs),  # Assuming train_envs is your vectorized environment\n",
    "        #         memory_size=10,                \n",
    "        #     ),\n",
    "        exploration_noise=True             \n",
    "        )\n",
    "        test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "        \n",
    "    print(\"Buffer Warming Up \")    \n",
    "    for i in range(trainer_params[\"warmup_size\"]):#int(trainer_params['batch_size'] / (300 * 10 ) )):\n",
    "        \n",
    "        train_collector.collect(n_episode=train_env_num)#,random=True) #trainer_params['batch_size'] * train_env_num))\n",
    "        #train_collector.collect(n_step=300 * 10)\n",
    "        print(\".\", end=\"\") \n",
    "    \n",
    "    # len_buffer = len(train_collector.buffer) / (Spread_Config[\"max_cycles\"] * Spread_Config[\"N\"])\n",
    "    # print(\"\\nBuffer Lenght: \", len_buffer ) \n",
    "    \n",
    "    info = { \"Buffer\"  : \"PriorizedReplayBuffer\", \" Warmup_ep\" : runConfig[\"warmup_size\"]}\n",
    "    # ======== tensorboard logging setup =========                       \n",
    "    logger = WandbLogger(\n",
    "        train_interval = runConfig[\"max_cycles\"] * runConfig[\"N\"] ,\n",
    "        test_interval = 1,#runConfig[\"max_cycles\"] * runConfig[\"n_pursuers\"],\n",
    "        update_interval = runConfig[\"max_cycles\"],\n",
    "        save_interval = 1,\n",
    "        write_flush = True,\n",
    "        project = \"Spread_Eval01\",\n",
    "        name = log_name,\n",
    "        entity = None,\n",
    "        run_id = log_name,\n",
    "        config = runConfig,\n",
    "        monitor_gym = True )\n",
    "    \n",
    "    writer = SummaryWriter(log_path)    \n",
    "    writer.add_text(\"args\", str(runConfig))    \n",
    "    logger.load(writer)\n",
    "\n",
    "    \n",
    "    global_step_holder = [0] \n",
    "    \n",
    "    \n",
    "    # ======== Step 4: Callback functions setup =========\n",
    "    def save_best_fn(policy):                \n",
    "        \n",
    "        if Policy_Config[\"same_policy\"]:\n",
    "            torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_BestRew.pth\")\n",
    "            print(\"Best Saved Rew\" , str(global_step_holder[0]))\n",
    "        \n",
    "        else:\n",
    "            for n,agent in enumerate(agents):\n",
    "                torch.save(policy.policies[agent].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_\" + agent + \".pth\")\n",
    "            \n",
    "            print(\"Bests Saved Rew\" , str(global_step_holder[0]))\n",
    "        \n",
    "    def save_test_best_fn(policy):                \n",
    "        \n",
    "        if Policy_Config[\"same_policy\"]:\n",
    "            torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_BestLen.pth\")\n",
    "            print(\"Best Saved Length\" , str(global_step_holder[0]))\n",
    "        \n",
    "        else:\n",
    "            for n,agent in enumerate(agents):\n",
    "                torch.save(policy.policies[agent].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_\" + agent + \".pth\")\n",
    "            \n",
    "            print(\"Best Saved Length\" , str(global_step_holder[0]))\n",
    "        \n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= 99999939.0\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        epsilon = trainer_params['tn_eps_max'] - (trainer_params['tn_eps_max'] - trainer_params['tn_eps_max']/100)*(epoch/trainer_params['max_epoch'])          \n",
    "        if Policy_Config[\"same_policy\"]:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:\n",
    "            for agent in agents:\n",
    "                policy.policies[agent].set_eps(epsilon)\n",
    "                \n",
    "        \n",
    "        # if env_step % 500 == 0:\n",
    "            # logger.write(\"train/env_step\", env_step, {\"train/eps\": eps})\n",
    "\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "               \n",
    "        epsilon = trainer_params['ts_eps_max']#0.01#max(0.001, 0.1 - epoch * 0.001)\n",
    "        if Policy_Config[\"same_policy\"]:\n",
    "            policy.policies[agents[0]].set_eps(epsilon)\n",
    "        else:            \n",
    "            for agent in agents:                             \n",
    "                 policy.policies[agent].set_eps(epsilon)\n",
    "                \n",
    "        \n",
    "        if global_step_holder[0] % 10 == 0:\n",
    "            \n",
    "            if Policy_Config[\"same_policy\"]:\n",
    "                torch.save(policy.policies[agents[0]].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_Step.pth\")\n",
    "                print(\"Steps Policy Saved \" , str(global_step_holder[0]))\n",
    "            \n",
    "            else:\n",
    "                for n,agent in enumerate(agents):\n",
    "                    torch.save(policy.policies[agent].state_dict(), model_save_path + \"_\" + str(global_step_holder[0]) + \"_\" + agent + \"Step\" + str(global_step_holder[0]) + \".pth\")\n",
    "                \n",
    "                print(\"Steps Policy Saved \" , str(global_step_holder[0]))\n",
    "        \n",
    "    def reward_metric(rews):       \n",
    "                \n",
    "        global_step_holder[0] +=1 \n",
    "        return rews\n",
    "\n",
    "\n",
    "    # # ======== Step 5: Run the trainer =========\n",
    "    offPolicyTrainer = OffpolicyTrainer(\n",
    "        policy=policy,\n",
    "        train_collector=train_collector,\n",
    "        test_collector=test_collector,        \n",
    "        max_epoch=trainer_params['max_epoch'],\n",
    "        step_per_epoch=trainer_params['step_per_epoch'],\n",
    "        step_per_collect=trainer_params['step_per_collect'],        \n",
    "        episode_per_test= trainer_params['episode_per_test'],\n",
    "        batch_size=trainer_params['batch_size'],\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        # save_test_best_fn=save_test_best_fn,\n",
    "        update_per_step=trainer_params['update_per_step'],\n",
    "        logger=logger,\n",
    "        test_in_train=True,\n",
    "        reward_metric=reward_metric,\n",
    "        show_progress = True \n",
    "               \n",
    "        )\n",
    "    \n",
    "    result = offPolicyTrainer.run()\n",
    "    writer.close()\n",
    "    # return result, policy.policies[agents[1]]\n",
    "    print(f\"\\n==========Result==========\\n{result}\")\n",
    "    print(\"\\n(the trained policy can be accessed via policy.policies[agents[0]])\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PettingZoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
